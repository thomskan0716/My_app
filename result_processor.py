import pandas as pd
import sqlite3
import os
import hashlib
import shutil
from datetime import datetime
from typing import Optional, Dict, Any

class DBManager:
    def __init__(self, db_path="results.db", custom_conn=None):
        if custom_conn is not None:
            self.conn = custom_conn
            # ES: Intentar deducir el path real del archivo DB desde la conexiÃ³n (para backups)
            # EN: Try to infer the real DB file path from the connection (for backups)
            # JA: æ¥ç¶šã‹ã‚‰DBãƒ•ã‚¡ã‚¤ãƒ«ã®å®Ÿãƒ‘ã‚¹ã‚’æ¨å®šï¼ˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ç”¨ï¼‰
            self.db_path = self._infer_db_path_from_conn(custom_conn) or db_path
        else:
            self.conn = sqlite3.connect(db_path)
            self.db_path = db_path
        self.create_tables()
        self._migrate_db_schema()

    @staticmethod
    def _infer_db_path_from_conn(conn) -> Optional[str]:
        try:
            cur = conn.cursor()
            cur.execute("PRAGMA database_list;")
            rows = cur.fetchall()
            # (seq, name, file)
            for _, name, file_path in rows:
                if name == "main" and file_path:
                    return str(file_path)
        except Exception:
            return None
        return None

    @staticmethod
    def map_column_names(df):
        column_mapping = {
            'ä¸Šé¢ãƒ€ãƒ¬': 'ä¸Šé¢ãƒ€ãƒ¬é‡',
            'ä¸Šé¢ãƒ€ãƒ¬é‡': 'ä¸Šé¢ãƒ€ãƒ¬é‡',
            'å´é¢ãƒ€ãƒ¬': 'å´é¢ãƒ€ãƒ¬é‡',
            'å´é¢ãƒ€ãƒ¬é‡': 'å´é¢ãƒ€ãƒ¬é‡',
            'å›è»¢æ–¹å‘': 'UPã‚«ãƒƒãƒˆ',
            'UPã‚«ãƒƒãƒˆ': 'UPã‚«ãƒƒãƒˆ',
            'åˆ‡è¾¼é‡': 'åˆ‡è¾¼é‡',
            'åˆ‡è¾¼ã¿é‡': 'åˆ‡è¾¼é‡',
            'é¢ç²—åº¦(Ra)å‰': 'é¢ç²—åº¦å‰',
            'ç²—åº¦(Ra)å‰': 'é¢ç²—åº¦å‰',
            'é¢ç²—åº¦å‰': 'é¢ç²—åº¦å‰',
            'é¢ç²—åº¦(Ra)å¾Œ': 'é¢ç²—åº¦å¾Œ',
            'ç²—åº¦(Ra)å¾Œ': 'é¢ç²—åº¦å¾Œ',
            'é¢ç²—åº¦å¾Œ': 'é¢ç²—åº¦å¾Œ',
            'çªå‡ºé‡': 'çªå‡ºé‡',
            'çªå‡ºã—é‡': 'çªå‡ºé‡',
            'è¼‰ã›ç‡': 'è¼‰ã›ç‡',
            'ç·šæé•·': 'ç·šæé•·',  # Keep original name
            'å®Ÿé¨“æ—¥': 'å®Ÿé¨“æ—¥',  # Keep original name
            'æ‘©è€—é‡': 'æ‘©è€—é‡',
            'å›è»¢é€Ÿåº¦': 'å›è»¢é€Ÿåº¦',
            'é€ã‚Šé€Ÿåº¦': 'é€ã‚Šé€Ÿåº¦',
            'ãƒ‘ã‚¹æ•°': 'ãƒ‘ã‚¹æ•°',
            'åˆ‡å‰ŠåŠ›X': 'åˆ‡å‰ŠåŠ›X',
            'åˆ‡å‰ŠåŠ›Y': 'åˆ‡å‰ŠåŠ›Y',
            'åˆ‡å‰ŠåŠ›Z': 'åˆ‡å‰ŠåŠ›Z',
            # 'åŠ å·¥æ™‚é–“': 'åŠ å·¥æ™‚é–“(s/100mm)'  # No se importa, se calcula automÃ¡ticamente
        }
        return df.rename(columns=column_mapping)

    def create_tables(self):
        query = """
        CREATE TABLE IF NOT EXISTS main_results (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            å®Ÿé¨“æ—¥ TEXT,
            ãƒãƒªé™¤å» INTEGER,
            ä¸Šé¢ãƒ€ãƒ¬é‡ REAL,
            å´é¢ãƒ€ãƒ¬é‡ REAL,
            æ‘©è€—é‡ REAL,
            é¢ç²—åº¦å‰ REAL,
            é¢ç²—åº¦å¾Œ REAL,
            A13 INTEGER,
            A11 INTEGER,
            A21 INTEGER,
            A32 INTEGER,
            ç›´å¾„ REAL,
            ææ–™ TEXT,
            ç·šæé•· INTEGER,
            ç·šææœ¬æ•° INTEGER DEFAULT 6,
            å›è»¢é€Ÿåº¦ INTEGER,
            é€ã‚Šé€Ÿåº¦ INTEGER,
            UPã‚«ãƒƒãƒˆ INTEGER,
            åˆ‡è¾¼é‡ REAL,
            çªå‡ºé‡ INTEGER,
            è¼‰ã›ç‡ REAL,
            ãƒ‘ã‚¹æ•° INTEGER,
            åŠ å·¥æ™‚é–“ REAL
        );
        """
        self.conn.execute(query)
        self.conn.commit()

    def _migrate_db_schema(self):
        """ES: AÃ±ade columnas nuevas a tablas existentes sin romper BDs antiguas.
        EN: Add new columns to existing tables without breaking old DBs.
        JA: æ—¢å­˜ãƒ†ãƒ¼ãƒ–ãƒ«ã«æ–°åˆ—ã‚’è¿½åŠ ï¼ˆå¤ã„DBã‚’å£Šã•ãªã„ï¼‰ã€‚"""
        try:
            table = "main_results"
            desired_cols = {
                "ç·šææœ¬æ•°": "INTEGER DEFAULT 6",
            }
            cur = self.conn.cursor()
            cur.execute(f"PRAGMA table_info({table});")
            existing = {row[1] for row in cur.fetchall()}  # row[1] = name
            for col, col_type in desired_cols.items():
                if col not in existing:
                    self.conn.execute(f"ALTER TABLE {table} ADD COLUMN {col} {col_type}")
            try:
                self.conn.execute("UPDATE main_results SET ç·šææœ¬æ•° = 6 WHERE ç·šææœ¬æ•° IS NULL")
                self.conn.commit()
            except Exception:
                pass
        except Exception:
            # MigraciÃ³n best-effort
            pass

    @staticmethod
    def normalize_for_hash(df, key_cols):
        df_norm = df[key_cols].copy().fillna("")
        for col in key_cols:
            df_norm[col] = df_norm[col].apply(
                lambda x: f"{float(x):.5f}" if str(x).replace('.', '', 1).isdigit() else str(x).strip()
            )
        return df_norm

    def insert_results(self, df):
        if df.empty:
            print("âš ï¸ æŒ¿å…¥ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            return

        if "id" in df.columns:
            df = df.drop(columns=["id"])

        # ğŸ”‘ Columnas clave para identificar duplicados
        key_cols = [
            "å®Ÿé¨“æ—¥",
            "A13", "A11", "A21", "A32",
            "ç›´å¾„", "ææ–™",
            "ç·šæé•·",
            "ç·šææœ¬æ•°",
            "å›è»¢é€Ÿåº¦", "é€ã‚Šé€Ÿåº¦", "UPã‚«ãƒƒãƒˆ", "åˆ‡è¾¼é‡", "çªå‡ºé‡", "è¼‰ã›ç‡", "ãƒ‘ã‚¹æ•°",
            "ãƒãƒªé™¤å»",
            "ä¸Šé¢ãƒ€ãƒ¬é‡", "å´é¢ãƒ€ãƒ¬é‡", "æ‘©è€—é‡",
            "åˆ‡å‰ŠåŠ›X", "åˆ‡å‰ŠåŠ›Y", "åˆ‡å‰ŠåŠ›Z",
            "é¢ç²—åº¦å‰", "é¢ç²—åº¦å¾Œ",
        ]

        for col in key_cols:
            if col not in df.columns:
                raise ValueError(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚­ãƒ¼åˆ—ãŒã‚ã‚Šã¾ã›ã‚“: {col}")

        # ES: ğŸ’¾ Leer registros actuales desde la BBDD
        # EN: ğŸ’¾ Read current records from the database
        # JP: ğŸ’¾ ç¾åœ¨ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’DBã‹ã‚‰èª­ã¿è¾¼ã‚€
        db_df = pd.read_sql_query(f"SELECT {', '.join(key_cols)} FROM main_results", self.conn)

        df_cmp_norm = DBManager.normalize_for_hash(df, key_cols)
        db_cmp_norm = DBManager.normalize_for_hash(db_df, key_cols)

        df_cmp_norm_hashes = df_cmp_norm.apply(lambda row: "||".join(row.values.astype(str)), axis=1)
        db_cmp_norm_hashes = db_cmp_norm.apply(lambda row: "||".join(row.values.astype(str)), axis=1)

        df["__hash"] = df_cmp_norm_hashes
        db_hashes = set(db_cmp_norm_hashes)

        df_to_insert = df[~df["__hash"].isin(db_hashes)].drop(columns=["__hash"])

        # âœ… AHORA VA ESTO:
        if df_to_insert.empty:
            print("âš ï¸ ã™ã¹ã¦ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã¯ã™ã§ã«DBã«å­˜åœ¨ã—ã¾ã™ã€‚")
            return

        df_to_insert.to_sql("main_results", self.conn, if_exists="append", index=False)
        print(f"âœ… æ–°è¦ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ {len(df_to_insert)} ä»¶æŒ¿å…¥ã—ã¾ã—ãŸã€‚")

    def _create_db_backup(self) -> Optional[str]:
        """
        Crear backup del archivo SQLite antes de sobrescribir registros.
        Devuelve el path del backup si se pudo crear.
        """
        try:
            db_path = self.db_path
            if not db_path:
                db_path = self._infer_db_path_from_conn(self.conn)
            if not db_path or not os.path.exists(db_path):
                return None
            db_dir = os.path.dirname(db_path) or "."
            backup_dir = os.path.join(db_dir, "backup")
            os.makedirs(backup_dir, exist_ok=True)
            ts = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_path = os.path.join(backup_dir, f"results_backup_{ts}.db")
            shutil.copy2(db_path, backup_path)
            return backup_path
        except Exception:
            return None

    def upsert_results(self, df: pd.DataFrame, debug: bool = False) -> Dict[str, Any]:
        """
        Upsert:
        - Si existe una fila con la misma clave (condiciones), se actualiza (sobrescribe) el resto de campos.
        - Si no existe, se inserta.

        Devuelve: {'inserted': int, 'updated': int, 'db_backup_path': str|None}
        """
        if df.empty:
            return {"inserted": 0, "updated": 0, "db_backup_path": None}

        if "id" in df.columns:
            df = df.drop(columns=["id"])

        # Clave de comparaciÃ³n (condiciones + meta necesaria)
        key_cols = [
            "å®Ÿé¨“æ—¥",
            "A13", "A11", "A21", "A32",
            "ç›´å¾„", "ææ–™",
            "ç·šæé•·",
            "ç·šææœ¬æ•°",
            "å›è»¢é€Ÿåº¦", "é€ã‚Šé€Ÿåº¦", "UPã‚«ãƒƒãƒˆ", "åˆ‡è¾¼é‡", "çªå‡ºé‡", "è¼‰ã›ç‡", "ãƒ‘ã‚¹æ•°",
        ]

        for col in key_cols:
            if col not in df.columns:
                raise ValueError(f"âŒ upsert ç”¨ã®ã‚­ãƒ¼åˆ—ãŒã‚ã‚Šã¾ã›ã‚“: {col}")

        # ES: Columnas a actualizar (todas menos la clave)
        # EN: Columns to update (everything except the key)
        # JA: æ›´æ–°å¯¾è±¡åˆ—ï¼ˆã‚­ãƒ¼åˆ—ä»¥å¤–ã™ã¹ã¦ï¼‰
        # ES: Importante:
        # EN: Notes:
        # JA: æ³¨æ„:
        # ES: - Ignorar columnas que no existan en la tabla real
        # EN: - Ignore columns that do not exist in the actual table
        # JA: - å®Ÿãƒ†ãƒ¼ãƒ–ãƒ«ã«å­˜åœ¨ã—ãªã„åˆ—ã¯ç„¡è¦–
        # ES: - Ignorar columnas totalmente vacÃ­as (suelen venir de defaults cuando el archivo no las trae)
        # EN: - Ignore columns that are entirely empty (often defaults when the file doesn't include them)
        # JA: - å…¨ã¦ç©ºã®åˆ—ã¯ç„¡è¦–ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«æœªæä¾›æ™‚ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆç”±æ¥ãŒå¤šã„ï¼‰
        update_cols_raw = [c for c in df.columns if c not in key_cols]
        update_cols_raw = [c for c in update_cols_raw if not df[c].isna().all()]

        # Columnas reales en la tabla (evita comparar/actualizar campos no existentes)
        cur = self.conn.cursor()
        cur.execute("PRAGMA table_info(main_results)")
        existing_cols = {row[1] for row in cur.fetchall()}  # name
        update_cols = [c for c in update_cols_raw if c in existing_cols]

        # ES: Leer ids existentes + clave + columnas a comparar (mÃ­nimas)
        # EN: Read existing ids + key + minimal columns to compare
        # JP: æ—¢å­˜ID + ã‚­ãƒ¼ + æ¯”è¼ƒç”¨ã®æœ€å°åˆ—ã‚’èª­ã¿è¾¼ã‚€
        db_cols = ["id"] + key_cols + update_cols
        db_df = pd.read_sql_query(f"SELECT {', '.join(db_cols)} FROM main_results", self.conn)

        df_key_norm = DBManager.normalize_for_hash(df, key_cols)
        db_key_norm = DBManager.normalize_for_hash(db_df, key_cols) if not db_df.empty else db_df

        df_keys = df_key_norm.apply(lambda r: "||".join(r.values.astype(str)), axis=1).tolist()
        db_map = {}
        if not db_df.empty:
            db_keys = db_key_norm.apply(lambda r: "||".join(r.values.astype(str)), axis=1).tolist()
            for k, row_id in zip(db_keys, db_df["id"].tolist()):
                # ES: Si hay duplicados previos, nos quedamos con el primero
                # EN: If there are existing duplicates, keep the first one
                # JA: æ—¢å­˜ã®é‡è¤‡ãŒã‚ã‚‹å ´åˆã¯æœ€åˆã®ã‚‚ã®ã‚’æ¡ç”¨
                if k not in db_map:
                    db_map[k] = row_id

        # Preparar updates/inserts
        to_update = []
        to_insert_rows = []
        updated_count = 0
        inserted_count = 0

        def _norm_val(v: Any) -> str:
            # ES: NormalizaciÃ³n robusta para comparar "igualdad" (evita falsos positivos por formato)
            # EN: Robust normalization for equality checks (avoids format-based false positives)
            # JA: ç­‰ä¾¡æ¯”è¼ƒç”¨ã®å …ç‰¢ãªæ­£è¦åŒ–ï¼ˆæ›¸å¼å·®ã«ã‚ˆã‚‹èª¤åˆ¤å®šã‚’é˜²ãï¼‰
            try:
                if pd.isna(v):
                    return ""
            except Exception:
                pass
            if v is None:
                return ""
            # ES: NumÃ©ricos: fijar precisiÃ³n
            # EN: Numerics: fix precision
            # JA: æ•°å€¤ï¼šç²¾åº¦ã‚’å›ºå®š
            try:
                if isinstance(v, (int, float)) and not isinstance(v, bool):
                    return f"{float(v):.6f}"
            except Exception:
                pass
            # ES: Strings numÃ©ricos: intentar convertir
            # EN: Numeric strings: try to convert
            # JA: æ•°å€¤æ–‡å­—åˆ—ï¼šå¤‰æ›ã‚’è©¦è¡Œ
            try:
                s = str(v).strip()
                if s == "":
                    return ""
                n = pd.to_numeric(s, errors="coerce")
                if pd.notna(n):
                    return f"{float(n):.6f}"
                return s
            except Exception:
                return str(v).strip()

        # Para decidir si realmente se sobrescribe algo, comparamos update_cols (si existen en DB)
        db_update_lookup = {}
        if not db_df.empty and update_cols:
            # index por id
            db_update_lookup = db_df.set_index("id")[update_cols].to_dict(orient="index")

        def _key_brief_from_row(row: pd.Series) -> str:
            # Resumen corto para logs
            parts = []
            for c in ["å®Ÿé¨“æ—¥", "å›è»¢é€Ÿåº¦", "é€ã‚Šé€Ÿåº¦", "UPã‚«ãƒƒãƒˆ", "åˆ‡è¾¼é‡", "çªå‡ºé‡", "è¼‰ã›ç‡", "ãƒ‘ã‚¹æ•°", "ç·šæé•·"]:
                if c in row.index:
                    parts.append(f"{c}={row.get(c)}")
            # Brush one-hot
            for c in ["A13", "A11", "A21", "A32"]:
                if c in row.index:
                    parts.append(f"{c}={row.get(c)}")
            return ", ".join(parts)

        if debug:
            try:
                print(
                    f"ğŸ§¾ UPSERT DEBUG: filas_entrada={len(df)} | cols_update={len(update_cols)} | cols_update_list={update_cols}",
                    flush=True,
                )
            except Exception:
                pass

        for i, k in enumerate(df_keys):
            if k in db_map:
                row_id = db_map[k]
                # Determinar si cambia algo
                will_change = False
                diffs = []
                if update_cols:
                    old = db_update_lookup.get(row_id, {})
                    for c in update_cols:
                        new_val = df.iloc[i][c]
                        old_val = old.get(c)
                        # ES: Si el archivo no trae valor (NaN/None/""), no lo usamos para decidir ni para sobrescribir
                        # EN: If the file does not provide a value (NaN/None/\"\"), do not use it to decide or overwrite
                        # JP: ãƒ•ã‚¡ã‚¤ãƒ«å´ã«å€¤ãŒç„¡ã„å ´åˆï¼ˆNaN/None/\"\"ï¼‰ã€åˆ¤æ–­ã«ã‚‚ä¸Šæ›¸ãã«ã‚‚ä½¿ã‚ãªã„
                        if _norm_val(new_val) == "":
                            continue
                        if _norm_val(new_val) != _norm_val(old_val):
                            will_change = True
                            if debug:
                                diffs.append((c, old_val, new_val, _norm_val(old_val), _norm_val(new_val)))
                            else:
                                # Si no estamos en modo debug, con el primer cambio basta
                                break
                else:
                    will_change = False

                if will_change:
                    updated_count += 1
                    params = [df.iloc[i][c] for c in update_cols] + [row_id]
                    to_update.append(params)
                    if debug:
                        try:
                            brief = _key_brief_from_row(df.iloc[i])
                            if diffs:
                                diff_str = " | ".join(
                                    [
                                        f"{c}: {old_norm} -> {new_norm} (raw {old_raw!r} -> {new_raw!r})"
                                        for (c, old_raw, new_raw, old_norm, new_norm) in diffs
                                    ]
                                )
                                print(f"ğŸŸ¥ UPSERT UPDATE id={row_id} | {brief} | diffs={diff_str}", flush=True)
                            else:
                                print(f"ğŸŸ¥ UPSERT UPDATE id={row_id} | {brief}", flush=True)
                        except Exception:
                            pass
                else:
                    if debug:
                        try:
                            brief = _key_brief_from_row(df.iloc[i])
                            print(f"ğŸŸ¦ UPSERT SKIPï¼ˆåŒä¸€ï¼‰ id={row_id} | {brief}", flush=True)
                        except Exception:
                            pass
            else:
                to_insert_rows.append(df.iloc[i])
                inserted_count += 1
                if debug:
                    try:
                        brief = _key_brief_from_row(df.iloc[i])
                        print(f"ğŸŸ© UPSERT INSERT | {brief}", flush=True)
                    except Exception:
                        pass

        db_backup_path = None
        if updated_count > 0:
                # ES: âœ… Crear backup UNA SOLA VEZ antes de sobrescribir
                # EN: âœ… Create a backup ONCE before overwriting
                # JP: âœ… ä¸Šæ›¸ãå‰ã«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ä¸€åº¦ã ã‘ä½œæˆã™ã‚‹
            db_backup_path = self._create_db_backup()
            if db_backup_path:
                print(f"ğŸ“‹ Backup de BBDD creado: {db_backup_path}", flush=True)
            else:
                print("âš ï¸ è‡ªå‹•DBãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ä½œæˆã§ãã¾ã›ã‚“ã§ã—ãŸï¼ˆãƒ‘ã‚¹ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ï¼‰ã€‚", flush=True)

        # Ejecutar updates
        if to_update and update_cols:
            # COALESCE: si llega NULL, conserva el valor existente (no sobrescribe con vacÃ­o)
            set_clause = ", ".join([f"{c} = COALESCE(?, {c})" for c in update_cols])
            sql = f"UPDATE main_results SET {set_clause} WHERE id = ?"
            cur = self.conn.cursor()
            cur.executemany(sql, to_update)
            self.conn.commit()

        # Ejecutar inserts
        if to_insert_rows:
            df_to_insert = pd.DataFrame(to_insert_rows)
            df_to_insert.to_sql("main_results", self.conn, if_exists="append", index=False)

        return {"inserted": inserted_count, "updated": updated_count, "db_backup_path": db_backup_path}

    def fetch_all(self, table):
        """Obtener todos los registros de una tabla"""
        cursor = self.conn.cursor()
        cursor.execute(f"SELECT * FROM {table};")
        return cursor.fetchall()
    
    def print_all_results(self):
        """Imprimir todos los registros de la tabla main_results"""
        cursor = self.conn.cursor()
        cursor.execute("SELECT * FROM main_results")
        results = cursor.fetchall()
        print(f"ğŸ“Š DBã®ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(results)}")
        if results:
            print("ğŸ“‹ Primeros 5 registros:")
            for i, row in enumerate(results[:5]):
                print(f"  Registro {i+1}: {row}")
        else:
            print("ğŸ“‹ DBã«ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“")

        df["__hash"] = df_cmp_norm_hashes
        db_hashes = set(db_cmp_norm_hashes)

        df_to_insert = df[~df["__hash"].isin(db_hashes)].drop(columns=["__hash"])

        # âœ… AHORA VA ESTO:
        if df_to_insert.empty:
            print("âš ï¸ ã™ã¹ã¦ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã¯ã™ã§ã«DBã«å­˜åœ¨ã—ã¾ã™ã€‚")
            return

        df_to_insert.to_sql("main_results", self.conn, if_exists="append", index=False)
        print(f"âœ… {len(df_to_insert)} registros nuevos insertados.")

    def close(self):
        self.conn.close()

    def print_all_results(self):
        query = "SELECT * FROM main_results"
        df = pd.read_sql_query(query, self.conn)

        if df.empty:
            print("âš ï¸ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãŒç©ºã§ã™ã€‚")
        else:
            pd.set_option("display.max_columns", None)
            pd.set_option("display.max_rows", None)
            pd.set_option("display.width", None)
            pd.set_option("display.colheader_justify", "left")
            print("ğŸ“Š DBã®å…¨å†…å®¹:\n")
            print(df)


class ResultProcessor:
    def __init__(self, db_manager):
        self.db = db_manager

    def _read_any_table(self, file_path: str) -> pd.DataFrame:
        ext = os.path.splitext(str(file_path))[1].lower()
        if ext == ".csv":
            df = pd.read_csv(file_path, encoding="utf-8-sig")
        else:
            df = pd.read_excel(file_path, header=0)

        # Normalizar nombres de columnas (evita fallos por espacios invisibles)
        try:
            if isinstance(df.columns, pd.MultiIndex):
                df.columns = [" ".join([str(x).strip() for x in tup if str(x).strip() != ""]).strip() for tup in df.columns]
            else:
                df.columns = [str(c).strip() for c in df.columns]
        except Exception:
            pass
        return df

    def process_results_file(self, file_path, selected_brush, senzai_length):
        df = self._read_any_table(file_path)
        df = DBManager.map_column_names(df)
        
        # ES: Eliminar åŠ å·¥æ™‚é–“ si estÃ¡ presente (se calcula automÃ¡ticamente)
        # EN: Drop åŠ å·¥æ™‚é–“ if present (it is computed automatically)
        # JA: åŠ å·¥æ™‚é–“ ãŒã‚ã‚Œã°å‰Šé™¤ï¼ˆè‡ªå‹•è¨ˆç®—ã™ã‚‹ãŸã‚ï¼‰
        if 'åŠ å·¥æ™‚é–“' in df.columns:
            df = df.drop(columns=['åŠ å·¥æ™‚é–“'])
        if 'åŠ å·¥æ™‚é–“(s/100mm)' in df.columns:
            df = df.drop(columns=['åŠ å·¥æ™‚é–“(s/100mm)'])

        columns_required = ['å›è»¢é€Ÿåº¦', 'é€ã‚Šé€Ÿåº¦', 'UPã‚«ãƒƒãƒˆ', 'åˆ‡è¾¼é‡', 'çªå‡ºé‡', 'è¼‰ã›ç‡', 'ãƒ‘ã‚¹æ•°',
                            'ç·šæé•·', 'ä¸Šé¢ãƒ€ãƒ¬é‡', 'å´é¢ãƒ€ãƒ¬é‡', 'æ‘©è€—é‡', 'é¢ç²—åº¦å‰', 'é¢ç²—åº¦å¾Œ', 'å®Ÿé¨“æ—¥']

        missing_columns = [col for col in columns_required if col not in df.columns]
        if missing_columns:
            raise ValueError(f"âŒ El archivo de resultados no contiene las siguientes columnas necesarias: {', '.join(missing_columns)}")

        df_filtered = df[columns_required].copy()

        # ES: Calcular ãƒãƒªé™¤å» basado en ä¸Šé¢ãƒ€ãƒ¬é‡
        # EN: Compute ãƒãƒªé™¤å» based on ä¸Šé¢ãƒ€ãƒ¬é‡
        # JA: ä¸Šé¢ãƒ€ãƒ¬é‡ ã«åŸºã¥ã ãƒãƒªé™¤å» ã‚’ç®—å‡º
        df_filtered['ãƒãƒªé™¤å»'] = df_filtered['ä¸Šé¢ãƒ€ãƒ¬é‡'].apply(lambda x: 1 if x > 0 else 0)

        # ES: Brush: SIEMPRE desde el archivo (one-hot A13/A11/A21/A32). No usar UI.
        # EN: Brush: ALWAYS from the file (one-hot A13/A11/A21/A32). Do not use UI.
        # JA: ãƒ–ãƒ©ã‚·ï¼šå¿…ãšãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ï¼ˆA13/A11/A21/A32ã®one-hotï¼‰ã€‚UIã¯ä½¿ã‚ãªã„ã€‚
        brush_cols = ["A13", "A11", "A21", "A32"]
        missing_brush = [c for c in brush_cols if c not in df.columns]
        if missing_brush:
            raise ValueError(
                "âŒ El archivo de resultados debe incluir columnas de cepillo one-hot: "
                f"{', '.join(brush_cols)} (faltan: {', '.join(missing_brush)})"
            )

        for c in brush_cols:
            df_filtered[c] = pd.to_numeric(df[c], errors="coerce").fillna(0).astype(int)

        # ES: ValidaciÃ³n bÃ¡sica: exactamente 1 cepillo activo por fila
        # EN: Basic validation: exactly one active brush per row
        # JA: åŸºæœ¬æ¤œè¨¼ï¼šå„è¡Œã§æœ‰åŠ¹ãƒ–ãƒ©ã‚·ã¯1ã¤ã®ã¿
        try:
            s = df_filtered[brush_cols].sum(axis=1)
            bad = df_filtered[(s != 1)]
            if not bad.empty:
                raise ValueError(
                    "âŒ Formato de cepillo invÃ¡lido: cada fila debe tener exactamente un 1 "
                    f"en {brush_cols}. Filas invÃ¡lidas: {bad.index.tolist()[:10]}"
                )
        except ValueError:
            raise
        except Exception:
            # ES: Si falla la validaciÃ³n por algÃºn motivo, no bloquear el import
            # EN: If validation fails for any reason, do not block the import
            # JA: æ¤œè¨¼ãŒå¤±æ•—ã—ã¦ã‚‚ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’ãƒ–ãƒ­ãƒƒã‚¯ã—ãªã„ï¼ˆå®‰å…¨å´ï¼‰
            pass
        
        # ES: Calcular åŠ å·¥æ™‚é–“ usando la fÃ³rmula: 100/é€ã‚Šé€Ÿåº¦*60
        # EN: Compute åŠ å·¥æ™‚é–“ using the formula: 100/é€ã‚Šé€Ÿåº¦*60
        # JA: åŠ å·¥æ™‚é–“ ã‚’è¨ˆç®—ï¼ˆå¼ï¼š100/é€ã‚Šé€Ÿåº¦*60ï¼‰
        df_filtered['åŠ å·¥æ™‚é–“'] = (100 / df_filtered['é€ã‚Šé€Ÿåº¦']) * 60
        
        # ES: Asignar valores por defecto para campos que pueden no estar en el archivo
        # EN: Assign default values for fields that may be missing from the file
        # JA: ãƒ•ã‚¡ã‚¤ãƒ«ã«ãªã„å¯èƒ½æ€§ã®ã‚ã‚‹é …ç›®ã«ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š
        if 'ç›´å¾„' in df.columns:
            df_filtered['ç›´å¾„'] = df['ç›´å¾„']
        else:
            df_filtered['ç›´å¾„'] = 0.15  # Default value
        if 'ææ–™' in df.columns:
            df_filtered['ææ–™'] = df['ææ–™']
        else:
            df_filtered['ææ–™'] = 'Steel'  # Default value

        # ES: Cutting forces opcionales:
        # EN: Optional cutting forces:
        # JA: åˆ‡å‰ŠåŠ›ï¼ˆä»»æ„ï¼‰
        # ES: Si no vienen en el archivo, NO crear la columna (asÃ­ no se usa para comparar/actualizar)
        # EN: If they are not present in the file, do NOT create the column (so it won't be used for compare/update)
        # JA: ãƒ•ã‚¡ã‚¤ãƒ«ã«ç„¡ã‘ã‚Œã°åˆ—ã‚’ä½œã‚‰ãªã„ï¼ˆæ¯”è¼ƒ/æ›´æ–°ã«ä½¿ã‚ãªã„ãŸã‚ï¼‰
        for c in ["åˆ‡å‰ŠåŠ›X", "åˆ‡å‰ŠåŠ›Y", "åˆ‡å‰ŠåŠ›Z"]:
            if c in df.columns:
                df_filtered[c] = pd.to_numeric(df[c], errors="coerce")

        df_filtered = DBManager.map_column_names(df_filtered)

        # ES: Upsert: sobreescribe si ya existe la misma clave (condiciones)
        # EN: Upsert: overwrite when the same key (conditions) already exists
        # JA: ã‚¢ãƒƒãƒ—ã‚µãƒ¼ãƒˆï¼šåŒä¸€ã‚­ãƒ¼ï¼ˆæ¡ä»¶ï¼‰ãŒã‚ã‚Œã°ä¸Šæ›¸ã
        res = self.db.upsert_results(df_filtered, debug=True)
        print(f"âœ… Upsert completado. insertados={res['inserted']} actualizados={res['updated']}")
        print("âœ… å‡¦ç†ã¨æŒ¿å…¥ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
        self.db.print_all_results()

    def process_results_file_with_ui_values(self, file_path, selected_brush, diameter, material, wire_count, custom_conn=None):
        """ES: Procesar archivo de resultados importando columnas especÃ­ficas y usando valores de UI
        EN: Process a results file importing specific columns and using UI values
        JA: çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ï¼ˆç‰¹å®šåˆ—ã‚’å–ã‚Šè¾¼ã¿ã€UIå€¤ã‚’ä½¿ç”¨ï¼‰
        """
        # ES: Leer todas las columnas del archivo para asegurar que å®Ÿé¨“æ—¥ estÃ© incluido
        # EN: Read all columns to ensure å®Ÿé¨“æ—¥ is included
        # JA: å®Ÿé¨“æ—¥ ã‚’ç¢ºå®Ÿã«å«ã‚ã‚‹ãŸã‚å…¨åˆ—ã‚’èª­ã¿è¾¼ã‚€
        df = self._read_any_table(file_path)
        
        # ES: Mapear nombres de columnas
        # EN: Map column names
        # JA: åˆ—åã‚’ãƒãƒƒãƒ”ãƒ³ã‚°
        df = DBManager.map_column_names(df)
        
        # ES: Eliminar åŠ å·¥æ™‚é–“ si estÃ¡ presente (se calcula automÃ¡ticamente)
        # EN: Drop åŠ å·¥æ™‚é–“ if present (it is computed automatically)
        # JA: åŠ å·¥æ™‚é–“ ãŒã‚ã‚Œã°å‰Šé™¤ï¼ˆè‡ªå‹•è¨ˆç®—ã™ã‚‹ãŸã‚ï¼‰
        if 'åŠ å·¥æ™‚é–“' in df.columns:
            df = df.drop(columns=['åŠ å·¥æ™‚é–“'])
        if 'åŠ å·¥æ™‚é–“(s/100mm)' in df.columns:
            df = df.drop(columns=['åŠ å·¥æ™‚é–“(s/100mm)'])
        
        # Columnas requeridas (despuÃ©s del mapeo)
        columns_required = ['å›è»¢é€Ÿåº¦', 'é€ã‚Šé€Ÿåº¦', 'UPã‚«ãƒƒãƒˆ', 'åˆ‡è¾¼é‡', 'çªå‡ºé‡', 'è¼‰ã›ç‡', 'ãƒ‘ã‚¹æ•°',
                            'ç·šæé•·', 'ä¸Šé¢ãƒ€ãƒ¬é‡', 'å´é¢ãƒ€ãƒ¬é‡', 'æ‘©è€—é‡', 'é¢ç²—åº¦å‰', 'é¢ç²—åº¦å¾Œ', 'å®Ÿé¨“æ—¥']
        
        # ES: Verificar columnas faltantes
        # EN: Check for missing columns
        # JA: ä¸è¶³åˆ—ã‚’ãƒã‚§ãƒƒã‚¯
        missing_columns = [col for col in columns_required if col not in df.columns]
        if missing_columns:
            raise ValueError(f"âŒ El archivo de resultados no contiene las siguientes columnas necesarias: {', '.join(missing_columns)}")
        
        # ES: Filtrar solo las columnas requeridas
        # EN: Keep only the required columns
        # JA: å¿…é ˆåˆ—ã®ã¿æŠ½å‡º
        df_filtered = df[columns_required].copy()
        
        # ES: Calcular ãƒãƒªé™¤å» basado en ä¸Šé¢ãƒ€ãƒ¬é‡
        # EN: Compute ãƒãƒªé™¤å» based on ä¸Šé¢ãƒ€ãƒ¬é‡
        # JA: ä¸Šé¢ãƒ€ãƒ¬é‡ ã«åŸºã¥ã ãƒãƒªé™¤å» ã‚’ç®—å‡º
        df_filtered['ãƒãƒªé™¤å»'] = df_filtered['ä¸Šé¢ãƒ€ãƒ¬é‡'].apply(lambda x: 1 if x > 0 else 0)
        
        # ES: Brush: SIEMPRE desde el archivo (one-hot A13/A11/A21/A32). No usar UI.
        # EN: Brush: ALWAYS from the file (one-hot A13/A11/A21/A32). Do not use UI.
        # JA: ãƒ–ãƒ©ã‚·ï¼šå¿…ãšãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ï¼ˆA13/A11/A21/A32ã®one-hotï¼‰ã€‚UIã¯ä½¿ã‚ãªã„ã€‚
        brush_cols = ["A13", "A11", "A21", "A32"]
        missing_brush = [c for c in brush_cols if c not in df.columns]
        if missing_brush:
            raise ValueError(
                "âŒ El archivo de resultados debe incluir columnas de cepillo one-hot: "
                f"{', '.join(brush_cols)} (faltan: {', '.join(missing_brush)})"
            )

        for c in brush_cols:
            df_filtered[c] = pd.to_numeric(df[c], errors="coerce").fillna(0).astype(int)

        # ES: ValidaciÃ³n bÃ¡sica: exactamente 1 cepillo activo por fila
        # EN: Basic validation: exactly one active brush per row
        # JA: åŸºæœ¬æ¤œè¨¼ï¼šå„è¡Œã§æœ‰åŠ¹ãƒ–ãƒ©ã‚·ã¯1ã¤ã®ã¿
        try:
            s = df_filtered[brush_cols].sum(axis=1)
            bad = df_filtered[(s != 1)]
            if not bad.empty:
                raise ValueError(
                    "âŒ Formato de cepillo invÃ¡lido: cada fila debe tener exactamente un 1 "
                    f"en {brush_cols}. Filas invÃ¡lidas: {bad.index.tolist()[:10]}"
                )
        except ValueError:
            raise
        except Exception:
            # ES: Si falla la validaciÃ³n por algÃºn motivo, no bloquear el import
            # EN: If validation fails for any reason, do not block the import
            # JA: æ¤œè¨¼ãŒå¤±æ•—ã—ã¦ã‚‚ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’ãƒ–ãƒ­ãƒƒã‚¯ã—ãªã„ï¼ˆå®‰å…¨å´ï¼‰
            pass
        
        # ES: ç›´å¾„/ææ–™: usar archivo si existe, si no UI
        # EN: ç›´å¾„/ææ–™: use file values if present, otherwise UI
        # JA: ç›´å¾„/ææ–™ï¼šãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚ã‚Œã°ä½¿ç”¨ã€ãªã‘ã‚Œã°UI
        df_filtered['ç›´å¾„'] = df['ç›´å¾„'] if 'ç›´å¾„' in df.columns else diameter
        df_filtered['ææ–™'] = df['ææ–™'] if 'ææ–™' in df.columns else material

        # ES: ç·šææœ¬æ•°: SIEMPRE desde UI (ignorar archivo si existe)
        # EN: ç·šææœ¬æ•°: ALWAYS from UI (ignore file even if present)
        # JA: ç·šææœ¬æ•°ï¼šå¸¸ã«UIå€¤ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚ã£ã¦ã‚‚ç„¡è¦–ï¼‰
        df_filtered['ç·šææœ¬æ•°'] = int(wire_count)

        # ES: Cutting forces opcionales:
        # EN: Optional cutting forces:
        # JA: åˆ‡å‰ŠåŠ›ï¼ˆä»»æ„ï¼‰
        # ES: Si no vienen en el archivo, NO crear la columna (asÃ­ no se usa para comparar/actualizar)
        # EN: If they are not present in the file, do NOT create the column (so it won't be used for compare/update)
        # JA: ãƒ•ã‚¡ã‚¤ãƒ«ã«ç„¡ã‘ã‚Œã°åˆ—ã‚’ä½œã‚‰ãªã„ï¼ˆæ¯”è¼ƒ/æ›´æ–°ã«ä½¿ã‚ãªã„ãŸã‚ï¼‰
        for c in ["åˆ‡å‰ŠåŠ›X", "åˆ‡å‰ŠåŠ›Y", "åˆ‡å‰ŠåŠ›Z"]:
            if c in df.columns:
                df_filtered[c] = pd.to_numeric(df[c], errors="coerce")
        
        # ES: Calcular åŠ å·¥æ™‚é–“ usando la fÃ³rmula: 100/é€ã‚Šé€Ÿåº¦*60
        # EN: Compute åŠ å·¥æ™‚é–“ using the formula: 100/é€ã‚Šé€Ÿåº¦*60
        # JA: åŠ å·¥æ™‚é–“ ã‚’è¨ˆç®—ï¼ˆå¼ï¼š100/é€ã‚Šé€Ÿåº¦*60ï¼‰
        df_filtered['åŠ å·¥æ™‚é–“'] = (100 / df_filtered['é€ã‚Šé€Ÿåº¦']) * 60
        
        # ES: Mapear nombres de columnas para la base de datos
        # EN: Map column names for the database
        # JA: DBç”¨ã«åˆ—åã‚’ãƒãƒƒãƒ”ãƒ³ã‚°
        df_filtered = DBManager.map_column_names(df_filtered)
        
        # ES: Usar conexiÃ³n personalizada si se proporciona, sino usar la del db manager
        # EN: Use custom connection if provided; otherwise use the DB manager connection
        # JA: ã‚«ã‚¹ã‚¿ãƒ æ¥ç¶šãŒã‚ã‚Œã°ä½¿ç”¨ã€ãªã‘ã‚Œã°DBãƒãƒãƒ¼ã‚¸ãƒ£ã®æ¥ç¶šã‚’ä½¿ç”¨
        if custom_conn is not None:
            # ES: Crear un DBManager temporal con la conexiÃ³n personalizada
            # EN: Create a temporary DBManager with the custom connection
            # JA: ã‚«ã‚¹ã‚¿ãƒ æ¥ç¶šã§ä¸€æ™‚DBManagerã‚’ä½œæˆ
            temp_db = DBManager(custom_conn=custom_conn)
            res = temp_db.upsert_results(df_filtered, debug=True)
            print(f"âœ… Upsert completado (conn personalizada). insertados={res['inserted']} actualizados={res['updated']}")
            print("âœ… UIå€¤ï¼ˆã‚«ã‚¹ã‚¿ãƒ æ¥ç¶šï¼‰ã§ã®å‡¦ç†ã¨æŒ¿å…¥ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
            return res
        else:
            res = self.db.upsert_results(df_filtered, debug=True)
            print(f"âœ… Upsert completado. insertados={res['inserted']} actualizados={res['updated']}")
            print("âœ… Procesamiento completado con valores de UI.")
            self.db.print_all_results()
            return res

    def print_all_results(self):
        """Imprimir todos los registros de la tabla main_results"""
        cursor = self.conn.cursor()
        cursor.execute("SELECT * FROM main_results")
        results = cursor.fetchall()
        print(f"ğŸ“Š DBã®ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(results)}")
        if results:
            print("ğŸ“‹ Primeros 5 registros:")
            for i, row in enumerate(results[:5]):
                print(f"  Registro {i+1}: {row}")
        else:
            print("ğŸ“‹ DBã«ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“")