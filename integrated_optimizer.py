import matplotlib.pyplot as plt
from matplotlib import rcParams
import pandas as pd
import numpy as np
from itertools import product, combinations
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from scipy.linalg import qr
from scipy.spatial.distance import cdist
import time
import os
import json
import warnings
import shutil
from datetime import datetime
warnings.filterwarnings('ignore')

# ES: âœ… NUEVO: ConfiguraciÃ³n de joblib para evitar errores de subprocess en Windows | EN: âœ… NEW: joblib configuration to avoid subprocess errors on Windows | JA: âœ… æ–°è¦: Windowsã§ã®subprocessã‚¨ãƒ©ãƒ¼å›é¿ã®ãŸã‚joblibã‚’è¨­å®š
try:
    import joblib
    # ES: Configurar joblib para usar un nÃºmero fijo de workers y evitar detecciÃ³n automÃ¡tica de CPU
    # EN: Configure joblib to use a fixed number of workers and avoid automatic CPU detection
    # JA: joblibã®ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã‚’å›ºå®šã—ã€CPUè‡ªå‹•æ¤œå‡ºã‚’å›é¿
    joblib.parallel.BACKENDS['threading'].n_jobs = 1
    joblib.parallel.BACKENDS['multiprocessing'].n_jobs = 1
    print("âœ… subprocess ã‚¨ãƒ©ãƒ¼å›é¿ã®ãŸã‚ joblib ã‚’è¨­å®šã—ã¾ã—ãŸ")
except ImportError:
    print("âš ï¸ joblib ãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€å€‹åˆ¥è¨­å®šãªã—ã§ç¶šè¡Œã—ã¾ã™")

# ES: === ConfiguraciÃ³n de fuentes === | EN: === Font configuration === | JA: === ãƒ•ã‚©ãƒ³ãƒˆè¨­å®š ===
FONT_NAME = "Meiryo"
rcParams['font.family'] = FONT_NAME

# ES: ConfiguraciÃ³n de optimizaciÃ³n
# EN: Optimization configuration
# JA: æœ€é©åŒ–è¨­å®š
USE_NUMERICAL_STABLE_METHOD = True
CANDIDATE_REDUCTION_THRESHOLD = 10000
MAX_REDUCED_CANDIDATES = 5000
VERBOSE = True

def load_and_validate_existing_data(existing_file, design_df, verbose=True):
    """ES: Carga y valida datos experimentales existentes
    EN: Load and validate existing experimental data
    JA: æ—¢å­˜ã®å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ãƒ»æ¤œè¨¼
    """
    try:
        ext = os.path.splitext(str(existing_file))[1].lower()
        existing_df = pd.read_csv(existing_file, encoding="utf-8-sig") if ext == ".csv" else pd.read_excel(existing_file)
        if verbose:
            print(f"å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {len(existing_df)} è¡Œ Ã— {len(existing_df.columns)} åˆ—")
            print(f"â„¹ï¸ æ—¢å­˜åˆ—: {list(existing_df.columns)}")

        # ES: Obtener nombres de variables explicativas
        # EN: Get explanatory variable names
        # JA: èª¬æ˜å¤‰æ•°åã‚’å–å¾—
        # ES: `design_df` puede venir en formato "tabla de diseÃ±o" con columna 'èª¬æ˜å¤‰æ•°å'
        # EN: `design_df` may come as a "design table" with a 'èª¬æ˜å¤‰æ•°å' column
        # JA: `design_df` ã¯ã€Œè¨­è¨ˆè¡¨ã€å½¢å¼ã§ 'èª¬æ˜å¤‰æ•°å' åˆ—ã‚’å«ã‚€å ´åˆãŒã‚ã‚‹
        if isinstance(design_df, pd.DataFrame) and "èª¬æ˜å¤‰æ•°å" in design_df.columns:
            variable_names = design_df["èª¬æ˜å¤‰æ•°å"].astype(str).tolist()
        else:
            variable_names = design_df.columns.tolist() if isinstance(design_df, pd.DataFrame) else list(design_df)
        if verbose:
            print(f"ğŸ¯ ç›®çš„å¤‰æ•°: {variable_names}")

        # ES: Extraer solo variables explicativas de datos existentes
        # EN: Extract only explanatory variables from existing data
        # JA: æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰èª¬æ˜å¤‰æ•°ã®ã¿æŠ½å‡º
        missing_vars = []
        available_vars = []

        for var in variable_names:
            if var in existing_df.columns:
                available_vars.append(var)
            else:
                missing_vars.append(var)

        if missing_vars:
            print(f"âš ï¸ ä»¥ä¸‹ã®å¤‰æ•°èª¬æ˜å¤‰æ•°ãŒå®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {missing_vars}")
            if len(available_vars) < len(variable_names) * 0.7:
                print("âŒ å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ä¸è¶³ (70%æœªæº€) - å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ãªã„")
                return None, []
            else:
                print(f"âœ… åˆ©ç”¨å¯èƒ½ãªå¤‰æ•° ({len(available_vars)}/{len(variable_names)}) - ç¶šè¡Œ")

        # ES: Extraer solo variables explicativas
        # EN: Extract only explanatory variables
        # JA: èª¬æ˜å¤‰æ•°ã®ã¿æŠ½å‡º
        existing_explanatory = existing_df[available_vars]

        if verbose:
            print(f"âœ… å¤‰æ•°èª¬æ˜å¤‰æ•°æŠ½å‡ºå®Œäº†: {len(existing_explanatory)} è¡Œ Ã— {len(available_vars)} åˆ—")
            print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ« (æœ€åˆã®3è¡Œ):")
            print(existing_explanatory.head(3))
            print(f"ğŸ“ˆ ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ:")
            print(existing_explanatory.describe())

        # ES: Verificaciones de calidad especÃ­ficas para procesos quÃ­micos
        # EN: Quality checks tailored for chemical processes
        # JA: åŒ–å­¦ãƒ—ãƒ­ã‚»ã‚¹å‘ã‘ã®å“è³ªãƒã‚§ãƒƒã‚¯
        # ES: 1. VerificaciÃ³n de valores faltantes
        # EN: 1) Missing-value check
        # JA: 1) æ¬ æå€¤ãƒã‚§ãƒƒã‚¯
        missing_count = existing_explanatory.isnull().sum().sum()
        if missing_count > 0:
            print(f"âš ï¸ æ¬ æå€¤æ¤œå‡º: {missing_count}")
            existing_explanatory = existing_explanatory.dropna()
            print(f"ğŸ”§ æ¬ æå€¤å‰Šé™¤å¾Œ: {len(existing_explanatory)} è¡Œ")

        # ES: 2. VerificaciÃ³n de puntos experimentales duplicados
        # EN: 2) Duplicate-point check
        # JA: 2) é‡è¤‡ç‚¹ãƒã‚§ãƒƒã‚¯
        duplicates = existing_explanatory.duplicated().sum()
        if duplicates > 0:
            print(f"âš ï¸ å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿é‡è¤‡æ¤œå‡º: {duplicates}")
            existing_explanatory = existing_explanatory.drop_duplicates()
            print(f"ğŸ”§ é‡è¤‡å‰Šé™¤å¾Œ: {len(existing_explanatory)} è¡Œ")

        return existing_explanatory, available_vars

    except FileNotFoundError:
        print(f"âŒ å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {existing_file}")
        return None, []
    except Exception as e:
        print(f"âŒ å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿æ—¢å­˜èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return None, []

def match_existing_experiments_enhanced(candidate_points, existing_data, variable_names, 
                                      tolerance_relative=1e-6, tolerance_absolute=1e-8, verbose=True):
    """ES: Emparejamiento de alta precisiÃ³n de condiciones experimentales quÃ­micas
    EN: High-precision matching of chemical experimental conditions
    JA: åŒ–å­¦å®Ÿé¨“æ¡ä»¶ã®é«˜ç²¾åº¦ãƒãƒƒãƒãƒ³ã‚°
    """
    if existing_data is None or len(existing_data) == 0:
        return []

    print(f"ğŸ” å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿æ—¢å­˜ç‚¹æ¤œç´¢é–‹å§‹")
    print(f"  - å€™è£œç‚¹æ•°: {len(candidate_points):,}")
    print(f"  - æ—¢å­˜å®Ÿé¨“ç‚¹æ•°: {len(existing_data)}")
    print(f"  - ç›¸å¯¾è¨±å®¹èª¤å·®: {tolerance_relative}")
    print(f"  - çµ¶å¯¾è¨±å®¹èª¤å·®: {tolerance_absolute}")

    # ES: Convertir puntos candidatos a DataFrame | EN: Convert candidate points to a DataFrame | JA: å€™è£œç‚¹ã‚’DataFrameã«å¤‰æ›
    candidate_df = pd.DataFrame(candidate_points, columns=variable_names)

    # ES: Estandarizar ambos conjuntos de datos | EN: Standardize both datasets | JA: ä¸¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ¨™æº–åŒ–
    scaler = StandardScaler()
    candidate_scaled = scaler.fit_transform(candidate_df)

    # ES: Alinear datos existentes al mismo orden de variables | EN: Align existing data to the same variable order | JA: æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’å¤‰æ•°é †ã«åˆã‚ã›ã‚‹
    existing_aligned = existing_data[variable_names]
    existing_scaled = scaler.transform(existing_aligned)

    matched_indices = []
    match_details = []

    # ES: Para cada punto experimental existente, buscar el candidato mÃ¡s cercano
    # EN: For each existing experimental point, find the nearest candidate
    # JA: å„æ—¢å­˜å®Ÿé¨“ç‚¹ã«å¯¾ã—ã¦æœ€ã‚‚è¿‘ã„å€™è£œç‚¹ã‚’æ¢ç´¢
    for exist_idx, exist_row in enumerate(existing_aligned.values):
        min_distance = float('inf')
        best_candidate_idx = None

        for cand_idx, cand_row in enumerate(candidate_df.values):
            # ES: ComparaciÃ³n basada en error relativo | EN: Comparison based on relative error | JA: ç›¸å¯¾èª¤å·®ã«åŸºã¥ãæ¯”è¼ƒ
            relative_errors = []
            absolute_ok = True

            for var_idx, var_name in enumerate(variable_names):
                exist_val = exist_row[var_idx]
                cand_val = cand_row[var_idx]

                # ES: VerificaciÃ³n de error absoluto | EN: Absolute-error check | JA: çµ¶å¯¾èª¤å·®ãƒã‚§ãƒƒã‚¯
                abs_error = abs(exist_val - cand_val)
                if abs_error > tolerance_absolute:
                    # ES: TambiÃ©n verificar error relativo | EN: Also check relative error | JA: ç›¸å¯¾èª¤å·®ã‚‚ç¢ºèª
                    if exist_val != 0:
                        rel_error = abs_error / abs(exist_val)
                        if rel_error > tolerance_relative:
                            absolute_ok = False
                            break
                    else:
                        absolute_ok = False
                        break

                relative_errors.append(abs_error)

            if absolute_ok:
                # Distancia total (en espacio estandarizado)
                distance = np.linalg.norm(existing_scaled[exist_idx] - candidate_scaled[cand_idx])

                if distance < min_distance:
                    min_distance = distance
                    best_candidate_idx = cand_idx

        if best_candidate_idx is not None:
            matched_indices.append(best_candidate_idx)

            # ES: Registrar detalles del emparejamiento | EN: Record matching details | JA: ãƒãƒƒãƒãƒ³ã‚°è©³ç´°ã‚’è¨˜éŒ²
            match_detail = {
                'NÃºmero_experimento_existente': exist_idx,
                'NÃºmero_punto_candidato': best_candidate_idx,
                'Distancia': min_distance,
                'Condiciones_experimento_existente': existing_aligned.iloc[exist_idx].to_dict(),
                'Condiciones_punto_candidato': candidate_df.iloc[best_candidate_idx].to_dict()
            }
            match_details.append(match_detail)

            if verbose and len(matched_indices) <= 5:
                print(f"âœ… ãƒãƒƒãƒãƒ³ã‚° {len(matched_indices)}: Existente#{exist_idx} â†’ Candidato#{best_candidate_idx} (distancia: {min_distance:.4f})")

    # ES: Eliminar duplicados
    # EN: Remove duplicates
    # JA: é‡è¤‡ã‚’é™¤å»
    unique_matched = list(set(matched_indices))

    print(f"ğŸ“Š ãƒãƒƒãƒãƒ³ã‚°çµæœ:")
    print(f"  - åˆæœŸãƒãƒƒãƒãƒ³ã‚°: {len(matched_indices)}")
    print(f"  - é‡è¤‡å‰Šé™¤å¾Œ: {len(unique_matched)}")
    print(f"  - ãƒãƒƒãƒãƒ³ã‚°ç‡: {len(unique_matched)/len(existing_data)*100:.1f}%")

    if len(unique_matched) == 0:
        print("âš ï¸ æ—¢å­˜å®Ÿé¨“ç‚¹ãƒãƒƒãƒãƒ³ã‚°è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        print("ğŸ’¡ è€ƒãˆã‚‰ã‚Œã‚‹åŸå› :")
        print("  1. æ—¢å­˜å®Ÿé¨“ç‚¹æ¡ä»¶æ—¢å­˜ç¯„å›²å¤–")
        print("  2. ã‚¹ãƒ†ãƒƒãƒ—è¨­å®šæ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã«ä¸€è‡´ã—ãªã„")
        print("  3. è¨±å®¹èª¤å·®è¨­å®šå³ã—ã™ã")

        # ES: Proporcionar informaciÃ³n de diagnÃ³stico
        # EN: Provide diagnostic information
        # JA: è¨ºæ–­æƒ…å ±ã‚’å‡ºåŠ›
        print("\nğŸ” è¨ºæ–­æƒ…å ±:")
        for var in variable_names:
            exist_range = (existing_aligned[var].min(), existing_aligned[var].max())
            cand_range = (candidate_df[var].min(), candidate_df[var].max())
            print(f"  {var}: Existente{exist_range} vs Candidato{cand_range}")

    return unique_matched

def hierarchical_candidate_reduction(candidate_points, max_candidates=5000, existing_indices=None):
    """ES: ReducciÃ³n de candidatos mediante muestreo jerÃ¡rquico
    EN: Reduce candidates via hierarchical sampling
    JA: éšå±¤çš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã«ã‚ˆã‚‹å€™è£œã®å‰Šæ¸›"""
    n_original = len(candidate_points)

    if n_original <= max_candidates:
        print(f"ğŸ“Š å€™è£œç‚¹æ•° ({n_original:,}) è¦ç´„ä¸è¦ (é–¾å€¤: {max_candidates:,})")
        return candidate_points, list(range(n_original))

    print(f"ğŸ”„ âœ… éšå±¤çš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å®Ÿè¡Œ: {n_original:,} â†’ {max_candidates:,} ç‚¹")

    # ES: Proteger puntos experimentales existentes | EN: Preserve existing experimental points | JA: æ—¢å­˜å®Ÿé¨“ç‚¹ã‚’ä¿è­·
    if existing_indices:
        existing_set = set(existing_indices)
        available_indices = [i for i in range(n_original) if i not in existing_set]
        available_points = candidate_points[available_indices]
        n_to_select = max_candidates - len(existing_indices)
        print(f"ğŸ“ æ—¢å­˜å®Ÿé¨“ç‚¹ä¿æŒ: {len(existing_indices)} ç‚¹")
    else:
        available_indices = list(range(n_original))
        available_points = candidate_points
        n_to_select = max_candidates
        existing_indices = []

    if n_to_select <= 0:
        print("âš ï¸ æ—¢å­˜ç‚¹ã®ã¿ã§ä¸Šé™ã«é”")
        return candidate_points[existing_indices], existing_indices

    print(f"ğŸ¯ æ–°é¸æŠç›®æ¨™: {n_to_select:,} ç‚¹")

    try:
        from sklearn.cluster import MiniBatchKMeans

        n_clusters = min(n_to_select, len(available_points))
        print(f"ğŸ”§ MiniBatchKMeans Clustering: {n_clusters} clusters")

        kmeans = MiniBatchKMeans(
            n_clusters=n_clusters, 
            random_state=42, 
            batch_size=min(1000, len(available_points)//10),
            n_init=3,
            max_iter=100
        )

        start_time = time.time()
        # ES: scikit-learn reciente ya no soporta n_jobs en MiniBatchKMeans. Para replicar n_jobs=1 limitamos threads SOLO durante el fit.
        # EN: Recent scikit-learn no longer supports n_jobs in MiniBatchKMeans. To mimic n_jobs=1 we limit threads ONLY during fit.
        # JA: æœ€è¿‘ã®scikit-learnã¯MiniBatchKMeansã§n_jobsã‚’ã‚µãƒãƒ¼ãƒˆã—ãªã„ã€‚n_jobs=1ç›¸å½“ã®ãŸã‚fitä¸­ã®ã¿ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’åˆ¶é™ã€‚
        # ES: AdemÃ¡s, en algunos entornos Windows recientes `joblib/loky` intenta usar `wmic` para contar cores y puede fallar.
        # EN: Also, on some recent Windows environments `joblib/loky` tries to use `wmic` to count cores and can fail.
        # JA: ã•ã‚‰ã«ã€æœ€è¿‘ã®Windowsç’°å¢ƒã§ã¯ `joblib/loky` ãŒã‚³ã‚¢æ•°å–å¾—ã« `wmic` ã‚’ä½¿ãŠã†ã¨ã—ã¦å¤±æ•—ã™ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚
        # ES: Para evitarlo, forzamos backend "threading" SOLO en este fit.
        # EN: To avoid that, we force the "threading" backend ONLY for this fit.
        # JA: å›é¿ã®ãŸã‚ã€ã“ã®fitã®é–“ã ã‘ "threading" ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚’å¼·åˆ¶ã—ã¾ã™ã€‚
        try:
            from threadpoolctl import threadpool_limits
            # ES: Silenciar/evitar detecciÃ³n de cores fÃ­sicos vÃ­a wmic en loky (Windows). Mantiene el algoritmo; limita a 1 core en este bloque.
            # EN: Suppress physical-core detection via wmic in loky (Windows). Keeps algorithm; limits to 1 core in this block.
            # JA: lokyï¼ˆWindowsï¼‰ã§ã®wmicã«ã‚ˆã‚‹ç‰©ç†ã‚³ã‚¢æ¤œå‡ºã‚’æŠ‘åˆ¶ã€‚ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯ç¶­æŒã—ã€ã“ã®ãƒ–ãƒ­ãƒƒã‚¯å†…ã¯1ã‚³ã‚¢ã«åˆ¶é™ã€‚
            _prev_loky_max_cpu = os.environ.get("LOKY_MAX_CPU_COUNT")
            os.environ["LOKY_MAX_CPU_COUNT"] = "1"
            try:
                import joblib
                with joblib.parallel_backend("threading", n_jobs=1):
                    with threadpool_limits(limits=1):
                        clusters = kmeans.fit_predict(available_points)
            except Exception:
                with threadpool_limits(limits=1):
                    clusters = kmeans.fit_predict(available_points)
            finally:
                if _prev_loky_max_cpu is None:
                    os.environ.pop("LOKY_MAX_CPU_COUNT", None)
                else:
                    os.environ["LOKY_MAX_CPU_COUNT"] = _prev_loky_max_cpu
        except Exception:
            # ES: Si threadpoolctl no estÃ¡ disponible u ocurre cualquier problema, continuar sin limitar threads
            # EN: If threadpoolctl is unavailable or anything fails, proceed without limiting threads
            # JA: threadpoolctlãŒç„¡ã„/å¤±æ•—ã—ãŸå ´åˆã¯ã‚¹ãƒ¬ãƒƒãƒ‰åˆ¶é™ãªã—ã§ç¶šè¡Œ
            clusters = kmeans.fit_predict(available_points)
        clustering_time = time.time() - start_time
        print(f"â±ï¸ ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æ™‚é–“: {clustering_time:.2f} ç§’")

        # ES: Seleccionar punto representativo de cada cluster | EN: Pick one representative point per cluster | JA: ã‚¯ãƒ©ã‚¹ã‚¿ã”ã¨ã«ä»£è¡¨ç‚¹ã‚’é¸æŠ
        selected_indices = list(existing_indices)

        for i in range(n_clusters):
            cluster_mask = clusters == i
            if np.any(cluster_mask):
                cluster_indices_in_available = np.where(cluster_mask)[0]
                cluster_original_indices = [available_indices[j] for j in cluster_indices_in_available]

                # ES: Seleccionar punto mÃ¡s cercano al centro del cluster | EN: Pick the point closest to the cluster center | JA: ã‚¯ãƒ©ã‚¹ã‚¿ä¸­å¿ƒã«æœ€ã‚‚è¿‘ã„ç‚¹ã‚’é¸æŠ
                cluster_points = available_points[cluster_mask]
                center = kmeans.cluster_centers_[i]
                distances = np.linalg.norm(cluster_points - center, axis=1)
                closest_idx_in_cluster = np.argmin(distances)
                closest_original_idx = cluster_original_indices[closest_idx_in_cluster]

                selected_indices.append(closest_original_idx)

        reduced_points = candidate_points[selected_indices]

        print(f"âœ… éšå±¤çš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å®Œäº†: æœ€çµ‚å€™è£œç‚¹æ•° {len(reduced_points):,}")
        print(f"  - æ—¢å­˜å®Ÿé¨“ç‚¹ä¿æŒ: {len(existing_indices)} ç‚¹")
        print(f"  - æ–°é¸æŠç‚¹æ•°: {len(selected_indices) - len(existing_indices)} ç‚¹")

        return reduced_points, selected_indices

    except Exception as e:
        print(f"âš ï¸ éšå±¤çš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
        return candidate_points, list(range(len(candidate_points)))

def calculate_d_criterion_stable(X, method='auto'):
    """ES: CÃ¡lculo numÃ©ricamente estable del criterio D
    EN: Numerically stable computation of the D criterion
    JA: DåŸºæº–ã®æ•°å€¤çš„ã«å®‰å®šã—ãŸè¨ˆç®—"""
    try:
        condition_number = np.linalg.cond(X)

        if USE_NUMERICAL_STABLE_METHOD or method == 'auto' and condition_number > 1e12:
            method = 'svd'
            if VERBOSE and condition_number > 1e12:
                print(f"ğŸ”§ é«˜æ¡ä»¶æ¤œå‡º ({condition_number:.2e}) - SVDãƒ¡ã‚½ãƒƒãƒ‰é©ç”¨")

        if method == 'svd':
            _, s, _ = np.linalg.svd(X, full_matrices=False)
            valid_singular_values = s[s > 1e-14]
            if len(valid_singular_values) == 0:
                return -np.inf, condition_number
            log_det = np.sum(np.log(valid_singular_values))
        else:
            q, r = qr(X, mode='economic')
            diag_r = np.diag(r)
            det = np.abs(np.prod(diag_r))
            log_det = np.log(det) if det > 1e-300 else -np.inf

        return log_det, condition_number

    except Exception as e:
        if VERBOSE:
            print(f"âš ï¸ åŸºæº–Dè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
        return -np.inf, np.inf

def select_d_optimal_design_enhanced(X_all, existing_indices, new_experiments, verbose=True):
    """ES: SelecciÃ³n de diseÃ±o D-Ã³ptimo (puntos experimentales existentes + nuevos)
    EN: D-optimal design selection (existing experimental points + new ones)
    JA: Dæœ€é©è¨­è¨ˆã®é¸æŠï¼ˆæ—¢å­˜å®Ÿé¨“ç‚¹ï¼‹æ–°è¦ï¼‰"""
    base = list(existing_indices) if existing_indices else []
    remaining = [i for i in range(len(X_all)) if i not in base]
    total_select = len(base) + new_experiments

    if verbose:
        print(f"  - æ—¢å­˜å®Ÿé¨“ç‚¹æ•°: {len(base)} ç‚¹")
        print(f"  - æ–°è¦å®Ÿé¨“ç‚¹æ•°: {new_experiments} ç‚¹")
        print(f"  - é¸æŠç‚¹æ•°åˆè¨ˆ: {total_select} ç‚¹")

    if new_experiments <= 0:
        if verbose:
            print(f"  âœ… æ—¢å­˜å®Ÿé¨“ç‚¹ã®ã¿å®Œäº†")
        score, _ = calculate_d_criterion_stable(X_all[base])
        return base, score

    selected = list(base)

    for step in range(new_experiments):
        best_candidate = None
        best_score = -np.inf

        # Para datos grandes, usar muestreo
        if len(remaining) > 1000:
            sample_size = min(500, len(remaining))
            sample_indices = np.random.choice(remaining, sample_size, replace=False)
            candidates_to_check = sample_indices
        else:
            candidates_to_check = remaining

        for idx in candidates_to_check:
            trial_set = selected + [idx]
            X_subset = X_all[trial_set]
            score, condition_num = calculate_d_criterion_stable(X_subset)

            if score > best_score:
                best_score = score
                best_candidate = idx

        if best_candidate is not None:
            selected.append(best_candidate)
            remaining.remove(best_candidate)
            if verbose:
                print(f"  âœ… æ–°é¸æŠ {step+1}/{new_experiments}: ç‚¹{best_candidate}, ã‚¹ã‚³ã‚¢: {best_score:.4f}")
        else:
            if verbose:
                print(f"  âš ï¸ ã‚¹ãƒ†ãƒƒãƒ— {step+1} é©åˆ‡ãªå€™è£œè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            break

    final_score, final_condition = calculate_d_criterion_stable(X_all[selected])
    return selected, final_score

def select_i_optimal_design(X_all, new_experiments, existing_indices=None):
    """ES: SelecciÃ³n de diseÃ±o I-Ã³ptimo (puntos experimentales existentes + nuevos)
    EN: I-optimal design selection (existing experimental points + new ones)
    JA: Iæœ€é©è¨­è¨ˆã®é¸æŠï¼ˆæ—¢å­˜å®Ÿé¨“ç‚¹ï¼‹æ–°è¦ï¼‰"""
    if existing_indices:
        selected_indices = list(existing_indices)
        print(f"  - æ—¢å­˜å®Ÿé¨“ç‚¹æ•°: {len(existing_indices)} ç‚¹")
        print(f"  - æ–°è¦å®Ÿé¨“ç‚¹æ•°: {new_experiments} ç‚¹")
        print(f"  - é¸æŠç‚¹æ•°åˆè¨ˆ: {len(existing_indices) + new_experiments} ç‚¹")
    else:
        selected_indices = []
        print(f"  - æ–°è¦å®Ÿé¨“ç‚¹æ•°: {new_experiments} ç‚¹ (æ—¢å­˜ç‚¹ãªã—)")

    remaining_indices = [i for i in range(len(X_all)) if i not in selected_indices]
    target_total = len(selected_indices) + new_experiments

    step = 0
    while len(selected_indices) < target_total and remaining_indices:
        if len(selected_indices) == 0:
            # ES: Si no hay puntos seleccionados, elegir el primer punto disponible
            # EN: If no points selected yet, pick the first available point
            # JA: é¸æŠç‚¹ãŒç„¡ã„å ´åˆã¯æœ€åˆã®åˆ©ç”¨å¯èƒ½ç‚¹ã‚’é¸ã¶
            next_index = remaining_indices[0]
            selected_indices.append(next_index)
            remaining_indices.remove(next_index)
            step += 1
            print(f"  âœ… æ–°é¸æŠ {step}/{new_experiments}: ç‚¹{next_index} (æœ€åˆã®ç‚¹)")
        else:
            # ES: Calcular distancias solo si hay puntos seleccionados
            # EN: Compute distances only when there are selected points
            # JA: é¸æŠç‚¹ãŒã‚ã‚‹å ´åˆã®ã¿è·é›¢ã‚’è¨ˆç®—
            dists = cdist(X_all[remaining_indices], X_all[selected_indices])
            min_dists = dists.min(axis=1)
            next_idx_in_remaining = np.argmax(min_dists)
            next_index = remaining_indices[next_idx_in_remaining]
            selected_indices.append(next_index)
            remaining_indices.remove(next_index)
            step += 1
            print(f"  âœ… æ–°é¸æŠ {step}/{new_experiments}: ç‚¹{next_index}")

    return selected_indices

def visualize_feature_histograms(candidate_df, d_indices, i_indices, existing_indices, variable_names, output_folder, optimization_type="both"):
    """ES: ğŸ“Š Histogramas de caracterÃ­sticas con colores diferenciados (uno por variable)
    EN: ğŸ“Š Feature histograms with distinct colors (one per variable)
    JA: ğŸ“Š ç‰¹å¾´é‡ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ï¼ˆå¤‰æ•°ã”ã¨ã«è‰²åˆ†ã‘ï¼‰"""
    print(f"\nğŸ“Š ç‰¹å¾´é‡åˆ†å¸ƒã®å¯è¦–åŒ–é–‹å§‹... (æœ€é©åŒ–ã‚¿ã‚¤ãƒ—: {optimization_type})")

    image_paths = []
    for var_name in variable_names:
        plt.figure(figsize=(6, 4))

        # Histograma de todos los puntos candidatos (fondo)
        plt.hist(candidate_df[var_name], bins=30, alpha=0.3, color='lightgray', 
                label=f'å…¨å€™è£œç‚¹ ({len(candidate_df)})', density=True)

        # ES: Puntos experimentales existentes | EN: Existing experimental points | JA: æ—¢å­˜å®Ÿé¨“ç‚¹
        if existing_indices:
            existing_values = candidate_df.iloc[existing_indices][var_name]
            plt.hist(existing_values, bins=15, alpha=0.8, color='blue', 
                    label=f'æ—¢å­˜ç‚¹ ({len(existing_indices)})', density=True)

        # ES: Mostrar solo los datos relevantes segÃºn el tipo de optimizaciÃ³n | EN: Show only data relevant to the optimization type | JA: æœ€é©åŒ–ã‚¿ã‚¤ãƒ—ã«é–¢é€£ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã®ã¿è¡¨ç¤º
        if optimization_type in ["d", "D", "d_optimal"]:
            # ES: Solo mostrar datos D-Ã³ptimo | EN: Show only D-optimal data | JA: Dæœ€é©ãƒ‡ãƒ¼ã‚¿ã®ã¿è¡¨ç¤º
            d_new_indices = [idx for idx in d_indices if idx not in existing_indices]
            if d_new_indices:
                d_values = candidate_df.iloc[d_new_indices][var_name]
                plt.hist(d_values, bins=10, alpha=0.8, color='red', 
                        label=f'D-æœ€é©æ–°è¦ç‚¹ ({len(d_new_indices)})', density=True)
        elif optimization_type in ["i", "I", "i_optimal"]:
            # ES: Solo mostrar datos I-Ã³ptimo | EN: Show only I-optimal data | JA: Iæœ€é©ãƒ‡ãƒ¼ã‚¿ã®ã¿è¡¨ç¤º
            i_new_indices = [idx for idx in i_indices if idx not in existing_indices]
            if i_new_indices:
                i_values = candidate_df.iloc[i_new_indices][var_name]
                plt.hist(i_values, bins=10, alpha=0.8, color='green', 
                        label=f'I-æœ€é©æ–°è¦ç‚¹ ({len(i_new_indices)})', density=True)
        else:
            # ES: Mostrar ambos (comportamiento original) | EN: Show both (original behavior) | JA: ä¸¡æ–¹è¡¨ç¤ºï¼ˆå…ƒã®æŒ™å‹•ï¼‰
            d_new_indices = [idx for idx in d_indices if idx not in existing_indices]
            if d_new_indices:
                d_values = candidate_df.iloc[d_new_indices][var_name]
                plt.hist(d_values, bins=10, alpha=0.8, color='red', 
                        label=f'D-æœ€é©æ–°è¦ç‚¹ ({len(d_new_indices)})', density=True)

            i_new_indices = [idx for idx in i_indices if idx not in existing_indices]
            if i_new_indices:
                i_values = candidate_df.iloc[i_new_indices][var_name]
                plt.hist(i_values, bins=10, alpha=0.8, color='green', 
                        label=f'I-æœ€é©æ–°è¦ç‚¹ ({len(i_new_indices)})', density=True)

        # ES: Ajustar tÃ­tulo segÃºn el tipo de optimizaciÃ³n | EN: Adjust title based on optimization type | JA: æœ€é©åŒ–ã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦ã‚¿ã‚¤ãƒˆãƒ«èª¿æ•´
        if optimization_type in ["d", "D", "d_optimal"]:
            plt.title(f'{var_name}ã®åˆ†å¸ƒ (Dæœ€é©åŒ–)', fontsize=12, weight='bold')
        elif optimization_type in ["i", "I", "i_optimal"]:
            plt.title(f'{var_name}ã®åˆ†å¸ƒ (Iæœ€é©åŒ–)', fontsize=12, weight='bold')
        else:
            plt.title(f'{var_name}ã®åˆ†å¸ƒ', fontsize=12, weight='bold')
            
        plt.xlabel(var_name)
        plt.ylabel('å¯†åº¦')
        plt.legend(fontsize=8)
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        
        # ES: Guardar imagen individual con sufijo segÃºn el tipo de optimizaciÃ³n
        # EN: Save per-feature histogram with suffix based on optimization type
        # JA: æœ€é©åŒ–ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸæ¥å°¾è¾ã§ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã‚’ä¿å­˜
        safe_var_name = str(var_name).replace('/', '_').replace(' ', '_')
        if optimization_type in ["d", "D", "d_optimal"]:
            hist_path = os.path.join(output_folder, f"hist_D_{safe_var_name}.png")
        elif optimization_type in ["i", "I", "i_optimal"]:
            hist_path = os.path.join(output_folder, f"hist_I_{safe_var_name}.png")
        else:
            hist_path = os.path.join(output_folder, f"hist_{safe_var_name}.png")
            
        plt.savefig(hist_path, dpi=300, bbox_inches='tight')
        plt.close()
        image_paths.append(hist_path)
        print(f"âœ… ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ä¿å­˜å®Œäº†: {hist_path}")
    print(f"âœ… ç‰¹å¾´é‡åˆ†å¸ƒã®å¯è¦–åŒ–å®Œäº† ({len(image_paths)} ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ )")
    return image_paths

def visualize_separate_dimension_reduction(X_scaled, d_indices, i_indices, existing_indices, variable_names, output_folder, optimization_type="both", selected_d_df=None, selected_i_df=None):
    """ES: ğŸ“ˆ VisualizaciÃ³n de reducciÃ³n de dimensionalidad separada (PCA y UMAP individuales) con nÃºmeros de muestra
    EN: ğŸ“ˆ Separate dimensionality-reduction visualization (individual PCA and UMAP) with sample numbers
    JA: ğŸ“ˆ æ¬¡å…ƒå‰Šæ¸›å¯è¦–åŒ–ï¼ˆPCA/UMAPã‚’å€‹åˆ¥ï¼‰ã‚µãƒ³ãƒ—ãƒ«ç•ªå·ä»˜ã
    """
    print(f"\nğŸ“ˆ æ¬¡å…ƒå‰Šæ¸›å¯è¦–åŒ–é–‹å§‹... (æœ€é©åŒ–ã‚¿ã‚¤ãƒ—: {optimization_type})")
    
    image_paths = []
    
    try:
        import umap
        
        # ES: ParÃ¡metros UMAP optimizados | EN: Tuned UMAP parameters | JA: æœ€é©åŒ–æ¸ˆã¿UMAPãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        best_params = {"n_neighbors": 15, "min_dist": 0.1}
        
        # ES: Ejecutar UMAP | EN: Run UMAP | JA: UMAPã‚’å®Ÿè¡Œ
        print(f"ğŸ”§ UMAPå®Ÿè¡Œä¸­...")
        reducer = umap.UMAP(
            n_neighbors=best_params["n_neighbors"], 
            min_dist=best_params["min_dist"], 
            n_components=2, 
            random_state=42,
            verbose=False
        )
        
        start_time = time.time()
        reduced_umap = reducer.fit_transform(X_scaled)
        umap_time = time.time() - start_time
        print(f"â±ï¸ UMAPå®Ÿè¡Œæ™‚é–“: {umap_time:.2f} ç§’")
        
        # ES: Ejecutar PCA | EN: Run PCA | JA: PCAã‚’å®Ÿè¡Œ
        pca = PCA(n_components=2, random_state=42)
        reduced_pca = pca.fit_transform(X_scaled)
        
        # ES: === GRÃFICO PCA SEPARADO === | EN: === Separate PCA plot === | JA: === PCAã‚°ãƒ©ãƒ•ï¼ˆå€‹åˆ¥ï¼‰ ===
        plt.figure(figsize=(12, 8))
        
        # ES: Todos los candidatos (fondo) | EN: All candidates (background) | JA: å…¨å€™è£œï¼ˆèƒŒæ™¯ï¼‰
        plt.scatter(reduced_pca[:, 0], reduced_pca[:, 1], alpha=0.2, s=8, color='lightgray', label='å€™è£œç‚¹')
        
        # ES: Puntos experimentales existentes | EN: Existing experimental points | JA: æ—¢å­˜å®Ÿé¨“ç‚¹
        if existing_indices:
            existing_pca = reduced_pca[existing_indices]
            plt.scatter(existing_pca[:, 0], existing_pca[:, 1], 
                       s=120, color='blue', alpha=0.9, marker='o', 
                       edgecolors='navy', linewidth=2, zorder=10,
                       label=f'æ—¢å­˜ç‚¹ ({len(existing_indices)})')
        
        # ES: Mostrar solo los datos relevantes segÃºn el tipo de optimizaciÃ³n | EN: Show only data relevant to the optimization type | JA: æœ€é©åŒ–ã‚¿ã‚¤ãƒ—ã«é–¢é€£ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã®ã¿è¡¨ç¤º
        if optimization_type in ["d", "D", "d_optimal"]:
            # ES: Solo mostrar datos D-Ã³ptimo | EN: Show only D-optimal data | JA: Dæœ€é©ãƒ‡ãƒ¼ã‚¿ã®ã¿è¡¨ç¤º
            d_new = [idx for idx in d_indices if idx not in existing_indices]
            if d_new:
                d_pca = reduced_pca[d_new]
                plt.scatter(d_pca[:, 0], d_pca[:, 1], 
                           s=100, marker='x', color='red', linewidth=3, 
                           zorder=8, label=f'D-æœ€é©æ–°è¦ç‚¹ ({len(d_new)})')
                
                # ES: AÃ±adir nÃºmeros de muestra en puntos D-Ã³ptimo | EN: Add sample numbers on D-optimal points | JA: Dæœ€é©ç‚¹ã«ã‚µãƒ³ãƒ—ãƒ«ç•ªå·ã‚’ä»˜ä¸
                if selected_d_df is not None and 'No.' in selected_d_df.columns:
                    for i, (x, y) in enumerate(d_pca):
                        sample_num = selected_d_df.iloc[i]['No.']
                        plt.annotate(f'{sample_num}', (x, y), xytext=(5, 5), 
                                   textcoords='offset points', fontsize=10, 
                                   color='red', weight='bold', zorder=12)
        elif optimization_type in ["i", "I", "i_optimal"]:
            # ES: Solo mostrar datos I-Ã³ptimo | EN: Show only I-optimal data | JA: Iæœ€é©ãƒ‡ãƒ¼ã‚¿ã®ã¿è¡¨ç¤º
            i_new = [idx for idx in i_indices if idx not in existing_indices]
            if i_new:
                i_pca = reduced_pca[i_new]
                plt.scatter(i_pca[:, 0], i_pca[:, 1], 
                           s=100, marker='^', color='green', 
                           zorder=8, label=f'I-æœ€é©æ–°è¦ç‚¹ ({len(i_new)})')
                
                # ES: AÃ±adir nÃºmeros de muestra en puntos I-Ã³ptimo | EN: Add sample numbers on I-optimal points | JA: Iæœ€é©ç‚¹ã«ã‚µãƒ³ãƒ—ãƒ«ç•ªå·ã‚’ä»˜ä¸
                if selected_i_df is not None and 'No.' in selected_i_df.columns:
                    for i, (x, y) in enumerate(i_pca):
                        sample_num = selected_i_df.iloc[i]['No.']
                        plt.annotate(f'{sample_num}', (x, y), xytext=(5, 5), 
                                   textcoords='offset points', fontsize=10, 
                                   color='green', weight='bold', zorder=12)
        else:
            # ES: Mostrar ambos (comportamiento original) | EN: Show both (original behavior) | JA: ä¸¡æ–¹è¡¨ç¤ºï¼ˆå…ƒã®æŒ™å‹•ï¼‰
            d_new = [idx for idx in d_indices if idx not in existing_indices]
            if d_new:
                d_pca = reduced_pca[d_new]
                plt.scatter(d_pca[:, 0], d_pca[:, 1], 
                           s=100, marker='x', color='red', linewidth=3, 
                           zorder=8, label=f'D-æœ€é©æ–°è¦ç‚¹ ({len(d_new)})')
                
                # ES: AÃ±adir nÃºmeros de muestra en puntos D-Ã³ptimo | EN: Add sample numbers on D-optimal points | JA: Dæœ€é©ç‚¹ã«ã‚µãƒ³ãƒ—ãƒ«ç•ªå·ã‚’ä»˜ä¸
                if selected_d_df is not None and 'No.' in selected_d_df.columns:
                    for i, (x, y) in enumerate(d_pca):
                        sample_num = selected_d_df.iloc[i]['No.']
                        plt.annotate(f'{sample_num}', (x, y), xytext=(5, 5), 
                                   textcoords='offset points', fontsize=10, 
                                   color='red', weight='bold', zorder=12)
            
            i_new = [idx for idx in i_indices if idx not in existing_indices]
            if i_new:
                i_pca = reduced_pca[i_new]
                plt.scatter(i_pca[:, 0], i_pca[:, 1], 
                           s=100, marker='^', color='green', 
                           zorder=8, label=f'I-æœ€é©æ–°è¦ç‚¹ ({len(i_new)})')
                
                # ES: AÃ±adir nÃºmeros de muestra en puntos I-Ã³ptimo | EN: Add sample numbers on I-optimal points | JA: Iæœ€é©ç‚¹ã«ã‚µãƒ³ãƒ—ãƒ«ç•ªå·ã‚’ä»˜ä¸
                if selected_i_df is not None and 'No.' in selected_i_df.columns:
                    for i, (x, y) in enumerate(i_pca):
                        sample_num = selected_i_df.iloc[i]['No.']
                        plt.annotate(f'{sample_num}', (x, y), xytext=(5, 5), 
                                   textcoords='offset points', fontsize=10, 
                                   color='green', weight='bold', zorder=12)
        
        # ES: Ajustar tÃ­tulo segÃºn el tipo de optimizaciÃ³n | EN: Adjust title based on optimization type | JA: æœ€é©åŒ–ã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦ã‚¿ã‚¤ãƒˆãƒ«èª¿æ•´
        if optimization_type in ["d", "D", "d_optimal"]:
            plt.title('ä¸»æˆåˆ†åˆ†æ (PCA) æ¬¡å…ƒå‰Šæ¸› - Dæœ€é©åŒ–', fontsize=16, weight='bold')
        elif optimization_type in ["i", "I", "i_optimal"]:
            plt.title('ä¸»æˆåˆ†åˆ†æ (PCA) æ¬¡å…ƒå‰Šæ¸› - Iæœ€é©åŒ–', fontsize=16, weight='bold')
        else:
            plt.title('ä¸»æˆåˆ†åˆ†æ (PCA) æ¬¡å…ƒå‰Šæ¸›', fontsize=16, weight='bold')
            
        plt.xlabel(f'ä¸»æˆåˆ†1 ({pca.explained_variance_ratio_[0]:.1%})')
        plt.ylabel(f'ä¸»æˆåˆ†2 ({pca.explained_variance_ratio_[1]:.1%})')
        plt.legend(fontsize=12)
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        
        # ES: Guardar PCA con sufijo segÃºn el tipo de optimizaciÃ³n | EN: Save PCA with suffix based on optimization type | JA: æœ€é©åŒ–ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸæ¥å°¾è¾ã§PCAã‚’ä¿å­˜
        if optimization_type in ["d", "D", "d_optimal"]:
            pca_path = os.path.join(output_folder, "reduccion_dimensionalidad_pca_D.png")
        elif optimization_type in ["i", "I", "i_optimal"]:
            pca_path = os.path.join(output_folder, "reduccion_dimensionalidad_pca_I.png")
        else:
            pca_path = os.path.join(output_folder, "reduccion_dimensionalidad_pca.png")
            
        plt.savefig(pca_path, dpi=300, bbox_inches='tight')
        plt.close()
        image_paths.append(pca_path)
        print(f"âœ… PCAä¿å­˜å®Œäº†: {pca_path}")
        
        # ES: === GRÃFICO UMAP SEPARADO === | EN: === Separate UMAP plot === | JA: === UMAPã‚°ãƒ©ãƒ•ï¼ˆå€‹åˆ¥ï¼‰ ===
        plt.figure(figsize=(12, 8))
        
        # ES: Todos los candidatos (fondo) | EN: All candidates (background) | JA: å…¨å€™è£œï¼ˆèƒŒæ™¯ï¼‰
        plt.scatter(reduced_umap[:, 0], reduced_umap[:, 1], alpha=0.2, s=8, color='lightgray', label='å€™è£œç‚¹')
        
        # ES: Puntos experimentales existentes | EN: Existing experimental points | JA: æ—¢å­˜å®Ÿé¨“ç‚¹
        if existing_indices:
            existing_umap = reduced_umap[existing_indices]
            plt.scatter(existing_umap[:, 0], existing_umap[:, 1], 
                       s=120, color='blue', alpha=0.9, marker='o', 
                       edgecolors='navy', linewidth=2, zorder=10,
                       label=f'æ—¢å­˜ç‚¹ ({len(existing_indices)})')
            
            # ES: Mostrar nÃºmeros en puntos existentes (primeros 10) | EN: Show numbers on existing points (first 10) | JA: æ—¢å­˜ç‚¹ã«ç•ªå·è¡¨ç¤ºï¼ˆå…ˆé ­10ç‚¹ï¼‰
            for i, (x, y) in enumerate(existing_umap[:min(10, len(existing_umap))]):
                plt.annotate(f'{i+1}', (x, y), xytext=(3, 3), 
                           textcoords='offset points', fontsize=8, 
                           color='darkblue', weight='bold', zorder=11)
        
        # ES: Mostrar solo los datos relevantes segÃºn el tipo de optimizaciÃ³n | EN: Show only data relevant to the optimization type | JA: æœ€é©åŒ–ã‚¿ã‚¤ãƒ—ã«é–¢é€£ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã®ã¿è¡¨ç¤º
        if optimization_type in ["d", "D", "d_optimal"]:
            # ES: Solo mostrar datos D-Ã³ptimo | EN: Show only D-optimal data | JA: Dæœ€é©ãƒ‡ãƒ¼ã‚¿ã®ã¿è¡¨ç¤º
            d_new = [idx for idx in d_indices if idx not in existing_indices]
            if d_new:
                d_umap = reduced_umap[d_new]
                plt.scatter(d_umap[:, 0], d_umap[:, 1], 
                           s=100, marker='x', color='red', linewidth=3, 
                           zorder=8, label=f'D-æœ€é©æ–°è¦ç‚¹ ({len(d_new)})')
                
                # ES: AÃ±adir nÃºmeros de muestra en puntos D-Ã³ptimo | EN: Add sample numbers on D-optimal points | JA: Dæœ€é©ç‚¹ã«ã‚µãƒ³ãƒ—ãƒ«ç•ªå·ã‚’ä»˜ä¸
                if selected_d_df is not None and 'No.' in selected_d_df.columns:
                    for i, (x, y) in enumerate(d_umap):
                        sample_num = selected_d_df.iloc[i]['No.']
                        plt.annotate(f'{sample_num}', (x, y), xytext=(5, 5), 
                                   textcoords='offset points', fontsize=10, 
                                   color='red', weight='bold', zorder=12)
        elif optimization_type in ["i", "I", "i_optimal"]:
            # ES: Solo mostrar datos I-Ã³ptimo | EN: Show only I-optimal data | JA: Iæœ€é©ãƒ‡ãƒ¼ã‚¿ã®ã¿è¡¨ç¤º
            i_new = [idx for idx in i_indices if idx not in existing_indices]
            if i_new:
                i_umap = reduced_umap[i_new]
                plt.scatter(i_umap[:, 0], i_umap[:, 1], 
                           s=100, marker='^', color='green', 
                           zorder=8, label=f'I-æœ€é©æ–°è¦ç‚¹ ({len(i_new)})')
                
                # ES: AÃ±adir nÃºmeros de muestra en puntos I-Ã³ptimo | EN: Add sample numbers on I-optimal points | JA: Iæœ€é©ç‚¹ã«ã‚µãƒ³ãƒ—ãƒ«ç•ªå·ã‚’ä»˜ä¸
                if selected_i_df is not None and 'No.' in selected_i_df.columns:
                    for i, (x, y) in enumerate(i_umap):
                        sample_num = selected_i_df.iloc[i]['No.']
                        plt.annotate(f'{sample_num}', (x, y), xytext=(5, 5), 
                                   textcoords='offset points', fontsize=10, 
                                   color='green', weight='bold', zorder=12)
        else:
            # ES: Mostrar ambos (comportamiento original) | EN: Show both (original behavior) | JA: ä¸¡æ–¹è¡¨ç¤ºï¼ˆå…ƒã®æŒ™å‹•ï¼‰
            d_new = [idx for idx in d_indices if idx not in existing_indices]
            if d_new:
                d_umap = reduced_umap[d_new]
                plt.scatter(d_umap[:, 0], d_umap[:, 1], 
                           s=100, marker='x', color='red', linewidth=3, 
                           zorder=8, label=f'D-æœ€é©æ–°è¦ç‚¹ ({len(d_new)})')
                
                # ES: AÃ±adir nÃºmeros de muestra en puntos D-Ã³ptimo | EN: Add sample numbers on D-optimal points | JA: Dæœ€é©ç‚¹ã«ã‚µãƒ³ãƒ—ãƒ«ç•ªå·ã‚’ä»˜ä¸
                if selected_d_df is not None and 'No.' in selected_d_df.columns:
                    for i, (x, y) in enumerate(d_umap):
                        sample_num = selected_d_df.iloc[i]['No.']
                        plt.annotate(f'{sample_num}', (x, y), xytext=(5, 5), 
                                   textcoords='offset points', fontsize=10, 
                                   color='red', weight='bold', zorder=12)
            
            i_new = [idx for idx in i_indices if idx not in existing_indices]
            if i_new:
                i_umap = reduced_umap[i_new]
                plt.scatter(i_umap[:, 0], i_umap[:, 1], 
                           s=100, marker='^', color='green', 
                           zorder=8, label=f'I-æœ€é©æ–°è¦ç‚¹ ({len(i_new)})')
                
                # ES: AÃ±adir nÃºmeros de muestra en puntos I-Ã³ptimo | EN: Add sample numbers on I-optimal points | JA: Iæœ€é©ç‚¹ã«ã‚µãƒ³ãƒ—ãƒ«ç•ªå·ã‚’ä»˜ä¸
                if selected_i_df is not None and 'No.' in selected_i_df.columns:
                    for i, (x, y) in enumerate(i_umap):
                        sample_num = selected_i_df.iloc[i]['No.']
                        plt.annotate(f'{sample_num}', (x, y), xytext=(5, 5), 
                                   textcoords='offset points', fontsize=10, 
                                   color='green', weight='bold', zorder=12)
        
        # ES: Ajustar tÃ­tulo segÃºn el tipo de optimizaciÃ³n | EN: Adjust title based on optimization type | JA: æœ€é©åŒ–ã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦ã‚¿ã‚¤ãƒˆãƒ«èª¿æ•´
        if optimization_type in ["d", "D", "d_optimal"]:
            plt.title('UMAP æ¬¡å…ƒå‰Šæ¸› - Dæœ€é©åŒ–', fontsize=16, weight='bold')
        elif optimization_type in ["i", "I", "i_optimal"]:
            plt.title('UMAP æ¬¡å…ƒå‰Šæ¸› - Iæœ€é©åŒ–', fontsize=16, weight='bold')
        else:
            plt.title('UMAP æ¬¡å…ƒå‰Šæ¸›', fontsize=16, weight='bold')
            
        plt.xlabel('UMAP 1')
        plt.ylabel('UMAP 2')
        plt.legend(fontsize=12)
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        
        # ES: Guardar UMAP con sufijo segÃºn el tipo de optimizaciÃ³n | EN: Save UMAP with suffix based on optimization type | JA: æœ€é©åŒ–ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸæ¥å°¾è¾ã§UMAPã‚’ä¿å­˜
        if optimization_type in ["d", "D", "d_optimal"]:
            umap_path = os.path.join(output_folder, "reduccion_dimensionalidad_umap_D.png")
        elif optimization_type in ["i", "I", "i_optimal"]:
            umap_path = os.path.join(output_folder, "reduccion_dimensionalidad_umap_I.png")
        else:
            umap_path = os.path.join(output_folder, "reduccion_dimensionalidad_umap.png")
            
        plt.savefig(umap_path, dpi=300, bbox_inches='tight')
        plt.close()
        image_paths.append(umap_path)
        print(f"âœ… UMAPä¿å­˜å®Œäº†: {umap_path}")
        
        print(f"âœ… æ¬¡å…ƒå‰Šæ¸›å¯è¦–åŒ–å®Œäº† ({len(image_paths)} ã‚°ãƒ©ãƒ•)")
        return image_paths
        
    except ImportError:
        print("âŒ UMAPæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« - PCAã®ã¿è¡¨ç¤º")
        # ES: Solo PCA como respaldo | EN: Fallback to PCA only | JA: ä»£æ›¿ã¨ã—ã¦PCAã®ã¿å®Ÿè¡Œ
        pca = PCA(n_components=2, random_state=42)
        reduced_pca = pca.fit_transform(X_scaled)
        
        plt.figure(figsize=(12, 8))
        plt.scatter(reduced_pca[:, 0], reduced_pca[:, 1], alpha=0.2, s=8, color='lightgray', label='å€™è£œç‚¹')
        
        if existing_indices:
            existing_pca = reduced_pca[existing_indices]
            plt.scatter(existing_pca[:, 0], existing_pca[:, 1], 
                       s=120, color='blue', alpha=0.9, marker='o', 
                       edgecolors='navy', linewidth=2, zorder=10,
                       label=f'æ—¢å­˜ç‚¹ ({len(existing_indices)})')
        
        # ES: Mostrar solo los datos relevantes segÃºn el tipo de optimizaciÃ³n | EN: Show only data relevant to the optimization type | JA: æœ€é©åŒ–ã‚¿ã‚¤ãƒ—ã«é–¢é€£ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã®ã¿è¡¨ç¤º
        if optimization_type in ["d", "D", "d_optimal"]:
            d_new = [idx for idx in d_indices if idx not in existing_indices]
            if d_new:
                d_pca = reduced_pca[d_new]
                plt.scatter(d_pca[:, 0], d_pca[:, 1], 
                           s=100, marker='x', color='red', linewidth=3, 
                           zorder=8, label=f'D-æœ€é©æ–°è¦ç‚¹ ({len(d_new)})')
                
                # ES: AÃ±adir nÃºmeros de muestra en puntos D-Ã³ptimo | EN: Add sample numbers on D-optimal points | JA: Dæœ€é©ç‚¹ã«ã‚µãƒ³ãƒ—ãƒ«ç•ªå·ã‚’ä»˜ä¸
                if selected_d_df is not None and 'No.' in selected_d_df.columns:
                    for i, (x, y) in enumerate(d_pca):
                        sample_num = selected_d_df.iloc[i]['No.']
                        plt.annotate(f'{sample_num}', (x, y), xytext=(5, 5), 
                                   textcoords='offset points', fontsize=10, 
                                   color='red', weight='bold', zorder=12)
        elif optimization_type in ["i", "I", "i_optimal"]:
            i_new = [idx for idx in i_indices if idx not in existing_indices]
            if i_new:
                i_pca = reduced_pca[i_new]
                plt.scatter(i_pca[:, 0], i_pca[:, 1], 
                           s=100, marker='^', color='green', 
                           zorder=8, label=f'I-æœ€é©æ–°è¦ç‚¹ ({len(i_new)})')
                
                # ES: AÃ±adir nÃºmeros de muestra en puntos I-Ã³ptimo | EN: Add sample numbers on I-optimal points | JA: Iæœ€é©ç‚¹ã«ã‚µãƒ³ãƒ—ãƒ«ç•ªå·ã‚’ä»˜ä¸
                if selected_i_df is not None and 'No.' in selected_i_df.columns:
                    for i, (x, y) in enumerate(i_pca):
                        sample_num = selected_i_df.iloc[i]['No.']
                        plt.annotate(f'{sample_num}', (x, y), xytext=(5, 5), 
                                   textcoords='offset points', fontsize=10, 
                                   color='green', weight='bold', zorder=12)
        
        # ES: Ajustar tÃ­tulo segÃºn el tipo de optimizaciÃ³n | EN: Adjust title based on optimization type | JA: æœ€é©åŒ–ã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦ã‚¿ã‚¤ãƒˆãƒ«èª¿æ•´
        if optimization_type in ["d", "D", "d_optimal"]:
            plt.title('ä¸»æˆåˆ†åˆ†æ (PCA) æ¬¡å…ƒå‰Šæ¸› - Dæœ€é©åŒ–', fontsize=16, weight='bold')
        elif optimization_type in ["i", "I", "i_optimal"]:
            plt.title('ä¸»æˆåˆ†åˆ†æ (PCA) æ¬¡å…ƒå‰Šæ¸› - Iæœ€é©åŒ–', fontsize=16, weight='bold')
        else:
            plt.title('ä¸»æˆåˆ†åˆ†æ (PCA) æ¬¡å…ƒå‰Šæ¸›', fontsize=16, weight='bold')
            
        plt.xlabel(f'ä¸»æˆåˆ†1 ({pca.explained_variance_ratio_[0]:.1%})')
        plt.ylabel(f'ä¸»æˆåˆ†2 ({pca.explained_variance_ratio_[1]:.1%})')
        plt.legend(fontsize=12)
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        
        # ES: Guardar PCA con sufijo segÃºn el tipo de optimizaciÃ³n | EN: Save PCA with suffix based on optimization type | JA: æœ€é©åŒ–ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸæ¥å°¾è¾ã§PCAã‚’ä¿å­˜
        if optimization_type in ["d", "D", "d_optimal"]:
            pca_path = os.path.join(output_folder, "reduccion_dimensionalidad_pca_D.png")
        elif optimization_type in ["i", "I", "i_optimal"]:
            pca_path = os.path.join(output_folder, "reduccion_dimensionalidad_pca_I.png")
        else:
            pca_path = os.path.join(output_folder, "reduccion_dimensionalidad_pca.png")
            
        plt.savefig(pca_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"âœ… PCAä¿å­˜å®Œäº†: {pca_path}")
        return [pca_path]

def visualize_umap_enhanced(X_scaled, d_indices, i_indices, existing_indices, variable_names, output_folder, optimization_type="both", selected_d_df=None, selected_i_df=None):
    """ES: ğŸ“ˆ VisualizaciÃ³n UMAP mejorada (mantiene compatibilidad)
    EN: ğŸ“ˆ Enhanced UMAP visualization (keeps backward compatibility)
    JA: ğŸ“ˆ æ”¹è‰¯ç‰ˆUMAPå¯è¦–åŒ–ï¼ˆäº’æ›æ€§ç¶­æŒï¼‰
    """
    # ES: Usar la nueva funciÃ³n separada | EN: Use the new separated function | JA: æ–°ã—ã„åˆ†é›¢é–¢æ•°ã‚’ä½¿ç”¨
    return visualize_separate_dimension_reduction(X_scaled, d_indices, i_indices, existing_indices, variable_names, output_folder, optimization_type, selected_d_df, selected_i_df)

def get_project_name(sample_file):
    return os.path.splitext(os.path.basename(sample_file))[0]

def get_incremental_folder(base_dir, prefix):
    today = datetime.now().strftime('%Y%m%d')
    i = 1
    while True:
        folder = os.path.join(base_dir, f"{prefix}_{today}_{i:03d}")
        if not os.path.exists(folder):
            return folder
        i += 1

def run_integrated_optimizer(sample_file, existing_data_file=None, output_folder=".", num_experiments=15, 
                           sample_size=None, enable_hyperparameter_tuning=True, force_reoptimization=False, optimization_type="both"):
    """
    ES: Ejecuta el optimizador integrado D-Ã³ptimo + I-Ã³ptimo.
    EN: Run the integrated D-optimal + I-optimal optimizer.
    JA: Dæœ€é© + Iæœ€é© ã®çµ±åˆã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã‚’å®Ÿè¡Œã€‚

    ES: ParÃ¡metros:
    EN: Parameters:
    JA: å¼•æ•°:

    - sample_file:
      ES: Excel con combinaciones de muestras (sample_combinations.xlsx)
      EN: Excel file with sample combinations (sample_combinations.xlsx)
      JA: ã‚µãƒ³ãƒ—ãƒ«çµ„åˆã›ã®Excelï¼ˆsample_combinations.xlsxï¼‰
    - existing_data_file:
      ES: Excel con datos experimentales existentes (opcional)
      EN: Excel file with existing experimental data (optional)
      JA: æ—¢å­˜å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã®Excelï¼ˆä»»æ„ï¼‰
    - output_folder:
      ES: Carpeta de salida de resultados
      EN: Results output folder
      JA: çµæœå‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€
    - num_experiments:
      ES: NÃºmero de experimentos a seleccionar
      EN: Number of experiments to select
      JA: é¸æŠå®Ÿé¨“æ•°
    - sample_size:
      ES: TamaÃ±o de muestreo para reducciÃ³n (opcional)
      EN: Sample size for candidate reduction (optional)
      JA: å‰Šæ¸›ç”¨ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºï¼ˆä»»æ„ï¼‰
    - enable_hyperparameter_tuning:
      ES: Habilitar optimizaciÃ³n de hiperparÃ¡metros de UMAP
      EN: Enable UMAP hyperparameter tuning
      JA: UMAPãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã‚’æœ‰åŠ¹åŒ–
    - force_reoptimization:
      ES: Forzar re-optimizaciÃ³n de hiperparÃ¡metros de UMAP
      EN: Force UMAP hyperparameter re-optimization
      JA: UMAPãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å†æœ€é©åŒ–ã‚’å¼·åˆ¶
    - optimization_type:
      ES: "d", "i" o "both" (tipo de optimizaciÃ³n)
      EN: "d", "i", or "both" (optimization type)
      JA: "d" / "i" / "both"ï¼ˆæœ€é©åŒ–ã‚¿ã‚¤ãƒ—ï¼‰
    """
    print("ğŸš€ åŒ–å­¦å®Ÿé¨“è¨ˆç”»ã‚·ã‚¹ãƒ†ãƒ  - çµ±åˆãƒãƒ¼ã‚¸ãƒ§ãƒ³")
    print("="*60)
    if optimization_type in ["d", "D", "d_optimal"]:
        print("ğŸ“Š Dæœ€é©åŒ–å°‚ç”¨ã‚°ãƒ©ãƒ•ç”Ÿæˆ")
    elif optimization_type in ["i", "I", "i_optimal"]:
        print("ğŸ“Š Iæœ€é©åŒ–å°‚ç”¨ã‚°ãƒ©ãƒ•ç”Ÿæˆ")
    else:
        print("ğŸ“Š ç‰¹å¾´é‡ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ï¼ˆè‰²åˆ†ã‘ï¼‰")
        print("ğŸ“ˆ æ¬¡å…ƒå‰Šæ¸›UMAPã®å¯è¦–åŒ–ï¼ˆå¼·åŒ–ç‰ˆï¼‰")
    print("="*60)

    # ES: Crear carpeta de salida directamente en output_folder
    # EN: Create the output folder directly under output_folder
    # JA: output_folder ç›´ä¸‹ã«å‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆ
    project_name = get_project_name(sample_file)
    di_folder = output_folder  # Use output_folder directly (no intermediate folder)
    os.makedirs(di_folder, exist_ok=True)

    # ES: Leer archivo de combinaciones de muestras
    # EN: Read sample combination file
    # JA: ã‚µãƒ³ãƒ—ãƒ«çµ„åˆã›ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
    print(f"\nğŸ“Š ã‚µãƒ³ãƒ—ãƒ«çµ„åˆã›ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    sample_ext = os.path.splitext(str(sample_file))[1].lower()
    full_df = pd.read_csv(sample_file, encoding="utf-8-sig") if sample_ext == ".csv" else pd.read_excel(sample_file)

    # ES: Usar SOLO 7 variables core para optimizaciÃ³n/visualizaciÃ³n (no incluir ãƒ–ãƒ©ã‚· one-hot ni ç·šæé•·)
    # EN: Use only 7 core variables for optimization/visualization (exclude ãƒ–ãƒ©ã‚· one-hot and ç·šæé•·)
    # JA: æœ€é©åŒ–/å¯è¦–åŒ–ã«ã¯ã‚³ã‚¢7å¤‰æ•°ã®ã¿ä½¿ç”¨ï¼ˆãƒ–ãƒ©ã‚· one-hotãƒ»ç·šæé•·ã¯å«ã‚ãªã„ï¼‰
    dir_col = "UPã‚«ãƒƒãƒˆ" if "UPã‚«ãƒƒãƒˆ" in full_df.columns else ("å›è»¢æ–¹å‘" if "å›è»¢æ–¹å‘" in full_df.columns else None)
    if dir_col is None:
        raise ValueError("âŒ Falta columna de direcciÃ³n: 'UPã‚«ãƒƒãƒˆ' o 'å›è»¢æ–¹å‘'")
    design_cols = ["å›è»¢é€Ÿåº¦", "é€ã‚Šé€Ÿåº¦", dir_col, "åˆ‡è¾¼é‡", "çªå‡ºé‡", "è¼‰ã›ç‡", "ãƒ‘ã‚¹æ•°"]
    missing = [c for c in design_cols if c not in full_df.columns]
    if missing:
        raise ValueError(f"âŒ Faltan columnas de diseÃ±o: {missing}")

    candidate_df = full_df[design_cols].copy()
    candidate_points = candidate_df.values
    variable_names = design_cols
    
    print(f"âœ… ã‚µãƒ³ãƒ—ãƒ«çµ„åˆã›ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å®Œäº†:")
    print(f"  - èª¬æ˜å¤‰æ•°æ•°: {len(variable_names)}")
    print(f"  - å€™è£œç‚¹æ•°: {len(candidate_points):,}")
    print(f"  - èª¬æ˜å¤‰æ•°: {variable_names}")

    # ES: Procesar datos experimentales existentes
    # EN: Process existing experimental data
    # JA: æ—¢å­˜å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†
    existing_indices = []
    if existing_data_file and os.path.exists(existing_data_file):
        print(f"\nğŸ” æ—¢å­˜å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿å‡¦ç†ä¸­...")
        
        # ES: Crear DataFrame de compatibilidad temporal para diseÃ±o
        # EN: Create a temporary compatibility DataFrame for the design table
        # JA: è¨­è¨ˆè¡¨ã¨ã®ä¸€æ™‚äº’æ›DataFrameã‚’ä½œæˆ
        design_df = pd.DataFrame({
            "èª¬æ˜å¤‰æ•°å": variable_names,
            "æœ€å°å€¤": [candidate_df[var].min() for var in variable_names],
            "æœ€å¤§å€¤": [candidate_df[var].max() for var in variable_names],
            "åˆ»ã¿å¹…": [1.0] * len(variable_names)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        })
        
        existing_data, available_vars = load_and_validate_existing_data(
            existing_data_file, design_df, verbose=True
        )

        if existing_data is not None and len(existing_data) > 0:
            existing_indices = match_existing_experiments_enhanced(
                candidate_points, existing_data, variable_names,
                tolerance_relative=1e-4,
                tolerance_absolute=1e-6,
                verbose=True
            )
        else:
            print("âŒ æ—¢å­˜å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿åˆ©ç”¨ä¸å¯")
    else:
        print("â„¹ï¸ æ—¢å­˜å®Ÿé¨“ãƒ•ã‚¡ã‚¤ãƒ«æŒ‡å®šãªã—ã¾ãŸã¯å­˜åœ¨ã—ãªã„")

    # === å€™è£œç‚¹å‰Šæ¸› if excede umbral ===
    original_candidate_count = len(candidate_points)
    should_reduce = len(candidate_points) > CANDIDATE_REDUCTION_THRESHOLD

    if should_reduce:
        max_candidates = sample_size if sample_size else MAX_REDUCED_CANDIDATES
        print(f"\nğŸ”„ å€™è£œç‚¹å‰Šæ¸›å®Ÿè¡Œ: {original_candidate_count:,} â†’ {max_candidates:,}")
        candidate_points, reduced_mapping = hierarchical_candidate_reduction(
            candidate_points, max_candidates, existing_indices
        )

        if existing_indices:
            existing_indices = [reduced_mapping.index(idx) for idx in existing_indices if idx in reduced_mapping]
            print(f"âœ… æ—¢å­˜å®Ÿé¨“ç‚¹ãƒãƒƒãƒ”ãƒ³ã‚°æ›´æ–°å®Œäº†: {len(existing_indices)} ä¿æŒ")

        # ES: Reducir tambiÃ©n el DF completo para que Ã­ndices coincidan
        # EN: Also reduce the full DF so indices match
        # JA: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒä¸€è‡´ã™ã‚‹ã‚ˆã†ãƒ•ãƒ«DFã‚‚å‰Šæ¸›
        try:
            full_df = full_df.iloc[reduced_mapping].reset_index(drop=True)
        except Exception:
            pass
        candidate_df = pd.DataFrame(candidate_points, columns=variable_names)

    print(f"\nâœ… æœ€çµ‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ:")
    print(f"  - æœ€çµ‚å€™è£œç‚¹æ•°: {len(candidate_points):,}")
    print(f"  - æ—¢å­˜å®Ÿé¨“ç‚¹æ•°: {len(existing_indices)}")
    print(f"  - æ—¢å­˜å®Ÿé¨“ç‚¹åˆ©ç”¨ç‡: {len(existing_indices)/num_experiments*100:.1f}%")

    # === å‰å‡¦ç†ãƒ‡ãƒ¼ã‚¿ ===
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(candidate_points)
    print(f"âœ… ãƒ‡ãƒ¼ã‚¿æ¨™æº–åŒ–å®Œäº†")

    # === Dæœ€é©è¨ˆç”» ===
    print(f"\nğŸ¯ Dæœ€é©è¨ˆç”»ã‚’å®Ÿè¡Œä¸­")
    d_indices, d_score = select_d_optimal_design_enhanced(
        X_scaled, existing_indices, num_experiments, verbose=VERBOSE
    )
    print(f"âœ… Dæœ€é©è¨ˆç”»å®Œäº†")
    print(f"  - æœ€çµ‚ã‚¹ã‚³ã‚¢: {d_score:.4f}")
    print(f"  - é¸æŠç‚¹æ•°: {len(d_indices)}")
    print(f"  - æ—¢å­˜ç‚¹æ•°: {len([i for i in d_indices if i in existing_indices])} ç‚¹")
    print(f"  - æ–°è¦ç‚¹æ•°: {len([i for i in d_indices if i not in existing_indices])} ç‚¹")

    # === Iæœ€é©è¨ˆç”» ===
    print(f"\nğŸ¯ Iæœ€é©è¨ˆç”»ã‚’å®Ÿè¡Œä¸­")
    i_indices = select_i_optimal_design(
        X_scaled, num_experiments, existing_indices
    )
    print(f"âœ… Iæœ€é©è¨ˆç”»å®Œäº†")
    print(f"  - é¸æŠç‚¹æ•°: {len(i_indices)}")
    print(f"  - æ—¢å­˜ç‚¹æ•°: {len([i for i in i_indices if i in existing_indices])} ç‚¹")
    print(f"  - æ–°è¦ç‚¹æ•°: {len([i for i in i_indices if i not in existing_indices])} ç‚¹")

    # === çµæœå‡¦ç† (æ–°è¦ç‚¹ã®ã¿) ===
    d_new_indices = [idx for idx in d_indices if idx not in existing_indices]
    i_new_indices = [idx for idx in i_indices if idx not in existing_indices]

    # Exportar DF completo (incluye A13/A11/A21/A32, ç·šæé•·, etc.), pero optimizar con candidate_df (solo core)
    selected_d_df = full_df.iloc[d_new_indices].copy() if d_new_indices else pd.DataFrame()
    selected_i_df = full_df.iloc[i_new_indices].copy() if i_new_indices else pd.DataFrame()

    print(f"\nğŸ“Š é¸æŠçµæœã‚µãƒãƒªãƒ¼:")
    print(f"  - æ—¢å­˜å®Ÿé¨“ç‚¹åˆ©ç”¨: {len(existing_indices)} ç‚¹")
    print(f"  - Dæœ€é©æ–°è¦é¸æŠ: {len(d_new_indices)} ç‚¹")
    print(f"  - Iæœ€é©æ–°è¦é¸æŠ: {len(i_new_indices)} ç‚¹")
    print(f"  - Dæœ€é©å…¨ä½“: {len(d_indices)} ç‚¹")
    print(f"  - Iæœ€é©å…¨ä½“: {len(i_indices)} ç‚¹")

    # NOTE: mantener nombres originales (é¢ç²—åº¦(Ra)å‰/å¾Œ) para compatibilidad con reconocimiento/export en GUI

    # === å¾Œå‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹æº–å‚™ ===
    d_path = os.path.join(di_folder, "D_optimal_æ–°è¦å®Ÿé¨“ç‚¹.xlsx")
    i_path = os.path.join(di_folder, "Iæœ€é©åŒ–_æ–°è¦å®Ÿé¨“ç‚¹.xlsx")
    all_d_path = os.path.join(di_folder, "Dæœ€é©åŒ–_å…¨å®Ÿé¨“ç‚¹.xlsx")
    all_i_path = os.path.join(di_folder, "Iæœ€é©åŒ–_å…¨å®Ÿé¨“ç‚¹.xlsx")
    candidate_path = os.path.join(di_folder, "å€™è£œç‚¹ä¸€è¦§_v2.xlsx")

    # === å¯è¦–åŒ– ===
    print(f"\nğŸ“Š ç‰¹å¾´é‡åˆ†å¸ƒã®å¯è¦–åŒ–é–‹å§‹...")
    
    # ç‰¹å¾´é‡ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ  (1å¤‰æ•°ã”ã¨) - æœ€é©åŒ–ã‚¿ã‚¤ãƒ—ã‚’æŒ‡å®š
    hist_paths = visualize_feature_histograms(candidate_df, d_indices, i_indices, existing_indices, variable_names, di_folder, optimization_type)
    
    # æ¬¡å…ƒå‰Šæ¸›UMAP - æœ€é©åŒ–ã‚¿ã‚¤ãƒ—ã‚’æŒ‡å®š
    umap_path = visualize_umap_enhanced(X_scaled, d_indices, i_indices, existing_indices, variable_names, di_folder, optimization_type)

    print(f"\nğŸ‰ åŒ–å­¦å®Ÿé¨“è¨ˆç”»ã‚·ã‚¹ãƒ†ãƒ ï¼ˆçµ±åˆç‰ˆï¼‰å®Œäº†")
    print("="*60)
    if optimization_type in ["d", "D", "d_optimal"]:
        print("âœ… Dæœ€é©åŒ–å°‚ç”¨ã‚°ãƒ©ãƒ•ç”Ÿæˆå®Œäº†")
    elif optimization_type in ["i", "I", "i_optimal"]:
        print("âœ… Iæœ€é©åŒ–å°‚ç”¨ã‚°ãƒ©ãƒ•ç”Ÿæˆå®Œäº†")
    else:
        print("âœ… æ—¢å­˜å®Ÿé¨“ç‚¹ã‚’æ´»ç”¨ã—ãŸæœ€é©å®Ÿé¨“è¨ˆç”»å®Œäº†")
        print("ğŸ“Š å¯è¦–åŒ–: ç‰¹å¾´é‡åˆ†å¸ƒãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ  + æ¬¡å…ƒå‰Šæ¸›UMAP")
    print("ğŸ’¾ Excelãƒ•ã‚¡ã‚¤ãƒ«ã¯OKãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ãŸæ™‚ã«ä¿å­˜ã•ã‚Œã¾ã™")
    print("="*60)

    # ES: AÃ±adir DåŸºæº–å€¤ solo si d_score estÃ¡ definido | EN: Add DåŸºæº–å€¤ only if d_score is defined | JA: d_score ãŒå®šç¾©ã•ã‚Œã¦ã„ã‚‹å ´åˆã®ã¿ DåŸºæº–å€¤ ã‚’è¿½åŠ 
    if not selected_d_df.empty and 'd_score' in locals():
        selected_d_df['No.'] = range(1, len(selected_d_df) + 1)
        if 'ãƒ‘ã‚¹æ•°' in selected_d_df.columns:
            insert_at = selected_d_df.columns.get_loc('ãƒ‘ã‚¹æ•°') + 1
        else:
            insert_at = len(selected_d_df.columns)
        selected_d_df.insert(insert_at, 'DåŸºæº–å€¤', d_score)
        cols = ['No.'] + [c for c in selected_d_df.columns if c != 'No.']
        selected_d_df = selected_d_df[cols]
    # ES: AÃ±adir IåŸºæº–å€¤ (placeholder) | EN: Add IåŸºæº–å€¤ (placeholder) | JA: IåŸºæº–å€¤ ã‚’è¿½åŠ ï¼ˆãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ï¼‰
    if not selected_i_df.empty:
        selected_i_df['No.'] = range(1, len(selected_i_df) + 1)
        if 'ãƒ‘ã‚¹æ•°' in selected_i_df.columns:
            insert_at = selected_i_df.columns.get_loc('ãƒ‘ã‚¹æ•°') + 1
        else:
            insert_at = len(selected_i_df.columns)
        selected_i_df.insert(insert_at, 'IåŸºæº–å€¤', '')  # Placeholder value
        cols = ['No.'] + [c for c in selected_i_df.columns if c != 'No.']
        selected_i_df = selected_i_df[cols]

    # ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã‚’di_folderã«ä¿å­˜
    hist_paths = visualize_feature_histograms(candidate_df, d_indices, i_indices, existing_indices, variable_names, di_folder, optimization_type)
    # æ¬¡å…ƒå‰Šæ¸›ã‚°ãƒ©ãƒ•ã‚’å€‹åˆ¥ã«ä¿å­˜ (PCA + UMAP) - ã‚µãƒ³ãƒ—ãƒ«ç•ªå·ä»˜ã
    dimension_paths = visualize_separate_dimension_reduction(X_scaled, d_indices, i_indices, existing_indices, variable_names, di_folder, optimization_type, selected_d_df, selected_i_df)
    
    # NO æœ€é©åŒ–ä¸­ã®Excelä¿å­˜, ãƒ«ãƒ¼ãƒˆã®ã¿æº–å‚™
    # Excelãƒ•ã‚¡ã‚¤ãƒ«ã¯OKãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ãŸæ™‚ã«ä¿å­˜ã•ã‚Œã¾ã™
    return {
        "d_dataframe": selected_d_df,
        "i_dataframe": selected_i_df,
        "d_path": d_path,
        "i_path": i_path,
        "all_d_path": all_d_path,
        "all_i_path": all_i_path,
        "candidate_path": candidate_path,
        "image_paths": hist_paths + dimension_paths,
        "d_indices": d_indices,
        "i_indices": i_indices,
        "existing_indices": existing_indices,
        "candidate_df": candidate_df,  # ä¿å­˜å€™è£œç‚¹ãƒªã‚¹ãƒˆãŒå¿…è¦ãªå ´åˆ
        "all_d_df": candidate_df.iloc[d_indices].copy() if len(d_indices) > 0 else pd.DataFrame(),
        "all_i_df": candidate_df.iloc[i_indices].copy() if len(i_indices) > 0 else pd.DataFrame(),
        "output_folders": {"images": di_folder},
    }

# ES: Nueva funciÃ³n: guardar PCA y UMAP por separado, con etiquetas de muestra. Referencia: D_and_Iæœ€é©åŒ–_Greedyæ³•_ver3.py
# EN: New function: save PCA and UMAP separately, with sample labels. Reference: D_and_Iæœ€é©åŒ–_Greedyæ³•_ver3.py
# JA: æ–°é–¢æ•°: PCAã¨UMAPã‚’å€‹åˆ¥ã«ä¿å­˜ã€ã‚µãƒ³ãƒ—ãƒ«ãƒ©ãƒ™ãƒ«ä»˜ã
# ES: En el archivo D_and_Iæœ€é©åŒ–_Greedyæ³•_ver3.py, sÃ­ se calcula tanto el IåŸºæº–å€¤ (I-criterion) como el DåŸºæº–å€¤ (D-criterion).
# EN: In D_and_Iæœ€é©åŒ–_Greedyæ³•_ver3.py, both IåŸºæº–å€¤ (I-criterion) and DåŸºæº–å€¤ (D-criterion) are computed.
# JA: D_and_Iæœ€é©åŒ–_Greedyæ³•_ver3.py ã§ã¯ IåŸºæº–å€¤ ã¨ DåŸºæº–å€¤ ã®ä¸¡æ–¹ã‚’è¨ˆç®—ã—ã¦ã„ã¾ã™ã€‚
# ES: Normalmente, el cÃ¡lculo del DåŸºæº–å€¤ se realiza usando el determinante del submatriz de diseÃ±o seleccionada (...)
# EN: Typically, DåŸºæº–å€¤ is computed from the determinant of the selected design submatrix (...)
# JA: é€šå¸¸ã€DåŸºæº–å€¤ ã¯é¸æŠè¨­è¨ˆè¡Œåˆ—ã®éƒ¨åˆ†è¡Œåˆ—ã®è¡Œåˆ—å¼ãªã©ã‹ã‚‰è¨ˆç®—ã—ã¾ã™ï¼ˆä¾‹: log(det(Xáµ€X))ï¼‰ã€‚
# ES: y el IåŸºæº–å€¤ se calcula como la mÃ­nima distancia entre puntos seleccionados (...)
# EN: and IåŸºæº–å€¤ is computed as the minimum distance between selected points (...)
# JA: IåŸºæº–å€¤ ã¯é¸æŠç‚¹é–“ã®æœ€å°è·é›¢ãªã©ã§è¨ˆç®—ã—ã¾ã™ï¼ˆä¾‹: cdist ã¨æœ€å°å€¤ï¼‰ã€‚
# ES: Busca funciones o bloques de cÃ³digo con nombres como \"calculate_d_criterion\", \"calculate_i_criterion\" (...)
# EN: Look for code blocks named \"calculate_d_criterion\" / \"calculate_i_criterion\" or using np.linalg.det / np.linalg.qr / cdist.
# JA: \"calculate_d_criterion\" / \"calculate_i_criterion\"ã€ã¾ãŸã¯ np.linalg.det / np.linalg.qr / cdist ã‚’ä½¿ã†ç®‡æ‰€ã‚’æ¢ã—ã¦ãã ã•ã„ã€‚
# ES: En la mayorÃ­a de implementaciones, ambos valores se calculan para cada subconjunto candidato y se almacenan o se usan para seleccionar el mejor conjunto.
# EN: In most implementations, both metrics are computed per candidate subset and stored/used to pick the best subset.
# JA: å¤šãã®å®Ÿè£…ã§ã¯ã€å€™è£œã‚µãƒ–ã‚»ãƒƒãƒˆã”ã¨ã«ä¸¡æŒ‡æ¨™ã‚’è¨ˆç®—ã—ã€ä¿å­˜/é¸æŠã«åˆ©ç”¨ã—ã¾ã™ã€‚