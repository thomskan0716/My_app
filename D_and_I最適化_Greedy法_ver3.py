#!/usr/bin/env python
# coding: utf-8

# In[ ]:





# In[6]:


"""
åŒ–å­¦å®Ÿé¨“è¨ˆç”»æ³• - æ—¢å­˜å®Ÿé¨“ç‚¹æ¡ç”¨å•é¡Œä¿®æ­£ç‰ˆ
ä¿®æ­£å†…å®¹: æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰èª¬æ˜å¤‰æ•°ã®ã¿ã‚’æŠ½å‡ºã—ã¦ç…§åˆ
"""

import matplotlib.pyplot as plt
from matplotlib import rcParams
import pandas as pd
import numpy as np
from itertools import product, combinations
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from scipy.linalg import qr
from scipy.spatial.distance import cdist
import time
import os
import json
import optuna
import warnings
warnings.filterwarnings('ignore')

# === è¨­å®šé …ç›® ===
SETTING_FILE = "å®Ÿé¨“è¨ˆç”»è¨­å®šãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ.xlsx"
SHEET_DESIGN = "å®Ÿé¨“è¨ˆç”»_èª¬æ˜å¤‰æ•°è¨­å®š"
SHEET_INFO = "å®Ÿé¨“è¨ˆç”»_åŸºæœ¬æƒ…å ±"
USE_EXISTING_DATA = True
EXISTING_DATA_FILE = "æ—¢å­˜å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿.xlsx"
MAX_TRIALS = 100000
VERBOSE = True
FONT_NAME = "Meiryo"

# === æœ€é©åŒ–è¨­å®š ===
USE_NUMERICAL_STABLE_METHOD = True
CANDIDATE_REDUCTION_THRESHOLD = 10000
MAX_REDUCED_CANDIDATES = 5000
ENABLE_HYPERPARAMETER_TUNING = True
HYPERPARAMETER_CACHE_FILE = "umap_optimal_params.json"
FORCE_REOPTIMIZATION = False

DEFAULT_UMAP_PARAMS = {"n_neighbors": 15, "min_dist": 0.1}
rcParams['font.family'] = FONT_NAME

def load_and_validate_existing_data(existing_file, design_df, verbose=True):
    """
    ğŸ”§ ä¿®æ­£: æ—¢å­˜å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨èª¬æ˜å¤‰æ•°æŠ½å‡º

    åŒ–å­¦å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´:
    - èª¬æ˜å¤‰æ•°ï¼ˆãƒ—ãƒ­ã‚»ã‚¹æ¡ä»¶ï¼‰+ ç›®çš„å¤‰æ•°ï¼ˆå“è³ªç‰¹æ€§ï¼‰ã®æ··åœ¨
    - DOEç…§åˆã«ã¯èª¬æ˜å¤‰æ•°ã®ã¿ãŒå¿…è¦
    - æ•°å€¤ç²¾åº¦ã¨ã‚¹ã‚±ãƒ¼ãƒ«ã®è€ƒæ…®ãŒé‡è¦
    """
    try:
        existing_df = pd.read_excel(existing_file)
        if verbose:
            print(f"ğŸ“ æ—¢å­˜å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {len(existing_df)}è¡Œ Ã— {len(existing_df.columns)}åˆ—")
            print(f"ğŸ“‹ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿åˆ—å: {list(existing_df.columns)}")

        # èª¬æ˜å¤‰æ•°åã‚’å–å¾—
        variable_names = design_df["èª¬æ˜å¤‰æ•°å"].tolist()
        if verbose:
            print(f"ğŸ¯ å¯¾è±¡èª¬æ˜å¤‰æ•°: {variable_names}")

        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰èª¬æ˜å¤‰æ•°åˆ—ã®ã¿ã‚’æŠ½å‡º
        missing_vars = []
        available_vars = []

        for var in variable_names:
            if var in existing_df.columns:
                available_vars.append(var)
            else:
                missing_vars.append(var)

        if missing_vars:
            print(f"âš ï¸ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã«ä»¥ä¸‹ã®èª¬æ˜å¤‰æ•°ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {missing_vars}")
            if len(available_vars) < len(variable_names) * 0.7:  # 70%æœªæº€ã®å¤‰æ•°ã—ã‹ãªã„å ´åˆ
                print("âŒ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®å¤‰æ•°ä¸è¶³ï¼ˆ70%æœªæº€ï¼‰- æ—¢å­˜å®Ÿé¨“ç‚¹ã®ä½¿ç”¨ã‚’ã‚¹ã‚­ãƒƒãƒ—")
                return None, []
            else:
                print(f"âœ… åˆ©ç”¨å¯èƒ½å¤‰æ•°ï¼ˆ{len(available_vars)}/{len(variable_names)}ï¼‰ã§ç¶™ç¶š")

        # èª¬æ˜å¤‰æ•°ã®ã¿ã‚’æŠ½å‡º
        existing_explanatory = existing_df[available_vars]

        if verbose:
            print(f"âœ… èª¬æ˜å¤‰æ•°æŠ½å‡ºå®Œäº†: {len(existing_explanatory)}è¡Œ Ã— {len(available_vars)}åˆ—")
            print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«ï¼ˆæœ€åˆã®3è¡Œï¼‰:")
            print(existing_explanatory.head(3))
            print(f"ğŸ“ˆ ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ:")
            print(existing_explanatory.describe())

        # åŒ–å­¦ãƒ—ãƒ­ã‚»ã‚¹ç‰¹æœ‰ã®å“è³ªãƒã‚§ãƒƒã‚¯
        # 1. æ¬ æå€¤ãƒã‚§ãƒƒã‚¯
        missing_count = existing_explanatory.isnull().sum().sum()
        if missing_count > 0:
            print(f"âš ï¸ æ¬ æå€¤æ¤œå‡º: {missing_count}å€‹")
            existing_explanatory = existing_explanatory.dropna()
            print(f"ğŸ”§ æ¬ æå€¤é™¤å»å¾Œ: {len(existing_explanatory)}è¡Œ")

        # 2. é‡è¤‡å®Ÿé¨“ç‚¹ãƒã‚§ãƒƒã‚¯ï¼ˆåŒ–å­¦å®Ÿé¨“ã§ã¯é‡è¦ï¼‰
        duplicates = existing_explanatory.duplicated().sum()
        if duplicates > 0:
            print(f"âš ï¸ é‡è¤‡å®Ÿé¨“ç‚¹æ¤œå‡º: {duplicates}å€‹")
            existing_explanatory = existing_explanatory.drop_duplicates()
            print(f"ğŸ”§ é‡è¤‡é™¤å»å¾Œ: {len(existing_explanatory)}è¡Œ")

        # 3. æ•°å€¤ç¯„å›²ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ—ãƒ­ã‚»ã‚¹æ¡ä»¶ã®å¦¥å½“æ€§ï¼‰
        for var in available_vars:
            var_info = design_df[design_df["èª¬æ˜å¤‰æ•°å"] == var].iloc[0]
            min_val, max_val = var_info["æœ€å°å€¤"], var_info["æœ€å¤§å€¤"]

            out_of_range = (existing_explanatory[var] < min_val) | (existing_explanatory[var] > max_val)
            out_count = out_of_range.sum()

            if out_count > 0:
                print(f"âš ï¸ {var}: ç¯„å›²å¤–ãƒ‡ãƒ¼ã‚¿ {out_count}å€‹ (è¨­å®šç¯„å›²: {min_val}ï½{max_val})")
                # ç¯„å›²å¤–ãƒ‡ãƒ¼ã‚¿ã‚‚ä¿æŒï¼ˆå®Ÿéš›ã®å®Ÿé¨“æ¡ä»¶ã¨ã—ã¦æœ‰åŠ¹ãªå ´åˆãŒã‚ã‚‹ãŸã‚ï¼‰

        return existing_explanatory, available_vars

    except FileNotFoundError:
        print(f"âŒ æ—¢å­˜å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {existing_file}")
        return None, []
    except Exception as e:
        print(f"âŒ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return None, []

def match_existing_experiments_enhanced(candidate_points, existing_data, variable_names, 
                                      tolerance_relative=1e-6, tolerance_absolute=1e-8, verbose=True):
    """
    ğŸ”§ ä¿®æ­£: åŒ–å­¦å®Ÿé¨“æ¡ä»¶ã®é«˜ç²¾åº¦ãƒãƒƒãƒãƒ³ã‚°

    åŒ–å­¦ãƒ—ãƒ­ã‚»ã‚¹ã®ç‰¹å¾´ã‚’è€ƒæ…®:
    - æ¸¬å®šç²¾åº¦ã®é•ã„ï¼ˆå›è»¢é€Ÿåº¦ vs è¼‰ã›ç‡ï¼‰
    - ç›¸å¯¾èª¤å·®ã¨çµ¶å¯¾èª¤å·®ã®ä½µç”¨
    - ã‚¹ã‚±ãƒ¼ãƒ«æ­£è¦åŒ–ã«ã‚ˆã‚‹å…¬å¹³ãªæ¯”è¼ƒ
    """
    if existing_data is None or len(existing_data) == 0:
        return []

    print(f"ğŸ” æ—¢å­˜å®Ÿé¨“ç‚¹ã¨ã®ãƒãƒƒãƒãƒ³ã‚°é–‹å§‹")
    print(f"  - å€™è£œç‚¹æ•°: {len(candidate_points):,}")
    print(f"  - æ—¢å­˜å®Ÿé¨“æ•°: {len(existing_data)}")
    print(f"  - è¨±å®¹èª¤å·®ï¼ˆç›¸å¯¾ï¼‰: {tolerance_relative}")
    print(f"  - è¨±å®¹èª¤å·®ï¼ˆçµ¶å¯¾ï¼‰: {tolerance_absolute}")

    # å€™è£œç‚¹ã‚’DataFrameã«å¤‰æ›ï¼ˆåˆ—åçµ±ä¸€ï¼‰
    candidate_df = pd.DataFrame(candidate_points, columns=variable_names)

    # ä¸¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ¨™æº–åŒ–ï¼ˆã‚¹ã‚±ãƒ¼ãƒ«ã®é•ã„ã‚’å¸åï¼‰
    scaler = StandardScaler()
    candidate_scaled = scaler.fit_transform(candidate_df)

    # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚‚åŒã˜å¤‰æ•°é †åºã§ä¸¦ã³æ›¿ãˆ
    existing_aligned = existing_data[variable_names]
    existing_scaled = scaler.transform(existing_aligned)

    matched_indices = []
    match_details = []

    # å„æ—¢å­˜å®Ÿé¨“ç‚¹ã«ã¤ã„ã¦æœ€ã‚‚è¿‘ã„å€™è£œç‚¹ã‚’æ¢ç´¢
    for exist_idx, exist_row in enumerate(existing_aligned.values):
        min_distance = float('inf')
        best_candidate_idx = None

        for cand_idx, cand_row in enumerate(candidate_df.values):
            # 1. ç›¸å¯¾èª¤å·®ãƒ™ãƒ¼ã‚¹ã®æ¯”è¼ƒ
            relative_errors = []
            absolute_ok = True

            for var_idx, var_name in enumerate(variable_names):
                exist_val = exist_row[var_idx]
                cand_val = cand_row[var_idx]

                # çµ¶å¯¾èª¤å·®ãƒã‚§ãƒƒã‚¯
                abs_error = abs(exist_val - cand_val)
                if abs_error > tolerance_absolute:
                    # ç›¸å¯¾èª¤å·®ã‚‚ãƒã‚§ãƒƒã‚¯
                    if exist_val != 0:
                        rel_error = abs_error / abs(exist_val)
                        if rel_error > tolerance_relative:
                            absolute_ok = False
                            break
                    else:
                        absolute_ok = False
                        break

                relative_errors.append(abs_error)

            if absolute_ok:
                # ç·åˆè·é›¢ï¼ˆæ¨™æº–åŒ–ç©ºé–“ã§ã®è·é›¢ï¼‰
                distance = np.linalg.norm(existing_scaled[exist_idx] - candidate_scaled[cand_idx])

                if distance < min_distance:
                    min_distance = distance
                    best_candidate_idx = cand_idx

        if best_candidate_idx is not None:
            matched_indices.append(best_candidate_idx)

            # ãƒãƒƒãƒãƒ³ã‚°è©³ç´°ã‚’è¨˜éŒ²
            match_detail = {
                'æ—¢å­˜å®Ÿé¨“ç•ªå·': exist_idx,
                'å€™è£œç‚¹ç•ªå·': best_candidate_idx,
                'è·é›¢': min_distance,
                'æ—¢å­˜å®Ÿé¨“æ¡ä»¶': existing_aligned.iloc[exist_idx].to_dict(),
                'å€™è£œç‚¹æ¡ä»¶': candidate_df.iloc[best_candidate_idx].to_dict()
            }
            match_details.append(match_detail)

            if verbose and len(matched_indices) <= 5:  # æœ€åˆã®5ä»¶ã‚’è©³ç´°è¡¨ç¤º
                print(f"âœ… ãƒãƒƒãƒãƒ³ã‚° {len(matched_indices)}: æ—¢å­˜#{exist_idx} â†’ å€™è£œ#{best_candidate_idx} (è·é›¢: {min_distance:.4f})")

    # é‡è¤‡é™¤å»ï¼ˆ1ã¤ã®å€™è£œç‚¹ã«è¤‡æ•°ã®æ—¢å­˜å®Ÿé¨“ãŒãƒãƒƒãƒã—ãŸå ´åˆï¼‰
    unique_matched = list(set(matched_indices))

    print(f"ğŸ“Š ãƒãƒƒãƒãƒ³ã‚°çµæœ:")
    print(f"  - åˆæœŸãƒãƒƒãƒæ•°: {len(matched_indices)}")
    print(f"  - é‡è¤‡é™¤å»å¾Œ: {len(unique_matched)}")
    print(f"  - ãƒãƒƒãƒãƒ³ã‚°ç‡: {len(unique_matched)/len(existing_data)*100:.1f}%")

    if len(unique_matched) == 0:
        print("âš ï¸ æ—¢å­˜å®Ÿé¨“ç‚¹ãŒãƒãƒƒãƒã—ã¾ã›ã‚“ã§ã—ãŸ")
        print("ğŸ’¡ è€ƒãˆã‚‰ã‚Œã‚‹åŸå› :")
        print("  1. æ—¢å­˜å®Ÿé¨“æ¡ä»¶ãŒå€™è£œç‚¹ã®è¨­å®šç¯„å›²å¤–")
        print("  2. åˆ»ã¿å¹…è¨­å®šãŒæ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¨åˆã‚ãªã„")
        print("  3. è¨±å®¹èª¤å·®è¨­å®šãŒå³ã—ã™ãã‚‹")

        # è¨ºæ–­æƒ…å ±ã®æä¾›
        print("\nğŸ” è¨ºæ–­æƒ…å ±:")
        for var in variable_names:
            exist_range = (existing_aligned[var].min(), existing_aligned[var].max())
            cand_range = (candidate_df[var].min(), candidate_df[var].max())
            print(f"  {var}: æ—¢å­˜{exist_range} vs å€™è£œ{cand_range}")

    return unique_matched

def hierarchical_candidate_reduction(candidate_points, max_candidates=5000, existing_indices=None):
    """éšå±¤çš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã«ã‚ˆã‚‹å€™è£œç‚¹å‰Šæ¸›ï¼ˆæ—¢å­˜ç‚¹ä¿æŒï¼‰"""
    n_original = len(candidate_points)

    if n_original <= max_candidates:
        print(f"ğŸ“Š å€™è£œç‚¹æ•°({n_original:,})ã¯å‰Šæ¸›ä¸è¦ï¼ˆé–¾å€¤: {max_candidates:,}ï¼‰")
        return candidate_points, list(range(n_original))

    print(f"ğŸ”„ âœ… éšå±¤çš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å®Ÿè¡Œ: {n_original:,} â†’ {max_candidates:,}ç‚¹ã«å‰Šæ¸›")

    # æ—¢å­˜å®Ÿé¨“ç‚¹ã‚’ä¿è­·
    if existing_indices:
        existing_set = set(existing_indices)
        available_indices = [i for i in range(n_original) if i not in existing_set]
        available_points = candidate_points[available_indices]
        n_to_select = max_candidates - len(existing_indices)
        print(f"ğŸ“ æ—¢å­˜å®Ÿé¨“ç‚¹ä¿æŒ: {len(existing_indices)}ç‚¹")
    else:
        available_indices = list(range(n_original))
        available_points = candidate_points
        n_to_select = max_candidates
        existing_indices = []

    if n_to_select <= 0:
        print("âš ï¸ æ—¢å­˜ç‚¹ã®ã¿ã§ä¸Šé™ã«é”ã—ã¾ã—ãŸ")
        return candidate_points[existing_indices], existing_indices

    print(f"ğŸ¯ æ–°è¦é¸å®šå¯¾è±¡: {n_to_select:,}ç‚¹")

    try:
        from sklearn.cluster import MiniBatchKMeans

        n_clusters = min(n_to_select, len(available_points))
        print(f"ğŸ”§ MiniBatchKMeansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°: {n_clusters}ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼")

        kmeans = MiniBatchKMeans(
            n_clusters=n_clusters, 
            random_state=42, 
            batch_size=min(1000, len(available_points)//10),
            n_init=3,
            max_iter=100
        )

        start_time = time.time()
        clusters = kmeans.fit_predict(available_points)
        clustering_time = time.time() - start_time
        print(f"â±ï¸ ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æ™‚é–“: {clustering_time:.2f}ç§’")

        # å„ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‹ã‚‰ä»£è¡¨ç‚¹ã‚’é¸æŠ
        selected_indices = list(existing_indices)  # æ—¢å­˜ç‚¹ã¯å¿…ãšä¿æŒ

        for i in range(n_clusters):
            cluster_mask = clusters == i
            if np.any(cluster_mask):
                cluster_indices_in_available = np.where(cluster_mask)[0]
                cluster_original_indices = [available_indices[j] for j in cluster_indices_in_available]

                # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼é‡å¿ƒã«æœ€ã‚‚è¿‘ã„ç‚¹ã‚’é¸æŠ
                cluster_points = available_points[cluster_mask]
                center = kmeans.cluster_centers_[i]
                distances = np.linalg.norm(cluster_points - center, axis=1)
                closest_idx_in_cluster = np.argmin(distances)
                closest_original_idx = cluster_original_indices[closest_idx_in_cluster]

                selected_indices.append(closest_original_idx)

        reduced_points = candidate_points[selected_indices]

        print(f"âœ… éšå±¤çš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å®Œäº†: æœ€çµ‚å€™è£œç‚¹æ•° {len(reduced_points):,}")
        print(f"  - æ—¢å­˜å®Ÿé¨“ç‚¹ä¿æŒ: {len(existing_indices)}ç‚¹")
        print(f"  - æ–°è¦é¸å®šç‚¹: {len(selected_indices) - len(existing_indices)}ç‚¹")

        return reduced_points, selected_indices

    except Exception as e:
        print(f"âš ï¸ éšå±¤çš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†
        return candidate_points, list(range(len(candidate_points)))

def calculate_d_criterion_stable(X, method='auto'):
    """æ•°å€¤çš„ã«å®‰å®šãªD-criterionè¨ˆç®—"""
    try:
        condition_number = np.linalg.cond(X)

        if USE_NUMERICAL_STABLE_METHOD or method == 'auto' and condition_number > 1e12:
            method = 'svd'
            if VERBOSE and condition_number > 1e12:
                print(f"ğŸ”§ é«˜æ¡ä»¶æ•°æ¤œå‡º({condition_number:.2e}) - SVDæ³•é©ç”¨")

        if method == 'svd':
            _, s, _ = np.linalg.svd(X, full_matrices=False)
            valid_singular_values = s[s > 1e-14]
            if len(valid_singular_values) == 0:
                return -np.inf, condition_number
            log_det = np.sum(np.log(valid_singular_values))
        else:
            q, r = qr(X, mode='economic')
            diag_r = np.diag(r)
            det = np.abs(np.prod(diag_r))
            log_det = np.log(det) if det > 1e-300 else -np.inf

        return log_det, condition_number

    except Exception as e:
        if VERBOSE:
            print(f"âš ï¸ D-criterionè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
        return -np.inf, np.inf

def select_d_optimal_design_enhanced(X_all, existing_indices, new_experiments, verbose=True):
    """
    D-optimalè¨­è¨ˆé¸å®šï¼ˆæ—¢å­˜å®Ÿé¨“ç‚¹ + æ–°è¦å®Ÿé¨“ç‚¹ï¼‰

    Args:
        X_all: å…¨å€™è£œç‚¹
        existing_indices: æ—¢å­˜å®Ÿé¨“ç‚¹ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        new_experiments: æ–°è¦å®Ÿé¨“ç‚¹æ•°
        verbose: è©³ç´°è¡¨ç¤º
    """
    base = list(existing_indices) if existing_indices else []
    remaining = [i for i in range(len(X_all)) if i not in base]
    total_select = len(base) + new_experiments  # æ—¢å­˜ + æ–°è¦

    if verbose:
        print(f"  - æ—¢å­˜å®Ÿé¨“ç‚¹: {len(base)}ç‚¹")
        print(f"  - æ–°è¦å®Ÿé¨“ç‚¹: {new_experiments}ç‚¹")
        print(f"  - åˆè¨ˆé¸å®šç‚¹: {total_select}ç‚¹")

    if new_experiments <= 0:
        if verbose:
            print(f"  âœ… æ—¢å­˜å®Ÿé¨“ç‚¹ã®ã¿ã§å®Œäº†")
        score, _ = calculate_d_criterion_stable(X_all[base])
        return base, score

    selected = list(base)

    for step in range(new_experiments):
        best_candidate = None
        best_score = -np.inf

        # å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§ã¯ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        if len(remaining) > 1000:
            sample_size = min(500, len(remaining))
            sample_indices = np.random.choice(remaining, sample_size, replace=False)
            candidates_to_check = sample_indices
        else:
            candidates_to_check = remaining

        for idx in candidates_to_check:
            trial_set = selected + [idx]
            X_subset = X_all[trial_set]
            score, condition_num = calculate_d_criterion_stable(X_subset)

            if score > best_score:
                best_score = score
                best_candidate = idx

        if best_candidate is not None:
            selected.append(best_candidate)
            remaining.remove(best_candidate)
            if verbose:
                print(f"  âœ… æ–°è¦é¸å®š {step+1}/{new_experiments}: ç‚¹{best_candidate}, ã‚¹ã‚³ã‚¢: {best_score:.4f}")
        else:
            if verbose:
                print(f"  âš ï¸ ã‚¹ãƒ†ãƒƒãƒ—{step+1}ã§é©åˆ‡ãªå€™è£œç‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            break

    final_score, final_condition = calculate_d_criterion_stable(X_all[selected])
    return selected, final_score

    selected = list(base)

    for step in range(n_additional):
        best_candidate = None
        best_score = -np.inf

        # å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§ã¯ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        if len(remaining) > 1000:
            sample_size = min(500, len(remaining))
            sample_indices = np.random.choice(remaining, sample_size, replace=False)
            candidates_to_check = sample_indices
        else:
            candidates_to_check = remaining

        for idx in candidates_to_check:
            trial_set = selected + [idx]
            X_subset = X_all[trial_set]
            score, condition_num = calculate_d_criterion_stable(X_subset)

            if score > best_score:
                best_score = score
                best_candidate = idx

        if best_candidate is not None:
            selected.append(best_candidate)
            remaining.remove(best_candidate)
            if verbose:
                print(f"âœ… D-optimalé¸å®š {step+1}/{n_additional}: ç‚¹{best_candidate}, ã‚¹ã‚³ã‚¢: {best_score:.4f}")
        else:
            if verbose:
                print(f"âš ï¸ ã‚¹ãƒ†ãƒƒãƒ—{step+1}ã§é©åˆ‡ãªå€™è£œç‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            break

    final_score, final_condition = calculate_d_criterion_stable(X_all[selected])
    return selected, final_score

def select_i_optimal_design(X_all, new_experiments, existing_indices=None):
    """
    I-optimalè¨­è¨ˆé¸å®šï¼ˆæ—¢å­˜å®Ÿé¨“ç‚¹ + æ–°è¦å®Ÿé¨“ç‚¹ï¼‰

    Args:
        X_all: å…¨å€™è£œç‚¹
        new_experiments: æ–°è¦å®Ÿé¨“ç‚¹æ•°
        existing_indices: æ—¢å­˜å®Ÿé¨“ç‚¹ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
    """
    if existing_indices:
        selected_indices = list(existing_indices)
        print(f"  - æ—¢å­˜å®Ÿé¨“ç‚¹: {len(existing_indices)}ç‚¹")
        print(f"  - æ–°è¦å®Ÿé¨“ç‚¹: {new_experiments}ç‚¹")
        print(f"  - åˆè¨ˆé¸å®šç‚¹: {len(existing_indices) + new_experiments}ç‚¹")
    else:
        selected_indices = [0]
        print(f"  - æ–°è¦å®Ÿé¨“ç‚¹: {new_experiments}ç‚¹ï¼ˆæ—¢å­˜ç‚¹ãªã—ï¼‰")

    remaining_indices = [i for i in range(len(X_all)) if i not in selected_indices]
    target_total = len(selected_indices) + new_experiments

    step = 0
    while len(selected_indices) < target_total and remaining_indices:
        dists = cdist(X_all[remaining_indices], X_all[selected_indices])
        min_dists = dists.min(axis=1)
        next_idx_in_remaining = np.argmax(min_dists)
        next_index = remaining_indices[next_idx_in_remaining]
        selected_indices.append(next_index)
        remaining_indices.remove(next_index)
        step += 1
        print(f"  âœ… æ–°è¦é¸å®š {step}/{new_experiments}: ç‚¹{next_index}")

    return selected_indices

    while len(selected_indices) < total_select:
        dists = cdist(X_all[remaining_indices], X_all[selected_indices])
        min_dists = dists.min(axis=1)
        next_idx_in_remaining = np.argmax(min_dists)
        next_index = remaining_indices[next_idx_in_remaining]
        selected_indices.append(next_index)
        remaining_indices.remove(next_index)

    return selected_indices

def generate_candidate_points(design_df):
    """å€™è£œç‚¹ç”Ÿæˆ"""
    levels = []
    for _, row in design_df.iterrows():
        levels.append(np.arange(row["æœ€å°å€¤"], row["æœ€å¤§å€¤"] + row["åˆ»ã¿å¹…"], row["åˆ»ã¿å¹…"]))
    return np.array(list(product(*levels)))

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°ï¼ˆæ—¢å­˜å®Ÿé¨“ç‚¹å¯¾å¿œä¿®æ­£ç‰ˆï¼‰"""
    print("ğŸš€ åŒ–å­¦å®Ÿé¨“è¨ˆç”»æ³•ã‚·ã‚¹ãƒ†ãƒ  - æ—¢å­˜å®Ÿé¨“ç‚¹å¯¾å¿œä¿®æ­£ç‰ˆ")
    print("="*60)
    print("ğŸ”§ ä¿®æ­£å†…å®¹: æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰èª¬æ˜å¤‰æ•°ã®ã¿ã‚’æŠ½å‡ºã—ã¦ç…§åˆ")
    print("="*60)

    # è¨­å®šèª­ã¿è¾¼ã¿
    try:
        design_df = pd.read_excel(SETTING_FILE, sheet_name=SHEET_DESIGN)
        info_df = pd.read_excel(SETTING_FILE, sheet_name=SHEET_INFO)
        n_experiments = int(info_df.loc[info_df["è¨­å®šé …ç›®"] == "å®Ÿé¨“æ•°", "å€¤"].values[0])
        print(f"ğŸ“‹ è¨­å®šèª­ã¿è¾¼ã¿å®Œäº†")
        print(f"  - èª¬æ˜å¤‰æ•°æ•°: {len(design_df)}")
        print(f"  - ç›®æ¨™å®Ÿé¨“æ•°: {n_experiments}")
    except Exception as e:
        print(f"âŒ è¨­å®šèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return

    # å€™è£œç‚¹ç”Ÿæˆ
    print(f"\nğŸ“Š å€™è£œç‚¹ç”Ÿæˆä¸­...")
    candidate_points = generate_candidate_points(design_df)
    print(f"âœ… åˆæœŸå€™è£œç‚¹ç”Ÿæˆå®Œäº†: {len(candidate_points):,}ç‚¹")

    # ğŸ”§ ä¿®æ­£: æ—¢å­˜å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã®é©åˆ‡ãªå‡¦ç†
    existing_indices = []
    if USE_EXISTING_DATA:
        print(f"\nğŸ” æ—¢å­˜å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿å‡¦ç†é–‹å§‹")

        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨æ¤œè¨¼
        existing_data, available_vars = load_and_validate_existing_data(
            EXISTING_DATA_FILE, design_df, verbose=True
        )

        if existing_data is not None and len(existing_data) > 0:
            # é«˜ç²¾åº¦ãƒãƒƒãƒãƒ³ã‚°å®Ÿè¡Œ
            variable_names = design_df["èª¬æ˜å¤‰æ•°å"].tolist()
            existing_indices = match_existing_experiments_enhanced(
                candidate_points, existing_data, variable_names,
                tolerance_relative=1e-4,  # åŒ–å­¦ãƒ—ãƒ­ã‚»ã‚¹ç”¨ã«ç·©å’Œ
                tolerance_absolute=1e-6,  # åŒ–å­¦ãƒ—ãƒ­ã‚»ã‚¹ç”¨ã«ç·©å’Œ
                verbose=True
            )
        else:
            print("âŒ æ—¢å­˜å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")

    # å€™è£œç‚¹å‰Šæ¸›ï¼ˆæ—¢å­˜ç‚¹ã‚’ä¿æŒï¼‰
    original_candidate_count = len(candidate_points)
    should_reduce = len(candidate_points) > CANDIDATE_REDUCTION_THRESHOLD

    if should_reduce:
        print(f"\nğŸ”„ å€™è£œç‚¹å‰Šæ¸›å®Ÿè¡Œ: {original_candidate_count:,} â†’ {MAX_REDUCED_CANDIDATES:,}")
        candidate_points, reduced_mapping = hierarchical_candidate_reduction(
            candidate_points, MAX_REDUCED_CANDIDATES, existing_indices
        )

        # æ—¢å­˜å®Ÿé¨“ç‚¹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®æ›´æ–°
        if existing_indices:
            existing_indices = [reduced_mapping.index(idx) for idx in existing_indices if idx in reduced_mapping]
            print(f"âœ… æ—¢å­˜å®Ÿé¨“ç‚¹ãƒãƒƒãƒ”ãƒ³ã‚°æ›´æ–°: {len(existing_indices)}ä»¶ä¿æŒ")

    print(f"\nâœ… æœ€çµ‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ:")
    print(f"  - æœ€çµ‚å€™è£œç‚¹æ•°: {len(candidate_points):,}")
    print(f"  - æ—¢å­˜å®Ÿé¨“ç‚¹æ•°: {len(existing_indices)}")
    print(f"  - æ—¢å­˜å®Ÿé¨“æ´»ç”¨ç‡: {len(existing_indices)/n_experiments*100:.1f}%")

    # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
    candidate_df = pd.DataFrame(candidate_points, columns=design_df["èª¬æ˜å¤‰æ•°å"])
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(candidate_points)
    print(f"âœ… ãƒ‡ãƒ¼ã‚¿æ¨™æº–åŒ–å®Œäº†")

    # D-optimalè¨­è¨ˆ
    print(f"\nğŸ¯ D-optimalè¨­è¨ˆå®Ÿè¡Œï¼ˆæ—¢å­˜{len(existing_indices)}ç‚¹ã‚’å«ã‚€ï¼‰")
    d_indices, d_score = select_d_optimal_design_enhanced(
        X_scaled, existing_indices, n_experiments, verbose=VERBOSE
    )
    print(f"âœ… D-optimalè¨­è¨ˆå®Œäº†")
    print(f"  - æœ€çµ‚ã‚¹ã‚³ã‚¢: {d_score:.4f}")
    print(f"  - é¸å®šç‚¹æ•°: {len(d_indices)}")
    print(f"  - æ—¢å­˜ç‚¹æ´»ç”¨: {len([i for i in d_indices if i in existing_indices])}ç‚¹")

    # I-optimalè¨­è¨ˆ
    print(f"\nğŸ¯ I-optimalè¨­è¨ˆå®Ÿè¡Œï¼ˆæ—¢å­˜{len(existing_indices)}ç‚¹ã‚’å«ã‚€ï¼‰")
    i_indices = select_i_optimal_design(X_scaled, n_experiments, existing_indices)
    print(f"âœ… I-optimalè¨­è¨ˆå®Œäº†")
    print(f"  - é¸å®šç‚¹æ•°: {len(i_indices)}")
    print(f"  - æ—¢å­˜ç‚¹æ´»ç”¨: {len([i for i in i_indices if i in existing_indices])}ç‚¹")

    # çµæœä¿å­˜
    selected_d_df = candidate_df.iloc[d_indices]
    selected_i_df = candidate_df.iloc[i_indices]

    # æ—¢å­˜/æ–°è¦ã®åŒºåˆ¥ã‚’è¿½åŠ 
    selected_d_df['ãƒ‡ãƒ¼ã‚¿ç¨®åˆ¥'] = ['æ—¢å­˜' if i in existing_indices else 'æ–°è¦' for i in d_indices]
    selected_i_df['ãƒ‡ãƒ¼ã‚¿ç¨®åˆ¥'] = ['æ—¢å­˜' if i in existing_indices else 'æ–°è¦' for i in i_indices]

    selected_d_df.to_excel("D_optimal_ä¿®æ­£ç‰ˆ.xlsx", index=False)
    selected_i_df.to_excel("I_optimal_ä¿®æ­£ç‰ˆ.xlsx", index=False)

    print(f"\nğŸ“Š æœ€çµ‚çµæœã‚µãƒãƒªãƒ¼:")
    print(f"  - D-optimal: æ—¢å­˜{len([i for i in d_indices if i in existing_indices])}ç‚¹ + æ–°è¦{len(d_indices) - len([i for i in d_indices if i in existing_indices])}ç‚¹")
    print(f"  - I-optimal: æ—¢å­˜{len([i for i in i_indices if i in existing_indices])}ç‚¹ + æ–°è¦{len(i_indices) - len([i for i in i_indices if i in existing_indices])}ç‚¹")
    print(f"ğŸ’¾ çµæœãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜: D_optimal_ä¿®æ­£ç‰ˆ.xlsx, I_optimal_ä¿®æ­£ç‰ˆ.xlsx")

    print(f"\nğŸ‰ åŒ–å­¦å®Ÿé¨“è¨ˆç”»æ³•ã‚·ã‚¹ãƒ†ãƒ å®Œäº†ï¼ˆæ—¢å­˜å®Ÿé¨“ç‚¹å¯¾å¿œä¿®æ­£ç‰ˆï¼‰")
    print("="*60)

if __name__ == "__main__":
    main()


# In[7]:


import matplotlib.pyplot as plt
from matplotlib import rcParams
import pandas as pd
import numpy as np
from itertools import product, combinations
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from scipy.linalg import qr
from scipy.spatial.distance import cdist
import time
import os
import json
import optuna
import warnings
warnings.filterwarnings('ignore')

# === è¨­å®šé …ç›® ===
SETTING_FILE = "å®Ÿé¨“è¨ˆç”»è¨­å®šãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ.xlsx"
SHEET_DESIGN = "å®Ÿé¨“è¨ˆç”»_èª¬æ˜å¤‰æ•°è¨­å®š"
SHEET_INFO = "å®Ÿé¨“è¨ˆç”»_åŸºæœ¬æƒ…å ±"
USE_EXISTING_DATA = True
EXISTING_DATA_FILE = "æ—¢å­˜å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿.xlsx"
MAX_TRIALS = 100000
VERBOSE = True
FONT_NAME = "Meiryo"

# === æœ€é©åŒ–è¨­å®š ===
USE_NUMERICAL_STABLE_METHOD = True
CANDIDATE_REDUCTION_THRESHOLD = 10000
MAX_REDUCED_CANDIDATES = 5000
ENABLE_HYPERPARAMETER_TUNING = True
HYPERPARAMETER_CACHE_FILE = "umap_optimal_params.json"
FORCE_REOPTIMIZATION = False

DEFAULT_UMAP_PARAMS = {"n_neighbors": 15, "min_dist": 0.1}
rcParams['font.family'] = FONT_NAME

def load_and_validate_existing_data(existing_file, design_df, verbose=True):
    """æ—¢å­˜å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨èª¬æ˜å¤‰æ•°æŠ½å‡º"""
    try:
        existing_df = pd.read_excel(existing_file)
        if verbose:
            print(f"ğŸ“ æ—¢å­˜å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {len(existing_df)}è¡Œ Ã— {len(existing_df.columns)}åˆ—")
            print(f"ğŸ“‹ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿åˆ—å: {list(existing_df.columns)}")

        # èª¬æ˜å¤‰æ•°åã‚’å–å¾—
        variable_names = design_df["èª¬æ˜å¤‰æ•°å"].tolist()
        if verbose:
            print(f"ğŸ¯ å¯¾è±¡èª¬æ˜å¤‰æ•°: {variable_names}")

        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰èª¬æ˜å¤‰æ•°åˆ—ã®ã¿ã‚’æŠ½å‡º
        missing_vars = []
        available_vars = []

        for var in variable_names:
            if var in existing_df.columns:
                available_vars.append(var)
            else:
                missing_vars.append(var)

        if missing_vars:
            print(f"âš ï¸ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã«ä»¥ä¸‹ã®èª¬æ˜å¤‰æ•°ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {missing_vars}")
            if len(available_vars) < len(variable_names) * 0.7:
                print("âŒ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®å¤‰æ•°ä¸è¶³ï¼ˆ70%æœªæº€ï¼‰- æ—¢å­˜å®Ÿé¨“ç‚¹ã®ä½¿ç”¨ã‚’ã‚¹ã‚­ãƒƒãƒ—")
                return None, []
            else:
                print(f"âœ… åˆ©ç”¨å¯èƒ½å¤‰æ•°ï¼ˆ{len(available_vars)}/{len(variable_names)}ï¼‰ã§ç¶™ç¶š")

        # èª¬æ˜å¤‰æ•°ã®ã¿ã‚’æŠ½å‡º
        existing_explanatory = existing_df[available_vars]

        if verbose:
            print(f"âœ… èª¬æ˜å¤‰æ•°æŠ½å‡ºå®Œäº†: {len(existing_explanatory)}è¡Œ Ã— {len(available_vars)}åˆ—")
            print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«ï¼ˆæœ€åˆã®3è¡Œï¼‰:")
            print(existing_explanatory.head(3))
            print(f"ğŸ“ˆ ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ:")
            print(existing_explanatory.describe())

        # åŒ–å­¦ãƒ—ãƒ­ã‚»ã‚¹ç‰¹æœ‰ã®å“è³ªãƒã‚§ãƒƒã‚¯
        # 1. æ¬ æå€¤ãƒã‚§ãƒƒã‚¯
        missing_count = existing_explanatory.isnull().sum().sum()
        if missing_count > 0:
            print(f"âš ï¸ æ¬ æå€¤æ¤œå‡º: {missing_count}å€‹")
            existing_explanatory = existing_explanatory.dropna()
            print(f"ğŸ”§ æ¬ æå€¤é™¤å»å¾Œ: {len(existing_explanatory)}è¡Œ")

        # 2. é‡è¤‡å®Ÿé¨“ç‚¹ãƒã‚§ãƒƒã‚¯
        duplicates = existing_explanatory.duplicated().sum()
        if duplicates > 0:
            print(f"âš ï¸ é‡è¤‡å®Ÿé¨“ç‚¹æ¤œå‡º: {duplicates}å€‹")
            existing_explanatory = existing_explanatory.drop_duplicates()
            print(f"ğŸ”§ é‡è¤‡é™¤å»å¾Œ: {len(existing_explanatory)}è¡Œ")

        # 3. æ•°å€¤ç¯„å›²ãƒã‚§ãƒƒã‚¯
        for var in available_vars:
            var_info = design_df[design_df["èª¬æ˜å¤‰æ•°å"] == var].iloc[0]
            min_val, max_val = var_info["æœ€å°å€¤"], var_info["æœ€å¤§å€¤"]

            out_of_range = (existing_explanatory[var] < min_val) | (existing_explanatory[var] > max_val)
            out_count = out_of_range.sum()

            if out_count > 0:
                print(f"âš ï¸ {var}: ç¯„å›²å¤–ãƒ‡ãƒ¼ã‚¿ {out_count}å€‹ (è¨­å®šç¯„å›²: {min_val}ï½{max_val})")

        return existing_explanatory, available_vars

    except FileNotFoundError:
        print(f"âŒ æ—¢å­˜å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {existing_file}")
        return None, []
    except Exception as e:
        print(f"âŒ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return None, []

def match_existing_experiments_enhanced(candidate_points, existing_data, variable_names, 
                                      tolerance_relative=1e-6, tolerance_absolute=1e-8, verbose=True):
    """åŒ–å­¦å®Ÿé¨“æ¡ä»¶ã®é«˜ç²¾åº¦ãƒãƒƒãƒãƒ³ã‚°"""
    if existing_data is None or len(existing_data) == 0:
        return []

    print(f"ğŸ” æ—¢å­˜å®Ÿé¨“ç‚¹ã¨ã®ãƒãƒƒãƒãƒ³ã‚°é–‹å§‹")
    print(f"  - å€™è£œç‚¹æ•°: {len(candidate_points):,}")
    print(f"  - æ—¢å­˜å®Ÿé¨“æ•°: {len(existing_data)}")
    print(f"  - è¨±å®¹èª¤å·®ï¼ˆç›¸å¯¾ï¼‰: {tolerance_relative}")
    print(f"  - è¨±å®¹èª¤å·®ï¼ˆçµ¶å¯¾ï¼‰: {tolerance_absolute}")

    # å€™è£œç‚¹ã‚’DataFrameã«å¤‰æ›
    candidate_df = pd.DataFrame(candidate_points, columns=variable_names)

    # ä¸¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ¨™æº–åŒ–
    scaler = StandardScaler()
    candidate_scaled = scaler.fit_transform(candidate_df)

    # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚‚åŒã˜å¤‰æ•°é †åºã§ä¸¦ã³æ›¿ãˆ
    existing_aligned = existing_data[variable_names]
    existing_scaled = scaler.transform(existing_aligned)

    matched_indices = []
    match_details = []

    # å„æ—¢å­˜å®Ÿé¨“ç‚¹ã«ã¤ã„ã¦æœ€ã‚‚è¿‘ã„å€™è£œç‚¹ã‚’æ¢ç´¢
    for exist_idx, exist_row in enumerate(existing_aligned.values):
        min_distance = float('inf')
        best_candidate_idx = None

        for cand_idx, cand_row in enumerate(candidate_df.values):
            # ç›¸å¯¾èª¤å·®ãƒ™ãƒ¼ã‚¹ã®æ¯”è¼ƒ
            relative_errors = []
            absolute_ok = True

            for var_idx, var_name in enumerate(variable_names):
                exist_val = exist_row[var_idx]
                cand_val = cand_row[var_idx]

                # çµ¶å¯¾èª¤å·®ãƒã‚§ãƒƒã‚¯
                abs_error = abs(exist_val - cand_val)
                if abs_error > tolerance_absolute:
                    # ç›¸å¯¾èª¤å·®ã‚‚ãƒã‚§ãƒƒã‚¯
                    if exist_val != 0:
                        rel_error = abs_error / abs(exist_val)
                        if rel_error > tolerance_relative:
                            absolute_ok = False
                            break
                    else:
                        absolute_ok = False
                        break

                relative_errors.append(abs_error)

            if absolute_ok:
                # ç·åˆè·é›¢ï¼ˆæ¨™æº–åŒ–ç©ºé–“ã§ã®è·é›¢ï¼‰
                distance = np.linalg.norm(existing_scaled[exist_idx] - candidate_scaled[cand_idx])

                if distance < min_distance:
                    min_distance = distance
                    best_candidate_idx = cand_idx

        if best_candidate_idx is not None:
            matched_indices.append(best_candidate_idx)

            # ãƒãƒƒãƒãƒ³ã‚°è©³ç´°ã‚’è¨˜éŒ²
            match_detail = {
                'æ—¢å­˜å®Ÿé¨“ç•ªå·': exist_idx,
                'å€™è£œç‚¹ç•ªå·': best_candidate_idx,
                'è·é›¢': min_distance,
                'æ—¢å­˜å®Ÿé¨“æ¡ä»¶': existing_aligned.iloc[exist_idx].to_dict(),
                'å€™è£œç‚¹æ¡ä»¶': candidate_df.iloc[best_candidate_idx].to_dict()
            }
            match_details.append(match_detail)

            if verbose and len(matched_indices) <= 5:
                print(f"âœ… ãƒãƒƒãƒãƒ³ã‚° {len(matched_indices)}: æ—¢å­˜#{exist_idx} â†’ å€™è£œ#{best_candidate_idx} (è·é›¢: {min_distance:.4f})")

    # é‡è¤‡é™¤å»
    unique_matched = list(set(matched_indices))

    print(f"ğŸ“Š ãƒãƒƒãƒãƒ³ã‚°çµæœ:")
    print(f"  - åˆæœŸãƒãƒƒãƒæ•°: {len(matched_indices)}")
    print(f"  - é‡è¤‡é™¤å»å¾Œ: {len(unique_matched)}")
    print(f"  - ãƒãƒƒãƒãƒ³ã‚°ç‡: {len(unique_matched)/len(existing_data)*100:.1f}%")

    if len(unique_matched) == 0:
        print("âš ï¸ æ—¢å­˜å®Ÿé¨“ç‚¹ãŒãƒãƒƒãƒã—ã¾ã›ã‚“ã§ã—ãŸ")
        print("ğŸ’¡ è€ƒãˆã‚‰ã‚Œã‚‹åŸå› :")
        print("  1. æ—¢å­˜å®Ÿé¨“æ¡ä»¶ãŒå€™è£œç‚¹ã®è¨­å®šç¯„å›²å¤–")
        print("  2. åˆ»ã¿å¹…è¨­å®šãŒæ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¨åˆã‚ãªã„")
        print("  3. è¨±å®¹èª¤å·®è¨­å®šãŒå³ã—ã™ãã‚‹")

        # è¨ºæ–­æƒ…å ±ã®æä¾›
        print("\nğŸ” è¨ºæ–­æƒ…å ±:")
        for var in variable_names:
            exist_range = (existing_aligned[var].min(), existing_aligned[var].max())
            cand_range = (candidate_df[var].min(), candidate_df[var].max())
            print(f"  {var}: æ—¢å­˜{exist_range} vs å€™è£œ{cand_range}")

    return unique_matched

def hierarchical_candidate_reduction(candidate_points, max_candidates=5000, existing_indices=None):
    """éšå±¤çš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã«ã‚ˆã‚‹å€™è£œç‚¹å‰Šæ¸›"""
    n_original = len(candidate_points)

    if n_original <= max_candidates:
        print(f"ğŸ“Š å€™è£œç‚¹æ•°({n_original:,})ã¯å‰Šæ¸›ä¸è¦ï¼ˆé–¾å€¤: {max_candidates:,}ï¼‰")
        return candidate_points, list(range(n_original))

    print(f"ğŸ”„ âœ… éšå±¤çš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å®Ÿè¡Œ: {n_original:,} â†’ {max_candidates:,}ç‚¹ã«å‰Šæ¸›")

    # æ—¢å­˜å®Ÿé¨“ç‚¹ã‚’ä¿è­·
    if existing_indices:
        existing_set = set(existing_indices)
        available_indices = [i for i in range(n_original) if i not in existing_set]
        available_points = candidate_points[available_indices]
        n_to_select = max_candidates - len(existing_indices)
        print(f"ğŸ“ æ—¢å­˜å®Ÿé¨“ç‚¹ä¿æŒ: {len(existing_indices)}ç‚¹")
    else:
        available_indices = list(range(n_original))
        available_points = candidate_points
        n_to_select = max_candidates
        existing_indices = []

    if n_to_select <= 0:
        print("âš ï¸ æ—¢å­˜ç‚¹ã®ã¿ã§ä¸Šé™ã«é”ã—ã¾ã—ãŸ")
        return candidate_points[existing_indices], existing_indices

    print(f"ğŸ¯ æ–°è¦é¸å®šå¯¾è±¡: {n_to_select:,}ç‚¹")

    try:
        from sklearn.cluster import MiniBatchKMeans

        n_clusters = min(n_to_select, len(available_points))
        print(f"ğŸ”§ MiniBatchKMeansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°: {n_clusters}ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼")

        kmeans = MiniBatchKMeans(
            n_clusters=n_clusters, 
            random_state=42, 
            batch_size=min(1000, len(available_points)//10),
            n_init=3,
            max_iter=100
        )

        start_time = time.time()
        clusters = kmeans.fit_predict(available_points)
        clustering_time = time.time() - start_time
        print(f"â±ï¸ ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æ™‚é–“: {clustering_time:.2f}ç§’")

        # å„ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‹ã‚‰ä»£è¡¨ç‚¹ã‚’é¸æŠ
        selected_indices = list(existing_indices)

        for i in range(n_clusters):
            cluster_mask = clusters == i
            if np.any(cluster_mask):
                cluster_indices_in_available = np.where(cluster_mask)[0]
                cluster_original_indices = [available_indices[j] for j in cluster_indices_in_available]

                # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼é‡å¿ƒã«æœ€ã‚‚è¿‘ã„ç‚¹ã‚’é¸æŠ
                cluster_points = available_points[cluster_mask]
                center = kmeans.cluster_centers_[i]
                distances = np.linalg.norm(cluster_points - center, axis=1)
                closest_idx_in_cluster = np.argmin(distances)
                closest_original_idx = cluster_original_indices[closest_idx_in_cluster]

                selected_indices.append(closest_original_idx)

        reduced_points = candidate_points[selected_indices]

        print(f"âœ… éšå±¤çš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å®Œäº†: æœ€çµ‚å€™è£œç‚¹æ•° {len(reduced_points):,}")
        print(f"  - æ—¢å­˜å®Ÿé¨“ç‚¹ä¿æŒ: {len(existing_indices)}ç‚¹")
        print(f"  - æ–°è¦é¸å®šç‚¹: {len(selected_indices) - len(existing_indices)}ç‚¹")

        return reduced_points, selected_indices

    except Exception as e:
        print(f"âš ï¸ éšå±¤çš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
        return candidate_points, list(range(len(candidate_points)))

def calculate_d_criterion_stable(X, method='auto'):
    """æ•°å€¤çš„ã«å®‰å®šãªD-criterionè¨ˆç®—"""
    try:
        condition_number = np.linalg.cond(X)

        if USE_NUMERICAL_STABLE_METHOD or method == 'auto' and condition_number > 1e12:
            method = 'svd'
            if VERBOSE and condition_number > 1e12:
                print(f"ğŸ”§ é«˜æ¡ä»¶æ•°æ¤œå‡º({condition_number:.2e}) - SVDæ³•é©ç”¨")

        if method == 'svd':
            _, s, _ = np.linalg.svd(X, full_matrices=False)
            valid_singular_values = s[s > 1e-14]
            if len(valid_singular_values) == 0:
                return -np.inf, condition_number
            log_det = np.sum(np.log(valid_singular_values))
        else:
            q, r = qr(X, mode='economic')
            diag_r = np.diag(r)
            det = np.abs(np.prod(diag_r))
            log_det = np.log(det) if det > 1e-300 else -np.inf

        return log_det, condition_number

    except Exception as e:
        if VERBOSE:
            print(f"âš ï¸ D-criterionè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
        return -np.inf, np.inf

def select_d_optimal_design_enhanced(X_all, existing_indices, new_experiments, verbose=True):
    """D-optimalè¨­è¨ˆé¸å®šï¼ˆæ—¢å­˜å®Ÿé¨“ç‚¹ + æ–°è¦å®Ÿé¨“ç‚¹ï¼‰"""
    base = list(existing_indices) if existing_indices else []
    remaining = [i for i in range(len(X_all)) if i not in base]
    total_select = len(base) + new_experiments

    if verbose:
        print(f"  - æ—¢å­˜å®Ÿé¨“ç‚¹: {len(base)}ç‚¹")
        print(f"  - æ–°è¦å®Ÿé¨“ç‚¹: {new_experiments}ç‚¹")
        print(f"  - åˆè¨ˆé¸å®šç‚¹: {total_select}ç‚¹")

    if new_experiments <= 0:
        if verbose:
            print(f"  âœ… æ—¢å­˜å®Ÿé¨“ç‚¹ã®ã¿ã§å®Œäº†")
        score, _ = calculate_d_criterion_stable(X_all[base])
        return base, score

    selected = list(base)

    for step in range(new_experiments):
        best_candidate = None
        best_score = -np.inf

        # å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§ã¯ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        if len(remaining) > 1000:
            sample_size = min(500, len(remaining))
            sample_indices = np.random.choice(remaining, sample_size, replace=False)
            candidates_to_check = sample_indices
        else:
            candidates_to_check = remaining

        for idx in candidates_to_check:
            trial_set = selected + [idx]
            X_subset = X_all[trial_set]
            score, condition_num = calculate_d_criterion_stable(X_subset)

            if score > best_score:
                best_score = score
                best_candidate = idx

        if best_candidate is not None:
            selected.append(best_candidate)
            remaining.remove(best_candidate)
            if verbose:
                print(f"  âœ… æ–°è¦é¸å®š {step+1}/{new_experiments}: ç‚¹{best_candidate}, ã‚¹ã‚³ã‚¢: {best_score:.4f}")
        else:
            if verbose:
                print(f"  âš ï¸ ã‚¹ãƒ†ãƒƒãƒ—{step+1}ã§é©åˆ‡ãªå€™è£œç‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            break

    final_score, final_condition = calculate_d_criterion_stable(X_all[selected])
    return selected, final_score

def select_i_optimal_design(X_all, new_experiments, existing_indices=None):
    """I-optimalè¨­è¨ˆé¸å®šï¼ˆæ—¢å­˜å®Ÿé¨“ç‚¹ + æ–°è¦å®Ÿé¨“ç‚¹ï¼‰"""
    if existing_indices:
        selected_indices = list(existing_indices)
        print(f"  - æ—¢å­˜å®Ÿé¨“ç‚¹: {len(existing_indices)}ç‚¹")
        print(f"  - æ–°è¦å®Ÿé¨“ç‚¹: {new_experiments}ç‚¹")
        print(f"  - åˆè¨ˆé¸å®šç‚¹: {len(existing_indices) + new_experiments}ç‚¹")
    else:
        selected_indices = [0]
        print(f"  - æ–°è¦å®Ÿé¨“ç‚¹: {new_experiments}ç‚¹ï¼ˆæ—¢å­˜ç‚¹ãªã—ï¼‰")

    remaining_indices = [i for i in range(len(X_all)) if i not in selected_indices]
    target_total = len(selected_indices) + new_experiments

    step = 0
    while len(selected_indices) < target_total and remaining_indices:
        dists = cdist(X_all[remaining_indices], X_all[selected_indices])
        min_dists = dists.min(axis=1)
        next_idx_in_remaining = np.argmax(min_dists)
        next_index = remaining_indices[next_idx_in_remaining]
        selected_indices.append(next_index)
        remaining_indices.remove(next_index)
        step += 1
        print(f"  âœ… æ–°è¦é¸å®š {step}/{new_experiments}: ç‚¹{next_index}")

    return selected_indices

def generate_candidate_points(design_df):
    """å€™è£œç‚¹ç”Ÿæˆ"""
    levels = []
    for _, row in design_df.iterrows():
        levels.append(np.arange(row["æœ€å°å€¤"], row["æœ€å¤§å€¤"] + row["åˆ»ã¿å¹…"], row["åˆ»ã¿å¹…"]))
    return np.array(list(product(*levels)))

# =================== ğŸ“Š å¯è¦–åŒ–æ©Ÿèƒ½å¼·åŒ– ===================

def save_hyperparameters(params, filepath=HYPERPARAMETER_CACHE_FILE):
    """ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ä¿å­˜"""
    try:
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(params, f, indent=2, ensure_ascii=False)
        print(f"âœ… ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ä¿å­˜: {filepath}")
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

def load_hyperparameters(filepath=HYPERPARAMETER_CACHE_FILE):
    """ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼èª­ã¿è¾¼ã¿"""
    try:
        if os.path.exists(filepath):
            with open(filepath, 'r', encoding='utf-8') as f:
                params = json.load(f)
            print(f"âœ… ä¿å­˜æ¸ˆã¿ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼èª­ã¿è¾¼ã¿: {filepath}")
            return params
        return None
    except Exception as e:
        print(f"âš ï¸ èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def umap_objective_function(trial, X_scaled, d_indices, i_indices, existing_indices):
    """UMAPæœ€é©åŒ–ç›®çš„é–¢æ•°"""
    try:
        import umap

        n_neighbors = trial.suggest_int("n_neighbors", 5, 50)
        min_dist = trial.suggest_float("min_dist", 0.0, 0.5)

        # è¨ˆç®—åŠ¹ç‡ã®ãŸã‚ã€ãƒ‡ãƒ¼ã‚¿æ•°ãŒå¤šã„å ´åˆã¯ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        if len(X_scaled) > 2000:
            sample_indices = np.random.choice(len(X_scaled), 2000, replace=False)
            X_sample = X_scaled[sample_indices]

            # ãƒ©ãƒ™ãƒ«ã‚‚å¯¾å¿œã—ã¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            d_sample = [i for i, idx in enumerate(sample_indices) if idx in d_indices]
            i_sample = [i for i, idx in enumerate(sample_indices) if idx in i_indices]
            existing_sample = [i for i, idx in enumerate(sample_indices) if idx in existing_indices]
        else:
            X_sample = X_scaled
            d_sample = d_indices
            i_sample = i_indices
            existing_sample = existing_indices

        reducer = umap.UMAP(
            n_neighbors=n_neighbors, 
            min_dist=min_dist, 
            n_components=2,
            random_state=42,
            n_jobs=1
        )
        embedding = reducer.fit_transform(X_sample)

        # è©•ä¾¡æŒ‡æ¨™ã®è¨ˆç®—
        labels = np.zeros(len(X_sample))
        if d_sample:
            labels[d_sample] = 1
        if i_sample:
            labels[i_sample] = 2
        if existing_sample:
            labels[existing_sample] = 3

        # åˆ†é›¢åº¦ã®è¨ˆç®—
        unique_labels = np.unique(labels)
        if len(unique_labels) < 2:
            return 0.0

        centroids = []
        for label in unique_labels:
            if np.any(labels == label):
                centroid = embedding[labels == label].mean(axis=0)
                centroids.append(centroid)

        centroids = np.array(centroids)
        separation_score = np.mean(cdist(centroids, centroids)[np.triu_indices_from(centroids, k=1)])

        # å‡é›†åº¦ã®è¨ˆç®—
        cohesion_scores = []
        for label in unique_labels:
            cluster_points = embedding[labels == label]
            if len(cluster_points) > 1:
                centroid = cluster_points.mean(axis=0)
                distances = np.linalg.norm(cluster_points - centroid, axis=1)
                cohesion_scores.append(1.0 / (1.0 + np.mean(distances)))

        cohesion_score = np.mean(cohesion_scores) if cohesion_scores else 0.0

        # ç·åˆã‚¹ã‚³ã‚¢
        total_score = 0.7 * separation_score + 0.3 * cohesion_score
        return total_score

    except Exception as e:
        return 0.0

def get_umap_params_optimized(X_scaled, d_indices, i_indices, existing_indices):
    """UMAP ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼æœ€é©åŒ–ï¼ˆé¸æŠå¯èƒ½ï¼‰"""
    # ä¿å­˜æ¸ˆã¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã®ç¢ºèª
    cached_params = load_hyperparameters()

    if cached_params is not None and not FORCE_REOPTIMIZATION:
        print(f"ğŸ“ ä¿å­˜æ¸ˆã¿UMAPãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ä½¿ç”¨: {cached_params}")
        print("ğŸ’¡ ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼æœ€é©åŒ–ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆæ—¢ã«æœ€é©åŒ–æ¸ˆã¿ï¼‰")
        return cached_params

    elif ENABLE_HYPERPARAMETER_TUNING:
        if cached_params is None:
            print("ğŸ” åˆå›å®Ÿè¡Œ: UMAPãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼æœ€é©åŒ–ã‚’é–‹å§‹...")
        else:
            print("ğŸ”„ å¼·åˆ¶å†æœ€é©åŒ–: UMAPãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼æœ€é©åŒ–ã‚’å®Ÿè¡Œ...")

        print("âš™ï¸ æœ€é©åŒ–è¨­å®š:")
        print(f"  - è©¦è¡Œå›æ•°: 20å›ï¼ˆåŠ¹ç‡é‡è¦–ï¼‰")
        print(f"  - è©•ä¾¡æŒ‡æ¨™: åˆ†é›¢åº¦70% + å‡é›†åº¦30%")
        print(f"  - ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°: 2000ç‚¹ä¸Šé™ï¼ˆå¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿å¯¾å¿œï¼‰")

        start_time = time.time()

        study = optuna.create_study(
            direction="maximize", 
            sampler=optuna.samplers.TPESampler(seed=42)
        )
        study.optimize(
            lambda trial: umap_objective_function(trial, X_scaled, d_indices, i_indices, existing_indices), 
            n_trials=20,
            show_progress_bar=False
        )

        optimization_time = time.time() - start_time
        best_params = study.best_params

        print(f"âœ… æœ€é©åŒ–å®Œäº† - å®Ÿè¡Œæ™‚é–“: {optimization_time:.1f}ç§’")
        print(f"ğŸ¯ æœ€é©ã‚¹ã‚³ã‚¢: {study.best_value:.4f}")
        print(f"ğŸ”§ æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼: {best_params}")

        # çµæœã‚’ä¿å­˜
        save_hyperparameters(best_params)
        print("ğŸ’¾ æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’ä¿å­˜ï¼ˆæ¬¡å›å®Ÿè¡Œæ™‚ã«è‡ªå‹•ä½¿ç”¨ï¼‰")

        return best_params

    else:
        print("âš™ï¸ ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼æœ€é©åŒ–ç„¡åŠ¹ - ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ä½¿ç”¨")
        print(f"ğŸ“‹ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆUMAPãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼: {DEFAULT_UMAP_PARAMS}")
        return DEFAULT_UMAP_PARAMS

def visualize_feature_histograms(candidate_df, d_indices, i_indices, existing_indices, variable_names):
    """ğŸ“Š ä¿®æ­£1: å„ç‰¹å¾´é‡ã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ è‰²åˆ†ã‘è¡¨ç¤º"""
    print(f"\nğŸ“Š ç‰¹å¾´é‡åˆ†å¸ƒå¯è¦–åŒ–é–‹å§‹")

    n_features = len(variable_names)
    n_cols = 3
    n_rows = (n_features + n_cols - 1) // n_cols

    plt.figure(figsize=(15, 4 * n_rows))

    for i, var_name in enumerate(variable_names):
        plt.subplot(n_rows, n_cols, i + 1)

        # å…¨å€™è£œç‚¹ã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ï¼ˆèƒŒæ™¯ï¼‰
        plt.hist(candidate_df[var_name], bins=30, alpha=0.3, color='lightgray', 
                label=f'å…¨å€™è£œç‚¹ ({len(candidate_df)})', density=True)

        # æ—¢å­˜å®Ÿé¨“ç‚¹
        if existing_indices:
            existing_values = candidate_df.iloc[existing_indices][var_name]
            plt.hist(existing_values, bins=15, alpha=0.8, color='blue', 
                    label=f'æ—¢å­˜å®Ÿé¨“ç‚¹ ({len(existing_indices)})', density=True)

        # D-optimalæ–°è¦ç‚¹
        d_new_indices = [idx for idx in d_indices if idx not in existing_indices]
        if d_new_indices:
            d_values = candidate_df.iloc[d_new_indices][var_name]
            plt.hist(d_values, bins=10, alpha=0.8, color='red', 
                    label=f'D-optimalæ–°è¦ ({len(d_new_indices)})', density=True)

        # I-optimalæ–°è¦ç‚¹
        i_new_indices = [idx for idx in i_indices if idx not in existing_indices]
        if i_new_indices:
            i_values = candidate_df.iloc[i_new_indices][var_name]
            plt.hist(i_values, bins=10, alpha=0.8, color='green', 
                    label=f'I-optimalæ–°è¦ ({len(i_new_indices)})', density=True)

        plt.title(f'{var_name}ã®åˆ†å¸ƒ', fontsize=12, weight='bold')
        plt.xlabel(var_name)
        plt.ylabel('å¯†åº¦')
        plt.legend(fontsize=8)
        plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

    print(f"âœ… ç‰¹å¾´é‡åˆ†å¸ƒå¯è¦–åŒ–å®Œäº†")

def visualize_umap_enhanced(X_scaled, d_indices, i_indices, existing_indices, variable_names):
    """ğŸ“ˆ ä¿®æ­£2: UMAPæ¬¡å…ƒå‰Šæ¸›å¯è¦–åŒ–ï¼ˆãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼æœ€é©åŒ–ä»˜ãï¼‰"""
    print(f"\nğŸ“ˆ UMAPæ¬¡å…ƒå‰Šæ¸›å¯è¦–åŒ–é–‹å§‹")

    try:
        import umap

        # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼æœ€é©åŒ–
        best_params = get_umap_params_optimized(X_scaled, d_indices, i_indices, existing_indices)

        # UMAPå®Ÿè¡Œ
        print(f"ğŸ”§ UMAPå®Ÿè¡Œä¸­...")
        reducer = umap.UMAP(
            n_neighbors=best_params["n_neighbors"], 
            min_dist=best_params["min_dist"], 
            n_components=2, 
            random_state=42,
            verbose=False
        )

        start_time = time.time()
        reduced_umap = reducer.fit_transform(X_scaled)
        umap_time = time.time() - start_time
        print(f"â±ï¸ UMAPå®Ÿè¡Œæ™‚é–“: {umap_time:.2f}ç§’")

        # å¯è¦–åŒ–
        plt.figure(figsize=(14, 10))

        # PCAã¨UMAPã®æ¯”è¼ƒè¡¨ç¤º
        plt.subplot(2, 2, 1)
        # PCAå¯è¦–åŒ–
        pca = PCA(n_components=2, random_state=42)
        reduced_pca = pca.fit_transform(X_scaled)

        # å…¨å€™è£œç‚¹ï¼ˆèƒŒæ™¯ï¼‰
        plt.scatter(reduced_pca[:, 0], reduced_pca[:, 1], alpha=0.2, s=8, color='lightgray', label='å€™è£œç‚¹')

        # æ—¢å­˜å®Ÿé¨“ç‚¹
        if existing_indices:
            existing_pca = reduced_pca[existing_indices]
            plt.scatter(existing_pca[:, 0], existing_pca[:, 1], 
                       s=120, color='blue', alpha=0.9, marker='o', 
                       edgecolors='navy', linewidth=2, zorder=10,
                       label=f'æ—¢å­˜å®Ÿé¨“ç‚¹ ({len(existing_indices)})')

        # D-optimalæ–°è¦ç‚¹
        d_new = [idx for idx in d_indices if idx not in existing_indices]
        if d_new:
            d_pca = reduced_pca[d_new]
            plt.scatter(d_pca[:, 0], d_pca[:, 1], 
                       s=100, marker='x', color='red', linewidth=3, 
                       zorder=8, label=f'D-optimalæ–°è¦ ({len(d_new)})')

        # I-optimalæ–°è¦ç‚¹
        i_new = [idx for idx in i_indices if idx not in existing_indices]
        if i_new:
            i_pca = reduced_pca[i_new]
            plt.scatter(i_pca[:, 0], i_pca[:, 1], 
                       s=100, marker='^', color='green', 
                       zorder=8, label=f'I-optimalæ–°è¦ ({len(i_new)})')

        plt.title('PCAæ¬¡å…ƒå‰Šæ¸›', fontsize=14, weight='bold')
        plt.xlabel(f'ç¬¬1ä¸»æˆåˆ† ({pca.explained_variance_ratio_[0]:.1%})')
        plt.ylabel(f'ç¬¬2ä¸»æˆåˆ† ({pca.explained_variance_ratio_[1]:.1%})')
        plt.legend(fontsize=9)
        plt.grid(True, alpha=0.3)

        # UMAPå¯è¦–åŒ–
        plt.subplot(2, 2, 2)

        # å…¨å€™è£œç‚¹ï¼ˆèƒŒæ™¯ï¼‰
        plt.scatter(reduced_umap[:, 0], reduced_umap[:, 1], alpha=0.2, s=8, color='lightgray', label='å€™è£œç‚¹')

        # æ—¢å­˜å®Ÿé¨“ç‚¹
        if existing_indices:
            existing_umap = reduced_umap[existing_indices]
            plt.scatter(existing_umap[:, 0], existing_umap[:, 1], 
                       s=120, color='blue', alpha=0.9, marker='o', 
                       edgecolors='navy', linewidth=2, zorder=10,
                       label=f'æ—¢å­˜å®Ÿé¨“ç‚¹ ({len(existing_indices)})')

            # æ—¢å­˜å®Ÿé¨“ç‚¹ã«ç•ªå·è¡¨ç¤ºï¼ˆæœ€åˆã®10ç‚¹ã¾ã§ï¼‰
            for i, (x, y) in enumerate(existing_umap[:min(10, len(existing_umap))]):
                plt.annotate(f'{i+1}', (x, y), xytext=(3, 3), 
                           textcoords='offset points', fontsize=8, 
                           color='darkblue', weight='bold', zorder=11)

        # D-optimalæ–°è¦ç‚¹
        if d_new:
            d_umap = reduced_umap[d_new]
            plt.scatter(d_umap[:, 0], d_umap[:, 1], 
                       s=100, marker='x', color='red', linewidth=3, 
                       zorder=8, label=f'D-optimalæ–°è¦ ({len(d_new)})')

        # I-optimalæ–°è¦ç‚¹
        if i_new:
            i_umap = reduced_umap[i_new]
            plt.scatter(i_umap[:, 0], i_umap[:, 1], 
                       s=100, marker='^', color='green', 
                       zorder=8, label=f'I-optimalæ–°è¦ ({len(i_new)})')

        plt.title(f'UMAPæ¬¡å…ƒå‰Šæ¸› (n_neighbors={best_params["n_neighbors"]}, min_dist={best_params["min_dist"]:.3f})', 
                 fontsize=14, weight='bold')
        plt.xlabel('UMAPæ¬¡å…ƒ1')
        plt.ylabel('UMAPæ¬¡å…ƒ2')
        plt.legend(fontsize=9)
        plt.grid(True, alpha=0.3)

        # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼æœ€é©åŒ–æƒ…å ±è¡¨ç¤º
        plt.subplot(2, 1, 2)
        plt.axis('off')

        info_text = f"""
ğŸ”§ UMAPæœ€é©åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼:
   â€¢ n_neighbors: {best_params["n_neighbors"]} (è¿‘å‚ç‚¹æ•°)
   â€¢ min_dist: {best_params["min_dist"]:.3f} (æœ€å°è·é›¢)

ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆçµ±è¨ˆ:
   â€¢ å…¨å€™è£œç‚¹: {len(X_scaled):,}ç‚¹
   â€¢ æ—¢å­˜å®Ÿé¨“ç‚¹: {len(existing_indices)}ç‚¹
   â€¢ D-optimalç·ç‚¹æ•°: {len(d_indices)}ç‚¹ (æ—¢å­˜{len([i for i in d_indices if i in existing_indices])} + æ–°è¦{len(d_new)})
   â€¢ I-optimalç·ç‚¹æ•°: {len(i_indices)}ç‚¹ (æ—¢å­˜{len([i for i in i_indices if i in existing_indices])} + æ–°è¦{len(i_new)})

â±ï¸ å‡¦ç†æ™‚é–“:
   â€¢ UMAPå®Ÿè¡Œ: {umap_time:.2f}ç§’
        """

        plt.text(0.05, 0.95, info_text, transform=plt.gca().transAxes, 
                fontsize=11, verticalalignment='top', 
                bbox=dict(boxstyle="round,pad=0.5", facecolor="lightblue", alpha=0.8))

        plt.tight_layout()
        plt.show()

        print(f"âœ… UMAPæ¬¡å…ƒå‰Šæ¸›å¯è¦–åŒ–å®Œäº†")

    except ImportError:
        print("âŒ UMAPæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« - PCAã®ã¿è¡¨ç¤º")
        # PCAãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¯è¦–åŒ–
        pca = PCA(n_components=2, random_state=42)
        reduced_pca = pca.fit_transform(X_scaled)

        plt.figure(figsize=(10, 8))
        plt.scatter(reduced_pca[:, 0], reduced_pca[:, 1], alpha=0.2, s=8, color='lightgray', label='å€™è£œç‚¹')

        if existing_indices:
            existing_pca = reduced_pca[existing_indices]
            plt.scatter(existing_pca[:, 0], existing_pca[:, 1], 
                       s=120, color='blue', alpha=0.9, marker='o', 
                       edgecolors='navy', linewidth=2, zorder=10,
                       label=f'æ—¢å­˜å®Ÿé¨“ç‚¹ ({len(existing_indices)})')

        d_new = [idx for idx in d_indices if idx not in existing_indices]
        if d_new:
            d_pca = reduced_pca[d_new]
            plt.scatter(d_pca[:, 0], d_pca[:, 1], 
                       s=100, marker='x', color='red', linewidth=3, 
                       zorder=8, label=f'D-optimalæ–°è¦ ({len(d_new)})')

        i_new = [idx for idx in i_indices if idx not in existing_indices]
        if i_new:
            i_pca = reduced_pca[i_new]
            plt.scatter(i_pca[:, 0], i_pca[:, 1], 
                       s=100, marker='^', color='green', 
                       zorder=8, label=f'I-optimalæ–°è¦ ({len(i_new)})')

        plt.title('PCAæ¬¡å…ƒå‰Šæ¸›ï¼ˆUMAPãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰', fontsize=14, weight='bold')
        plt.xlabel(f'ç¬¬1ä¸»æˆåˆ† ({pca.explained_variance_ratio_[0]:.1%})')
        plt.ylabel(f'ç¬¬2ä¸»æˆåˆ† ({pca.explained_variance_ratio_[1]:.1%})')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.show()

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°ï¼ˆå¯è¦–åŒ–æ©Ÿèƒ½å¼·åŒ–ç‰ˆï¼‰"""
    print("ğŸš€ åŒ–å­¦å®Ÿé¨“è¨ˆç”»æ³•ã‚·ã‚¹ãƒ†ãƒ  - å¯è¦–åŒ–æ©Ÿèƒ½å¼·åŒ–ç‰ˆ")
    print("="*60)
    print("ğŸ“Š ä¿®æ­£1: å„ç‰¹å¾´é‡ã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ è‰²åˆ†ã‘è¡¨ç¤º")
    print("ğŸ“ˆ ä¿®æ­£2: UMAPæ¬¡å…ƒå‰Šæ¸›å¯è¦–åŒ–ï¼ˆãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼æœ€é©åŒ–æ©Ÿèƒ½ä»˜ãï¼‰")
    print("="*60)

    # è¨­å®šèª­ã¿è¾¼ã¿
    try:
        design_df = pd.read_excel(SETTING_FILE, sheet_name=SHEET_DESIGN)
        info_df = pd.read_excel(SETTING_FILE, sheet_name=SHEET_INFO)
        n_experiments = int(info_df.loc[info_df["è¨­å®šé …ç›®"] == "å®Ÿé¨“æ•°", "å€¤"].values[0])
        print(f"ğŸ“‹ è¨­å®šèª­ã¿è¾¼ã¿å®Œäº†")
        print(f"  - èª¬æ˜å¤‰æ•°æ•°: {len(design_df)}")
        print(f"  - ç›®æ¨™å®Ÿé¨“æ•°: {n_experiments}")
    except Exception as e:
        print(f"âŒ è¨­å®šèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return

    # å€™è£œç‚¹ç”Ÿæˆ
    print(f"\nğŸ“Š å€™è£œç‚¹ç”Ÿæˆä¸­...")
    candidate_points = generate_candidate_points(design_df)
    print(f"âœ… åˆæœŸå€™è£œç‚¹ç”Ÿæˆå®Œäº†: {len(candidate_points):,}ç‚¹")

    # æ—¢å­˜å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†
    existing_indices = []
    if USE_EXISTING_DATA:
        print(f"\nğŸ” æ—¢å­˜å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿å‡¦ç†é–‹å§‹")

        existing_data, available_vars = load_and_validate_existing_data(
            EXISTING_DATA_FILE, design_df, verbose=True
        )

        if existing_data is not None and len(existing_data) > 0:
            variable_names = design_df["èª¬æ˜å¤‰æ•°å"].tolist()
            existing_indices = match_existing_experiments_enhanced(
                candidate_points, existing_data, variable_names,
                tolerance_relative=1e-4,
                tolerance_absolute=1e-6,
                verbose=True
            )
        else:
            print("âŒ æ—¢å­˜å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")

    # å€™è£œç‚¹å‰Šæ¸›
    original_candidate_count = len(candidate_points)
    should_reduce = len(candidate_points) > CANDIDATE_REDUCTION_THRESHOLD

    if should_reduce:
        print(f"\nğŸ”„ å€™è£œç‚¹å‰Šæ¸›å®Ÿè¡Œ: {original_candidate_count:,} â†’ {MAX_REDUCED_CANDIDATES:,}")
        candidate_points, reduced_mapping = hierarchical_candidate_reduction(
            candidate_points, MAX_REDUCED_CANDIDATES, existing_indices
        )

        if existing_indices:
            existing_indices = [reduced_mapping.index(idx) for idx in existing_indices if idx in reduced_mapping]
            print(f"âœ… æ—¢å­˜å®Ÿé¨“ç‚¹ãƒãƒƒãƒ”ãƒ³ã‚°æ›´æ–°: {len(existing_indices)}ä»¶ä¿æŒ")

    print(f"\nâœ… æœ€çµ‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ:")
    print(f"  - æœ€çµ‚å€™è£œç‚¹æ•°: {len(candidate_points):,}")
    print(f"  - æ—¢å­˜å®Ÿé¨“ç‚¹æ•°: {len(existing_indices)}")
    print(f"  - æ—¢å­˜å®Ÿé¨“æ´»ç”¨ç‡: {len(existing_indices)/n_experiments*100:.1f}%")

    # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
    candidate_df = pd.DataFrame(candidate_points, columns=design_df["èª¬æ˜å¤‰æ•°å"])
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(candidate_points)
    print(f"âœ… ãƒ‡ãƒ¼ã‚¿æ¨™æº–åŒ–å®Œäº†")

    # D-optimalè¨­è¨ˆ
    print(f"\nğŸ¯ D-optimalè¨­è¨ˆå®Ÿè¡Œ")
    d_indices, d_score = select_d_optimal_design_enhanced(
        X_scaled, existing_indices, n_experiments, verbose=VERBOSE
    )
    print(f"âœ… D-optimalè¨­è¨ˆå®Œäº†")
    print(f"  - æœ€çµ‚ã‚¹ã‚³ã‚¢: {d_score:.4f}")
    print(f"  - ç·é¸å®šç‚¹æ•°: {len(d_indices)}")
    print(f"  - æ—¢å­˜ç‚¹: {len([i for i in d_indices if i in existing_indices])}ç‚¹")
    print(f"  - æ–°è¦ç‚¹: {len([i for i in d_indices if i not in existing_indices])}ç‚¹")

    # I-optimalè¨­è¨ˆ
    print(f"\nğŸ¯ I-optimalè¨­è¨ˆå®Ÿè¡Œ")
    i_indices = select_i_optimal_design(X_scaled, n_experiments, existing_indices)
    print(f"âœ… I-optimalè¨­è¨ˆå®Œäº†")
    print(f"  - ç·é¸å®šç‚¹æ•°: {len(i_indices)}")
    print(f"  - æ—¢å­˜ç‚¹: {len([i for i in i_indices if i in existing_indices])}ç‚¹")
    print(f"  - æ–°è¦ç‚¹: {len([i for i in i_indices if i not in existing_indices])}ç‚¹")

    # çµæœå‡¦ç†ï¼ˆæ–°è¦å®Ÿé¨“ç‚¹ã®ã¿æŠ½å‡ºï¼‰
    d_new_indices = [idx for idx in d_indices if idx not in existing_indices]
    i_new_indices = [idx for idx in i_indices if idx not in existing_indices]

    selected_d_df = candidate_df.iloc[d_new_indices] if d_new_indices else pd.DataFrame()
    selected_i_df = candidate_df.iloc[i_new_indices] if i_new_indices else pd.DataFrame()

    print(f"\nğŸ“Š é¸å®šçµæœã‚µãƒãƒªãƒ¼:")
    print(f"  - æ—¢å­˜å®Ÿé¨“ç‚¹æ´»ç”¨: {len(existing_indices)}ç‚¹")
    print(f"  - D-optimalæ–°è¦é¸å®š: {len(d_new_indices)}ç‚¹")
    print(f"  - I-optimalæ–°è¦é¸å®š: {len(i_new_indices)}ç‚¹")
    print(f"  - D-optimalç·å®Ÿé¨“ç‚¹: {len(d_indices)}ç‚¹")
    print(f"  - I-optimalç·å®Ÿé¨“ç‚¹: {len(i_indices)}ç‚¹")

    # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
    if len(selected_d_df) > 0:
        selected_d_df.to_excel("D_optimal_æ–°è¦å®Ÿé¨“ç‚¹.xlsx", index=False)
    if len(selected_i_df) > 0:
        selected_i_df.to_excel("I_optimal_æ–°è¦å®Ÿé¨“ç‚¹.xlsx", index=False)

    all_d_df = candidate_df.iloc[d_indices].copy()
    all_i_df = candidate_df.iloc[i_indices].copy()
    all_d_df['ãƒ‡ãƒ¼ã‚¿ç¨®åˆ¥'] = ['æ—¢å­˜' if i in existing_indices else 'æ–°è¦' for i in d_indices]
    all_i_df['ãƒ‡ãƒ¼ã‚¿ç¨®åˆ¥'] = ['æ—¢å­˜' if i in existing_indices else 'æ–°è¦' for i in i_indices]
    all_d_df.to_excel("D_optimal_å…¨å®Ÿé¨“ç‚¹.xlsx", index=False)
    all_i_df.to_excel("I_optimal_å…¨å®Ÿé¨“ç‚¹.xlsx", index=False)

    candidate_df.to_excel("å€™è£œç‚¹ä¸€è¦§_v2.xlsx", index=False)
    print(f"ğŸ’¾ çµæœãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†")

    # =============== ğŸ“Š å¯è¦–åŒ–å®Ÿè¡Œ ===============
    variable_names = design_df["èª¬æ˜å¤‰æ•°å"].tolist()

    # ä¿®æ­£1: ç‰¹å¾´é‡ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ è¡¨ç¤º
    visualize_feature_histograms(candidate_df, d_indices, i_indices, existing_indices, variable_names)

    # ä¿®æ­£2: UMAPæ¬¡å…ƒå‰Šæ¸›å¯è¦–åŒ–
    visualize_umap_enhanced(X_scaled, d_indices, i_indices, existing_indices, variable_names)

    print(f"\nğŸ‰ åŒ–å­¦å®Ÿé¨“è¨ˆç”»æ³•ã‚·ã‚¹ãƒ†ãƒ å®Œäº†ï¼ˆå¯è¦–åŒ–æ©Ÿèƒ½å¼·åŒ–ç‰ˆï¼‰")
    print("="*60)
    print("âœ… æ—¢å­˜å®Ÿé¨“ç‚¹ã‚’æ´»ç”¨ã—ãŸæœ€é©å®Ÿé¨“è¨ˆç”»ãŒå®Œæˆã—ã¾ã—ãŸ")
    print(f"ğŸ“ æ–°è¦å®Ÿé¨“ç‚¹ãƒ•ã‚¡ã‚¤ãƒ«: D_optimal_æ–°è¦å®Ÿé¨“ç‚¹.xlsx, I_optimal_æ–°è¦å®Ÿé¨“ç‚¹.xlsx")
    print(f"ğŸ“ å…¨å®Ÿé¨“ç‚¹ãƒ•ã‚¡ã‚¤ãƒ«: D_optimal_å…¨å®Ÿé¨“ç‚¹.xlsx, I_optimal_å…¨å®Ÿé¨“ç‚¹.xlsx")
    print("ğŸ“Š å¯è¦–åŒ–: ç‰¹å¾´é‡åˆ†å¸ƒãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ  + UMAPæ¬¡å…ƒå‰Šæ¸›")
    print("="*60)

if __name__ == "__main__":
    main()



# In[ ]:




