import os
import sys
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆè¨­å®š
PROJECT_ROOT = Path.cwd()
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

# Pythonã‚³ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ«ãƒ€ã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
PYTHON_CODE_FOLDER = PROJECT_ROOT / "00_Pythonã‚³ãƒ¼ãƒ‰"
if str(PYTHON_CODE_FOLDER) not in sys.path:
    sys.path.insert(0, str(PYTHON_CODE_FOLDER))

import pandas as pd
import numpy as np
import joblib
from config import Config

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
import matplotlib.pyplot as plt
plt.rcParams['font.family'] = Config.JAPANESE_FONT_FAMILY
plt.rcParams['axes.unicode_minus'] = Config.JAPANESE_FONT_UNICODE_MINUS

# --- å®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«è­˜åˆ¥ãƒ­ã‚°ï¼ˆè¤‡è£½ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚„ã‚«ãƒ¼ãƒãƒ«æ··ç·šã®æ—©æœŸç™ºè¦‹ç”¨ï¼‰ ---
import hashlib, time
def _print_self_identity():
    try:
        p = os.path.realpath(__file__)
        with open(p, 'rb') as f:
            h = hashlib.sha1(f.read()).hexdigest()[:10]
        mtime = time.ctime(os.path.getmtime(p))
        print(f"[RUNNING] {p}  mtime={mtime}  sha1={h}")
    except Exception as _e:
        # å¤±æ•—ã—ã¦ã‚‚å‡¦ç†ã¯ç¶™ç¶š
        pass
_print_self_identity()

# === Fail-Fast===
def assert_config_minimum(Config):
    required = [
        "RESULT_FOLDER","MODEL_FOLDER","PREDICTION_FOLDER","PREDICTION_DATA",
        "TARGET_COLUMNS","FEATURE_COLUMNS","RANDOM_STATE","get_n_jobs"
    ]
    missing = [k for k in required if not hasattr(Config, k)]
    if missing:
        raise RuntimeError(f"Configä¸è¶³: {missing}")
    n_jobs = int(Config.get_n_jobs("default"))
    if n_jobs < 1:
        raise RuntimeError("Config.get_n_jobs('default') ã¯ 1ä»¥ä¸ŠãŒå¿…è¦ã§ã™ã€‚")

def assert_some_features_exist(df, feature_cols):
    exists = [c for c in feature_cols if c in df.columns]
    if not exists:
        raise RuntimeError("FEATURE_COLUMNS ã«è©²å½“ã™ã‚‹åˆ—ãŒ1ã¤ã‚‚äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ã«å­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
    return exists

# ===== ãƒ¦ãƒ¼ã‚¶è¨­å®š =====
RESULT_FOLDER = Config.RESULT_FOLDER
TARGET_COLUMNS = Config.TARGET_COLUMNS
PREDICTION_FOLDER = Config.PREDICTION_FOLDER
PREDICTION_DATA = Config.PREDICTION_DATA
os.makedirs(PREDICTION_FOLDER, exist_ok=True)

assert_config_minimum(Config)


# === robust loader for saved pipelines (A+E) ===
def _describe_saved_obj(obj):
    if hasattr(obj, "transform"):
        return "sklearn_pipeline"
    if isinstance(obj, dict):
        if "sk_pipeline" in obj:
            return "dict+sk_pipeline"
        return "dict"
    return type(obj).__name__

def apply_saved_pipeline(X_new: pd.DataFrame, bundle: dict, pipeline_path: str) -> np.ndarray:
    """
    dict / sklearn.Pipeline / bundle(preprocessor+selector) ã®é †ã§é©ç”¨ã€‚
    æ–¹å¼E: schema_version / pipeline_format ãŒã‚ã‚Œã°ãƒ­ã‚°ã«å‡ºã™ã€‚
    """
    X_tmp = X_new.copy()

    # 1) pipeline_{target}.pkl ã‚’è©¦ã™
    if os.path.exists(pipeline_path):
        pipe_obj = joblib.load(pipeline_path)
        fmt = _describe_saved_obj(pipe_obj)
        schema = None
        if isinstance(pipe_obj, dict):
            schema = pipe_obj.get("schema_version", None)
        print(f"â„¹ æ—¢å­˜ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ¤œå‡º: format={fmt}" + (f", schema={schema}" if schema else ""))

        # 1-1) sklearn.Pipeline ã‚’æœ€å„ªå…ˆ
        if hasattr(pipe_obj, "transform"):
            return pipe_obj.transform(X_tmp)

        # 1-2) dictï¼ˆsk_pipeline ãŒåŒæ¢±ã•ã‚Œã¦ã„ã‚Œã°ãã‚Œã‚’å„ªå…ˆï¼‰
        if isinstance(pipe_obj, dict):
            # a) sk_pipeline ãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ã†
            if "sk_pipeline" in pipe_obj and hasattr(pipe_obj["sk_pipeline"], "transform"):
                return pipe_obj["sk_pipeline"].transform(X_tmp)

            # b) pre + sel ãŒä¸¡æ–¹ã‚ã‚Œã°é †ã«é©ç”¨
            pre = pipe_obj.get("preprocessor", None)
            sel = pipe_obj.get("selector", None)

            has_pre = (pre is not None and hasattr(pre, "transform"))
            has_sel = (sel is not None and hasattr(sel, "transform"))

            if has_pre and has_sel:
                X_tmp = pre.transform(X_tmp)
                X_tmp = sel.transform(X_tmp)
                return X_tmp

            if has_pre and not has_sel:
                # å‰å‡¦ç†ã ã‘ã‚ã‚‹ â†’ å‰å‡¦ç†ã¾ã§é©ç”¨ã—ã¦è¿”ã™ï¼ˆselectorä¸è¦ãªè¨­è¨ˆã‚‚è¨±å®¹ï¼‰
                X_tmp = pre.transform(X_tmp)
                return X_tmp

            if (not has_pre) and has_sel:
                # selector ã ã‘ã‚ã‚‹ â†’ å±é™ºï¼ˆå‰å‡¦ç†å‰ã®ç©ºé–“ã« selector ã‚’å½“ã¦ã‚‹æã‚Œï¼‰
                # â†’ ã“ã“ã§ã¯ä½¿ã‚ãšã€ä¸‹ã® bundle ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã«ä»»ã›ã‚‹
                print("â„¹ dictå†…ã«selectorã®ã¿æ¤œå‡º â†’ bundleã®preprocessor/selectorã¸ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™ã€‚")
                # ä½•ã‚‚returnã—ãªã„ï¼ˆã“ã®å¾Œã®bundleå‡¦ç†ã«é€²ã‚€ï¼‰


    # 2) ãƒãƒ³ãƒ‰ãƒ«ï¼ˆfinal_model_*.pklï¼‰ã¸ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    print("â„¹ pipelineãƒ•ã‚¡ã‚¤ãƒ«æœªä½¿ç”¨ â†’ bundle å†…ã® preprocessor/selector ã‚’é©ç”¨ã—ã¾ã™ã€‚")
    pre = bundle.get("preprocessor", None)
    sel = bundle.get("selector", None)
    if pre is None or sel is None:
        raise RuntimeError("å‰å‡¦ç†ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒè¦‹ã¤ã‹ã‚‰ãš transform ã§ãã¾ã›ã‚“ã€‚")
    X_tmp = pre.transform(X_tmp) if hasattr(pre, "transform") else X_tmp
    X_tmp = sel.transform(X_tmp) if hasattr(sel, "transform") else X_tmp
    return X_tmp


# ===== é€†å¤‰æ›ï¼ˆå­¦ç¿’æ™‚ã®method+shiftã«å³å¯†ä¸€è‡´ï¼‰ =====
def inverse_transform_target(y_transformed: np.ndarray, method: str, shift: float = 0.0) -> np.ndarray:
    y_t = np.asarray(y_transformed, dtype=float)
    if method == 'log':
        y_orig = np.exp(y_t)
        return y_orig - float(shift) if shift else y_orig
    elif method == 'sqrt':
        y_orig = np.square(y_t)
        return y_orig - float(shift) if shift else y_orig
    else:
        return y_t    

# ===== æ–°ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ =====
prediction_path = os.path.join(PREDICTION_FOLDER, PREDICTION_DATA)
if not os.path.exists(prediction_path):
    raise FileNotFoundError(f"äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {prediction_path}")

df_new = pd.read_excel(prediction_path)

print(f"âœ… äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿èª­è¾¼: {df_new.shape} from {prediction_path}")

# ===== åˆ‡å‰Šæ™‚é–“ã®è‡ªå‹•è¨ˆç®—ï¼ˆåˆ—ãŒç„¡ã„å ´åˆã®ã¿ï¼‰ =====
if Config.CUTTING_TIME_COLUMN_NAME not in df_new.columns:
    print(f"ğŸ”§ {Config.CUTTING_TIME_COLUMN_NAME}åˆ—ã‚’è‡ªå‹•è¨ˆç®—ä¸­...")
    if 'é€ã‚Šé€Ÿåº¦' in df_new.columns:
        with np.errstate(divide='ignore', invalid='ignore'):
            calc = (Config.CUTTING_DISTANCE_MM / df_new['é€ã‚Šé€Ÿåº¦'] * 60.0)
        df_new[Config.CUTTING_TIME_COLUMN_NAME] = np.round(calc.astype(float), 1)
        print(f"âœ… {Config.CUTTING_TIME_COLUMN_NAME}åˆ—ã‚’è¿½åŠ ã—ã¾ã—ãŸ")
        try:
            _min = df_new[Config.CUTTING_TIME_COLUMN_NAME].min()
            _max = df_new[Config.CUTTING_TIME_COLUMN_NAME].max()
            print(f"   è¨ˆç®—ç¯„å›²: {(_min if np.isfinite(_min) else np.nan):.1f}ï½{(_max if np.isfinite(_max) else np.nan):.1f}ç§’")
        except Exception:
            pass
    else:
        print("âš  é€ã‚Šé€Ÿåº¦åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚åˆ‡å‰Šæ™‚é–“ã®è¨ˆç®—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
else:
    print(f"â„¹ {Config.CUTTING_TIME_COLUMN_NAME}åˆ—ã¯æ—¢ã«å­˜åœ¨ã—ã¾ã™ã€‚è¨ˆç®—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")

# ã™ã¹ã¦ã®äºˆæ¸¬çµæœã‚’æ ¼ç´ã™ã‚‹DataFrame
df_out = df_new.copy()

# ===== ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã”ã¨ã«äºˆæ¸¬ =====
for target in TARGET_COLUMNS:
    model_folder = Config.MODEL_FOLDER
    bundle_path = os.path.join(model_folder, f"{Config.FINAL_MODEL_PREFIX}_{target}.pkl")
    if not os.path.exists(bundle_path):
        print(f"âš  ãƒãƒ³ãƒ‰ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {bundle_path}")
        continue

    print(f"\n=== {target} ç”¨ãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬ ===")
    try:
        bundle = joblib.load(bundle_path)
    except Exception as e:
        print(f"âŒ ãƒãƒ³ãƒ‰ãƒ«èª­è¾¼å¤±æ•—: {e}")
        continue

    # å­¦ç¿’æ™‚ä¿å­˜ç‰©ã®å–å¾—
    model = bundle['final_model']
    feature_columns = bundle.get('feature_columns', Config.FEATURE_COLUMNS)
    transform_method = bundle.get('transform_method', 'none')
    transform_shift = float(bundle.get('transform_shift', 0.0))
    
    # â† ã“ã“ã§ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã”ã¨ã®ç‰¹å¾´é‡å­˜åœ¨ã‚’ãƒã‚§ãƒƒã‚¯
    _ = assert_some_features_exist(df_new, feature_columns)


    # å…¥åŠ›åˆ—ã‚’æƒãˆã‚‹ï¼ˆä¸è¶³åˆ—ã¯0ã§è£œå®Œã€ä½™åˆ†ã¯ç„¡è¦–ï¼‰ï¼‹ æ•°å€¤åŒ–/å‹çµ±ä¸€
    X_base = df_new.copy()
    for c in feature_columns:
        if c not in X_base.columns:
            X_base[c] = 0
    X_new = X_base[feature_columns].copy()
    # å‹ã‚’float32ã«çµ±ä¸€ï¼ˆå‰å‡¦ç†ç³»ã¨æ•´åˆï¼‰
    for c in X_new.columns:
        # æ•°å€¤åŒ–ã«å¤±æ•—ï¼ˆä¾‹ï¼šæ–‡å­—åˆ—ï¼‰ã—ã¦ã‚‚0ã«è½ã¨ã™
        X_new[c] = pd.to_numeric(X_new[c], errors='coerce').fillna(0).astype(np.float32)

    # --- robust pipeline applicationï¼ˆæœ€å°ç‰ˆãƒ»ã“ã‚Œã ã‘ã§OKï¼‰ ---
    pipeline_path = os.path.join(model_folder, f'pipeline_{target}.pkl')
    try:
        X_new_sel = apply_saved_pipeline(X_new, bundle, pipeline_path)

    # è¡Œæ•°ãŒä¸€è‡´ã—ãªã„ï¼0ç‰¹å¾´é‡ãªã©ã®è¨­å®šäº‹æ•…ã‚’ fail-fast ã§æ¤œçŸ¥
        shape_attr = getattr(X_new_sel, 'shape', None)
        if shape_attr is None or len(shape_attr) < 2:
            raise RuntimeError("å‰å‡¦ç†å¾Œã®X_new_selã«shapeå±æ€§ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä¿å­˜å½¢å¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        
        n_rows, n_cols = shape_attr[0], shape_attr[1]
        if n_rows != len(X_new):
            raise RuntimeError(f"å‰å‡¦ç†å¾Œã®è¡Œæ•°ãŒä¸€è‡´ã—ã¾ã›ã‚“: before={len(X_new)} after={n_rows}")
        
        if n_cols == 0:
            raise RuntimeError("å‰å‡¦ç†å¾Œã®ç‰¹å¾´é‡ãŒ0æ¬¡å…ƒã§ã™ã€‚top_k ã‚„é¸æŠæ¡ä»¶ã‚’è¦‹ç›´ã—ã¦ãã ã•ã„ã€‚")
        
        print(f"â„¹ å‰å‡¦ç†å¾Œ: X_new_sel shape={shape_attr}")
    
    except Exception as e:
        print(f"âŒ å‰å‡¦ç†ã«å¤±æ•—: {e}")
        continue

    # äºˆæ¸¬ â†’ é€†å¤‰æ›ï¼ˆå­¦ç¿’æ™‚method+shiftï¼‰
    try:
        y_pred_trans = model.predict(X_new_sel)
    except Exception as e:
        print(f"âŒ äºˆæ¸¬å¤±æ•—: {e}")
        continue

    y_pred = inverse_transform_target(y_pred_trans, transform_method, transform_shift)

    # çµæœã‚’DataFrameã«è¿½åŠ 
    df_out[f"{Config.PREDICTION_COLUMN_PREFIX}_{target}"] = y_pred

    # äºˆæ¸¬çµ±è¨ˆã‚’è¡¨ç¤ºï¼ˆä»»æ„ï¼‰
    try:
        print(f"  äºˆæ¸¬ä»¶æ•°: {len(y_pred)}")
        print(f"  äºˆæ¸¬ç¯„å›²: [{np.nanmin(y_pred):.3f}, {np.nanmax(y_pred):.3f}]")
        print(f"  äºˆæ¸¬å¹³å‡: {np.nanmean(y_pred):.3f}")
        print(f"  äºˆæ¸¬æ¨™æº–åå·®: {np.nanstd(y_pred):.3f}")
    except Exception:
        pass

# ===== ä¿å­˜ =====
out_path = os.path.join(PREDICTION_FOLDER, Config.PREDICTION_OUTPUT_FILE)
df_out.to_excel(out_path, index=False)
print(f"\nâœ… äºˆæ¸¬å®Œäº†: {out_path}")
print(f"   å‡ºåŠ›åˆ—: {df_out.columns.tolist()}")

# __pycache__ãƒ•ã‚©ãƒ«ãƒ€ã‚’99_Tempã«ç§»å‹•
import shutil
import glob

def move_pycache_to_temp():
    temp_folder = Path("99_Temp")
    temp_folder.mkdir(exist_ok=True)
    pycache_folders = glob.glob("**/__pycache__", recursive=True)
    for pycache_path in pycache_folders:
        pycache_path = Path(pycache_path)
        if pycache_path.exists() and pycache_path.is_dir():
            try:
                relative_path = pycache_path.relative_to(Path.cwd())
                temp_dest = temp_folder / f"{relative_path.parent.name}__pycache__"
            except ValueError:
                temp_dest = temp_folder / f"{pycache_path.parent.name}__pycache__"
            if temp_dest.exists():
                shutil.rmtree(temp_dest)
            shutil.move(str(pycache_path), str(temp_dest))
            print(f"âœ… {pycache_path} â†’ {temp_dest}")

move_pycache_to_temp()
