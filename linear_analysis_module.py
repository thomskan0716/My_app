



#!/usr/bin/env python
# coding: utf-8

"""
ES: MÃ³dulo de AnÃ¡lisis Lineal para 0.00sec.
EN: Linear analysis module for 0.00sec.
JA: 0.00sec ç”¨ã®ç·šå½¢è§£æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã€‚

ES: Basado en ç·šå½¢ãƒ¢ãƒ‡ãƒ«_å›å¸°åˆ†é›¢æ··åˆ_Ver2_noA11A21A32.py.
EN: Based on ç·šå½¢ãƒ¢ãƒ‡ãƒ«_å›å¸°åˆ†é›¢æ··åˆ_Ver2_noA11A21A32.py.
JA: ç·šå½¢ãƒ¢ãƒ‡ãƒ«_å›å¸°åˆ†é›¢æ··åˆ_Ver2_noA11A21A32.py ã‚’ãƒ™ãƒ¼ã‚¹ã«ã—ã¦ã„ã¾ã™ã€‚

ES: Adaptado para trabajar con la base de datos del proyecto.
EN: Adapted to work with the project's database.
JA: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®DBã§å‹•ãã‚ˆã†ã«èª¿æ•´ã•ã‚Œã¦ã„ã¾ã™ã€‚
"""

import pandas as pd
import numpy as np
import os
import json
import joblib
import warnings
from pathlib import Path
from typing import Dict, List, Tuple, Any, Optional, Union
from datetime import datetime

# çµ±è¨ˆãƒ»æ©Ÿæ¢°å­¦ç¿’
from sklearn.model_selection import KFold, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler
from sklearn.linear_model import (
    LinearRegression, Ridge, Lasso, ElasticNet,
    LogisticRegression
)
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.feature_selection import SelectKBest, f_classif, f_regression
from sklearn.metrics import (
    mean_absolute_error, mean_squared_error, r2_score,
    classification_report, confusion_matrix, f1_score,
    accuracy_score, precision_score, recall_score
)

# çµ±è¨ˆçš„æ¤œå®š
from scipy import stats
from scipy.stats import shapiro, boxcox

# å¯è¦–åŒ–
import matplotlib.pyplot as plt
import matplotlib as mpl
import seaborn as sns

warnings.filterwarnings('ignore')

class LinearAnalysisConfig:
    """ES: ConfiguraciÃ³n del anÃ¡lisis lineal
    EN: Linear analysis configuration
    JA: ç·šå½¢è§£æã®è¨­å®š
    """
    
    # ES: Columnas objetivo (variables dependientes)
    # EN: Target columns (dependent variables)
    # JA: ç›®çš„å¤‰æ•°åˆ—ï¼ˆå¾“å±å¤‰æ•°ï¼‰
    TARGET_COLUMNS = ['ãƒãƒªé™¤å»', 'æ‘©è€—é‡', 'ä¸Šé¢ãƒ€ãƒ¬é‡', 'å´é¢ãƒ€ãƒ¬é‡']
    
    # ES: Tipos de tarea para cada objetivo
    # EN: Task type per target
    # JA: ç›®çš„å¤‰æ•°ã”ã¨ã®ã‚¿ã‚¹ã‚¯ç¨®åˆ¥
    TARGET_TYPES = {
        'ãƒãƒªé™¤å»': 'classification',
        'æ‘©è€—é‡': 'regression',
        'ä¸Šé¢ãƒ€ãƒ¬é‡': 'regression', 
        'å´é¢ãƒ€ãƒ¬é‡': 'regression'
    }
    
    # ES: Columnas de caracterÃ­sticas (variables independientes)
    # EN: Feature columns (independent variables)
    # JA: ç‰¹å¾´é‡åˆ—ï¼ˆç‹¬ç«‹å¤‰æ•°ï¼‰
    # ES: Mapeo de nombres de la BD a nombres del anÃ¡lisis
    # EN: Map DB column names to analysis column names
    # JA: DBåˆ—åâ†’è§£æåˆ—åã®ãƒãƒƒãƒ”ãƒ³ã‚°
    FEATURE_COLUMNS = [
        'é€ã‚Šé€Ÿåº¦', 'UPã‚«ãƒƒãƒˆ', 'åˆ‡è¾¼é‡', 
        'çªå‡ºé‡', 'è¼‰ã›ç‡', 'å›è»¢é€Ÿåº¦', 'ãƒ‘ã‚¹æ•°'  # Fixed: DB uses 'çªå‡ºé‡'
    ]
    
    # ES: Mapeo de nombres de la BD a nombres del anÃ¡lisis
    # EN: Map DB column names to analysis column names
    # JA: DBåˆ—åâ†’è§£æåˆ—åã®ãƒãƒƒãƒ”ãƒ³ã‚°
    # ES: Nota: La BD usa 'çªå‡ºé‡' pero el anÃ¡lisis espera 'çªå‡ºé‡' (sin ã—)
    # EN: Note: the DB uses 'çªå‡ºé‡' and the analysis expects 'çªå‡ºé‡' (no ã—)
    # JA: æ³¨æ„ï¼šDBã¯ã€Œçªå‡ºé‡ã€ã€è§£æã‚‚ã€Œçªå‡ºé‡ã€ï¼ˆã—ç„¡ã—ï¼‰ã‚’æœŸå¾…
    DB_TO_ANALYSIS_MAPPING = {
        'é€ã‚Šé€Ÿåº¦': 'é€ã‚Šé€Ÿåº¦',
        'UPã‚«ãƒƒãƒˆ': 'UPã‚«ãƒƒãƒˆ', 
        'åˆ‡è¾¼é‡': 'åˆ‡è¾¼é‡',
        'çªå‡ºé‡': 'çªå‡ºé‡',  # Fixed: DB uses 'çªå‡ºé‡'
        'è¼‰ã›ç‡': 'è¼‰ã›ç‡',
        'å›è»¢é€Ÿåº¦': 'å›è»¢é€Ÿåº¦',
        'ãƒ‘ã‚¹æ•°': 'ãƒ‘ã‚¹æ•°'
    }
    
    # Mapeo inverso
    ANALYSIS_TO_DB_MAPPING = {v: k for k, v in DB_TO_ANALYSIS_MAPPING.items()}
    
    INNER_CV_SPLITS = 5
    OUTER_CV_SPLITS = 5
    RANDOM_STATE = 42

class LinearAnalysisPipeline:
    """ES: Pipeline de anÃ¡lisis lineal simplificado
    EN: Simplified linear analysis pipeline
    JA: ç°¡æ˜“ç·šå½¢è§£æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
    """
    
    def __init__(self, output_dir: str = "output_analysis"):
        """ES: Inicializar el pipeline
        EN: Initialize the pipeline
        JA: ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’åˆæœŸåŒ–
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        self.models = {}
        self.scalers = {}
        self.results = {}
        self.transformation_info = {}
        
        # ES: Configurar matplotlib para japonÃ©s
        # EN: Configure matplotlib for Japanese fonts
        # JA: matplotlib ã‚’æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆå‘ã‘ã«è¨­å®š
        self._setup_japanese_font()
    
    def _setup_japanese_font(self):
        """ES: Configurar fuente japonesa para matplotlib
        EN: Configure Japanese font for matplotlib
        JA: matplotlib ã®æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
        """
        try:
            if os.name == 'nt':
                fonts = ['MS Gothic', 'Yu Gothic', 'Meiryo']
            else:
                fonts = ['IPAexGothic', 'Hiragino Sans', 'Noto Sans CJK JP']
            
            for font in fonts:
                try:
                    mpl.rcParams['font.family'] = font
                    mpl.rcParams['font.size'] = 12
                    break
                except:
                    continue
        except Exception as e:
            print(f"âš ï¸ æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚’è¨­å®šã§ãã¾ã›ã‚“ã§ã—ãŸ: {e}")
    
    def prepare_data(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """ES: Preparar datos para el anÃ¡lisis
        EN: Prepare data for analysis
        JA: è§£æç”¨ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™"""
        print("ğŸ”§ è§£æç”¨ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ä¸­...")
        
        # ES: Mapear nombres de columnas de la BD a nombres del anÃ¡lisis
        # EN: Map DB column names to analysis names
        # JA: DBåˆ—åã‚’è§£æåã«ãƒãƒƒãƒ”ãƒ³ã‚°
        column_mapping = {}
        for db_col, analysis_col in LinearAnalysisConfig.DB_TO_ANALYSIS_MAPPING.items():
            if db_col in df.columns:
                column_mapping[db_col] = analysis_col
        
        # ES: Crear DataFrame con nombres mapeados
        # EN: Create a DataFrame with mapped names
        # JP: ãƒãƒƒãƒ”ãƒ³ã‚°æ¸ˆã¿åç§°ã§DataFrameã‚’ä½œæˆ
        df_mapped = df.rename(columns=column_mapping)
        
        # ES: Seleccionar solo las columnas necesarias
        # EN: Select only the required columns
        # JP: å¿…è¦ãªåˆ—ã®ã¿é¸æŠ
        available_features = [col for col in LinearAnalysisConfig.FEATURE_COLUMNS 
                            if col in df_mapped.columns]
        available_targets = [col for col in LinearAnalysisConfig.TARGET_COLUMNS 
                           if col in df_mapped.columns]
        
        print(f"ğŸ”§ åˆ©ç”¨å¯èƒ½ãªç‰¹å¾´é‡: {available_features}")
        print(f"ğŸ”§ åˆ©ç”¨å¯èƒ½ãªç›®çš„å¤‰æ•°: {available_targets}")
        
        # ES: Crear X (caracterÃ­sticas) e y (objetivos) | EN: Build X (features) and y (targets) | JA: Xï¼ˆç‰¹å¾´é‡ï¼‰ã¨yï¼ˆç›®çš„ï¼‰ã‚’ä½œæˆ
        X = df_mapped[available_features].copy()
        y = df_mapped[available_targets].copy()
        
        # Manejar valores faltantes
        X = X.fillna(X.median())
        for col in y.columns:
            if y[col].dtype in ['int64', 'float64']:
                y[col] = y[col].fillna(y[col].median())
        
        return X, y
    
    def train_models(self, X: pd.DataFrame, y: pd.DataFrame):
        """Entrenar modelos para cada objetivo"""
        print("ğŸ”§ ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ä¸­...")
        
        for target_col in y.columns:
            if target_col not in LinearAnalysisConfig.TARGET_TYPES:
                continue
                
            task_type = LinearAnalysisConfig.TARGET_TYPES[target_col]
            print(f"ğŸ”§ {target_col} ã®ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ä¸­ï¼ˆ{task_type}ï¼‰")
            
            try:
                # ES: Obtener datos vÃ¡lidos para este objetivo | EN: Get valid data for this target | JA: ã“ã®ç›®çš„å¤‰æ•°ç”¨ã®æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                valid_mask = ~y[target_col].isnull()
                X_valid = X[valid_mask]
                y_valid = y[target_col][valid_mask]
                
                if len(X_valid) < 10:
                    print(f"âš ï¸ ãƒ‡ãƒ¼ã‚¿ä¸è¶³: {target_col}ï¼ˆ{len(X_valid)} ã‚µãƒ³ãƒ—ãƒ«ï¼‰")
                    continue
                
                if task_type == 'regression':
                    model_info = self._train_regression_model(X_valid, y_valid, target_col)
                else:
                    model_info = self._train_classification_model(X_valid, y_valid, target_col)
                
                self.models[target_col] = model_info
                
            except Exception as e:
                print(f"âŒ ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ä¸­ã«ã‚¨ãƒ©ãƒ¼: {target_col}: {e}")
                self.models[target_col] = {'error': str(e)}
    
    def _train_regression_model(self, X: pd.DataFrame, y: pd.Series, target_name: str) -> Dict:
        """ES: Entrenar modelo de regresiÃ³n
        EN: Train regression model
        JA: å›å¸°ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’"""
        models = {
            'LinearRegression': LinearRegression(),
            'Ridge': Ridge(random_state=LinearAnalysisConfig.RANDOM_STATE),
            'Lasso': Lasso(random_state=LinearAnalysisConfig.RANDOM_STATE, max_iter=2000)
        }
        
        best_model_name = None
        best_score = -float('inf')
        best_model = None
        
        # ES: ValidaciÃ³n cruzada simple | EN: Simple cross-validation | JA: ç°¡æ˜“äº¤å·®æ¤œè¨¼
        cv = KFold(n_splits=LinearAnalysisConfig.INNER_CV_SPLITS, 
                   shuffle=True, random_state=LinearAnalysisConfig.RANDOM_STATE)
        
        for name, model in models.items():
            try:
                scores = cross_val_score(model, X, y, cv=cv, scoring='r2')
                mean_score = scores.mean()
                
                if mean_score > best_score:
                    best_score = mean_score
                    best_model_name = name
                    best_model = model
                    
            except Exception as e:
                print(f"âš ï¸ {name} ã§ã‚¨ãƒ©ãƒ¼: {e}")
        
        if best_model is None:
            best_model = LinearRegression()
            best_model_name = 'LinearRegression'
        
        # ES: Entrenar el mejor modelo
        # EN: Train the best model
        # JP: æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’
        best_model.fit(X, y)
        y_pred = best_model.predict(X)
        
        # MÃ©tricas finales
        mae = mean_absolute_error(y, y_pred)
        rmse = np.sqrt(mean_squared_error(y, y_pred))
        r2 = r2_score(y, y_pred)
        
        # ES: Guardar modelo
        # EN: Save the model
        # JP: ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
        model_path = self.output_dir / f'model_{target_name}.pkl'
        model_data = {
            'model': best_model,
            'feature_names': X.columns.tolist(),
            'target_name': target_name,
            'model_name': best_model_name
        }
        joblib.dump(model_data, model_path)
        
        # ES: Crear grÃ¡fico | EN: Create chart | JA: ã‚°ãƒ©ãƒ•ã‚’ä½œæˆ
        self._plot_regression_results(y, y_pred, target_name)
        
        return {
            'model': best_model,
            'model_name': best_model_name,
            'model_path': str(model_path),
            'metrics': {'mae': mae, 'rmse': rmse, 'r2': r2},
            'task_type': 'regression'
        }
    
    def _train_classification_model(self, X: pd.DataFrame, y: pd.Series, target_name: str) -> Dict:
        """ES: Entrenar modelo de clasificaciÃ³n
        EN: Train classification model
        JA: åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´"""
        # ES: Verificar que hay suficientes muestras por clase
        # EN: Check that there are enough samples per class
        # JP: ã‚¯ãƒ©ã‚¹ã”ã¨ã®ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒååˆ†ã‹ç¢ºèªã™ã‚‹
        class_counts = y.value_counts()
        if len(class_counts) < 2 or class_counts.min() < 5:
            return {'error': 'insufficient_samples'}
        
        # Codificar etiquetas
        le = LabelEncoder()
        y_encoded = le.fit_transform(y)
        
        # ES: Entrenar modelo
        # EN: Train the model
        # JP: ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’
        model = LogisticRegression(random_state=LinearAnalysisConfig.RANDOM_STATE, max_iter=2000)
        model.fit(X, y_encoded)
        
        # Predicciones
        y_pred = model.predict(X)
        y_proba = model.predict_proba(X) if hasattr(model, 'predict_proba') else None
        
        # MÃ©tricas
        accuracy = accuracy_score(y_encoded, y_pred)
        f1 = f1_score(y_encoded, y_pred, average='weighted')
        
        # ES: Guardar modelo
        # EN: Save the model
        # JP: ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
        model_path = self.output_dir / f'model_{target_name}.pkl'
        model_data = {
            'model': model,
            'label_encoder': le,
            'feature_names': X.columns.tolist(),
            'target_name': target_name
        }
        joblib.dump(model_data, model_path)
        
        return {
            'model': model,
            'label_encoder': le,
            'model_name': 'LogisticRegression',
            'model_path': str(model_path),
            'metrics': {'accuracy': accuracy, 'f1_score': f1},
            'task_type': 'classification'
        }
    
    def _plot_regression_results(self, y_true: pd.Series, y_pred: np.ndarray, target_name: str):
        """ES: Crear grÃ¡fico de resultados de regresiÃ³n
        EN: Create regression results plot
        JA: å›å¸°çµæœã®ã‚°ãƒ©ãƒ•ã‚’ä½œæˆ"""
        try:
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
            
            # ES: GrÃ¡fico 1: PredicciÃ³n vs Real | EN: Chart 1: Prediction vs Actual | JA: ã‚°ãƒ©ãƒ•1ï¼šäºˆæ¸¬vså®Ÿæ¸¬
            ax1.scatter(y_true, y_pred, alpha=0.6)
            ax1.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)
            ax1.set_xlabel('Valor Real')
            ax1.set_ylabel('PredicciÃ³n')
            ax1.set_title(f'{target_name}: PredicciÃ³n vs Real')
            ax1.grid(True, alpha=0.3)
            
            # ES: GrÃ¡fico 2: Residuales | EN: Chart 2: Residuals | JA: ã‚°ãƒ©ãƒ•2ï¼šæ®‹å·®
            residuals = y_true - y_pred
            ax2.scatter(y_pred, residuals, alpha=0.6)
            ax2.axhline(y=0, color='r', linestyle='--', linewidth=2)
            ax2.set_xlabel('PredicciÃ³n')
            ax2.set_ylabel('Residuales')
            ax2.set_title(f'{target_name}: AnÃ¡lisis de Residuales')
            ax2.grid(True, alpha=0.3)
            
            plt.tight_layout()
            
            # ES: Guardar grÃ¡fico | EN: Save chart | JA: ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜
            plot_path = self.output_dir / f'regression_{target_name}.png'
            plt.savefig(plot_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"âœ… ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {plot_path}")
            
        except Exception as e:
            print(f"âš ï¸ ã‚°ãƒ©ãƒ•ä½œæˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {target_name}: {e}")
    
    def save_results(self):
        """ES: Guardar resultados del anÃ¡lisis
        EN: Save analysis results
        JA: è§£æçµæœã‚’ä¿å­˜"""
        print("ğŸ”§ çµæœã‚’ä¿å­˜ä¸­...")
        
        # Resumen de resultados
        results_summary = []
        for target_col, model_info in self.models.items():
            if 'error' in model_info:
                row = {
                    'Target': target_col,
                    'Status': 'Failed',
                    'Error': model_info['error']
                }
            else:
                row = {
                    'Target': target_col,
                    'Status': 'Success',
                    'Model': model_info['model_name'],
                    'Task_Type': model_info['task_type']
                }
                
                if 'metrics' in model_info:
                    metrics = model_info['metrics']
                    if model_info['task_type'] == 'regression':
                        row.update({
                            'MAE': f"{metrics.get('mae', 'N/A'):.4f}",
                            'RMSE': f"{metrics.get('rmse', 'N/A'):.4f}",
                            'R2': f"{metrics.get('r2', 'N/A'):.4f}"
                        })
                    else:
                        row.update({
                            'Accuracy': f"{metrics.get('accuracy', 'N/A'):.4f}",
                            'F1_Score': f"{metrics.get('f1_score', 'N/A'):.4f}"
                        })
            
            results_summary.append(row)
        
        # ES: Guardar como Excel
        # EN: Save as Excel
        # JP: Excelã¨ã—ã¦ä¿å­˜
        results_df = pd.DataFrame(results_summary)
        results_path = self.output_dir / 'analysis_results.xlsx'
        results_df.to_excel(results_path, index=False)
        print(f"âœ… çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {results_path}")
        
        # ES: Guardar como JSON
        # EN: Save as JSON
        # JP: JSONã¨ã—ã¦ä¿å­˜
        results_json = {
            'timestamp': datetime.now().isoformat(),
            'models': {k: {
                'model_name': v.get('model_name', 'Unknown'),
                'task_type': v.get('task_type', 'Unknown'),
                'metrics': v.get('metrics', {}),
                'error': v.get('error', None)
            } for k, v in self.models.items()},
            'output_directory': str(self.output_dir)
        }
        
        json_path = self.output_dir / 'analysis_results.json'
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(results_json, f, indent=2, ensure_ascii=False)
        print(f"âœ… çµæœJSONã‚’ä¿å­˜ã—ã¾ã—ãŸ: {json_path}")
    
    def run_analysis(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ES: Ejecutar anÃ¡lisis completo
        EN: Run full analysis
        JA: è§£æã‚’ä¸€é€šã‚Šå®Ÿè¡Œ"""
        print("ğŸš€ ç·šå½¢è§£æã‚’é–‹å§‹...")
        
        try:
            # Preparar datos
            X, y = self.prepare_data(df)
            
            if X.empty or y.empty:
                raise ValueError("è§£æã«ä½¿ç”¨ã§ãã‚‹æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            
            print(f"âœ… ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†: {X.shape[0]} ã‚µãƒ³ãƒ—ãƒ«, {X.shape[1]} ç‰¹å¾´é‡")
            
            # Entrenar modelos
            self.train_models(X, y)
            
            # ES: Guardar resultados
            # EN: Save results
            # JP: çµæœã‚’ä¿å­˜
            self.save_results()
            
            print("âœ… ç·šå½¢è§£æãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ")
            
            return {
                'success': True,
                'models': self.models,
                'output_directory': str(self.output_dir),
                'data_shape': X.shape
            }
            
        except Exception as e:
            print(f"âŒ ç·šå½¢è§£æä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            return {
                'success': False,
                'error': str(e)
            }

def run_linear_analysis_from_db(db_manager, filters: Dict = None) -> Dict[str, Any]:
    """ES: FunciÃ³n principal para ejecutar anÃ¡lisis lineal desde la base de datos
    EN: Main function to run linear analysis from the database
    JA: DBã‹ã‚‰ç·šå½¢è§£æã‚’å®Ÿè¡Œã™ã‚‹ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    try:
        # Obtener datos de la base de datos
        if filters:
            # ES: Aplicar filtros (implementar segÃºn la estructura de la BD)
            # EN: Apply filters (implement per DB structure)
            # JA: ãƒ•ã‚£ãƒ«ã‚¿ã‚’é©ç”¨ï¼ˆDBæ§‹é€ ã«å¿œã˜ã¦å®Ÿè£…ï¼‰
            query = "SELECT * FROM main_results WHERE 1=1"
            params = []
            
            for field, value in filters.items():
                if value and value != "":
                    if isinstance(value, tuple):  # Rango de valores
                        if value[0] and value[1]:
                            query += f" AND {field} BETWEEN ? AND ?"
                            params.extend([value[0], value[1]])
                    elif field in ['A13', 'A11', 'A21', 'A32']:  # Campos de cepillos
                        # ES: Filtrar por cepillo especÃ­fico = 1 | EN: Filter by specific brush = 1 | JA: ç‰¹å®šãƒ–ãƒ©ã‚·ï¼1ã§ãƒ•ã‚£ãƒ«ã‚¿
                        query += f" AND {field} = ?"
                        params.append(value)
                    else:  # Valor Ãºnico
                        query += f" AND {field} = ?"
                        params.append(value)
            
            cursor = db_manager.conn.cursor()
            cursor.execute(query, params)
            columns = [description[0] for description in cursor.description]
            data = cursor.fetchall()
            
            if not data:
                return {'success': False, 'error': 'No se encontraron datos con los filtros especificados'}
            
            df = pd.DataFrame(data, columns=columns)
            
        else:
            # Sin filtros, obtener todos los datos
            try:
                # Obtener datos usando fetch_all
                data = db_manager.fetch_all('main_results')
                if not data:
                    return {'success': False, 'error': 'La tabla main_results estÃ¡ vacÃ­a'}
                
                # Obtener nombres de columnas
                cursor = db_manager.conn.cursor()
                cursor.execute("PRAGMA table_info(main_results)")
                columns_info = cursor.fetchall()
                column_names = [col[1] for col in columns_info]
                
                df = pd.DataFrame(data, columns=column_names)
                
            except Exception as e:
                print(f"âš ï¸ DBã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
                return {'success': False, 'error': f'Error accediendo a la base de datos: {str(e)}'}
        
        print(f"ğŸ“Š å–å¾—ãƒ‡ãƒ¼ã‚¿: {df.shape[0]} è¡Œ, {df.shape[1]} åˆ—")
        
        # ES: Crear y ejecutar pipeline de anÃ¡lisis | EN: Create and run analysis pipeline | JA: è§£æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½œæˆãƒ»å®Ÿè¡Œ
        pipeline = LinearAnalysisPipeline()
        results = pipeline.run_analysis(df)
        
        return results
        
    except Exception as e:
        print(f"âŒ ç·šå½¢è§£æã®å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        return {
            'success': False,
            'error': str(e)
        }
